# Multiple Linear Regression: Extending to More Features

Now, we're going to take what you've learned about linear regression and make it **faster** and **more powerful** by incorporating multiple features into our model.

## Moving Beyond One Feature

In the basic version of linear regression, we used a **single feature** to predict the output. For example, you might have used **$x$** (the size of the house) to predict **$y$** (the price of the house). The model was written as:

$$
f_{w, b}(x) = w \cdot x + b
$$

But what if we had **more features** to help us predict the price? Features like the **number of bedrooms**, **number of floors**, and the **age of the house** could give us much more information. This leads us to **multiple linear regression**, where we use several features to make predictions.

You learned multiple linear regression in the previous chapter(***Data Analysis with Python***), but now we're going to take it a step further by introducing **vectorization**.

## Introducing Multiple Features

Let‚Äôs introduce some new notation to handle multiple features. Suppose we have four features: 

- **$x_1$**: the size of the house (square feet),
- **$x_2$**: the number of bedrooms,
- **$x_3$**: the number of floors,
- **$x_4$**: the age of the house (years).

For simplicity, we‚Äôll use **$x_j$** to represent each feature, where **$j$** can go from 1 to **n** (the total number of features). So, in this example, **$n$** is 4.

We‚Äôll also use **$X^{(i)}$** to represent the **i-th training example**. This means **$X^{(i)}$** is a **vector** that contains all the features of the **i-th** example. For instance, if the second training example has:

- 1416 square feet,
- 3 bedrooms,
- 2 floors,
- and is 40 years old,

then **$X^{(2)} = [1416, 3, 2, 40]$**.

### Referring to Individual Features

If you want to refer to a specific feature of a training example, use **$ X^{(i)}_j$**. For example, **$X^{(2)}_3$** would be the number of floors in the second training example, which is 2. 


To make things easier to visualize, we sometimes draw an arrow on top of the vector **$\vec X^{(i)}$** to emphasize that it‚Äôs a list of numbers, not just a single value. However, the arrow is optional and more of a visual aid.

> **Note**: The arrow notation is standard in linear algebra. It‚Äôs used to distinguish between **scalars** (single numbers) and **vectors** (lists of numbers). Vector includes direction and magnitude, while scalar only has magnitude. We will learn more about vectors in the next section, ***Mathematics for Machine Learning and Data Science with Python***.

## The Model with Multiple Features

Now that we have **multiple features**, our model becomes more complex. Here‚Äôs what the model looks like with four features:

$$
f_{w, b}(X) = w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + b
$$

Let‚Äôs make this concrete by looking at a possible model for predicting house prices:

$$
f_{w, b}(X) = 0.1 X_1 + 4 X_2 + 10 X_3 - 2 X_4 + 80
$$

### Interpreting the Parameters

Each parameter **$w_j$** has a specific meaning:

- **$w_1 = 0.1$**: For every additional square foot, the house price increases by **$100** (since 0.1 times $1,000 equals $100).
- **$w_2 = 4$**: Each additional bedroom adds **$4,000** to the price.
- **$w_3 = 10$**: Each additional floor adds **$10,000**.
- **$w_4 = -2$**: For each additional year of age, the price **decreases** by **$2,000**.
- **$b = 80$**: The base price starts at **$80,000** (when all features are zero).

So, if you have **n** features, the model is written as:

$$
f_{w, b}(X) = w_1 X_1 + w_2 X_2 + ... + w_n X_n + b
$$

## A More Compact Notation

To simplify things, let‚Äôs introduce **vector notation**. Instead of writing out each **$w_j$** and **$X_j$** individually, we‚Äôll collect all the **$w_j$'s** into a vector **$W$**. This vector contains the parameters **$w_1, w_2, ..., w_n$**. Similarly, we‚Äôll represent the features **$X_1, X_2, ..., X_n$** as a vector **$X$**.

Now, we can rewrite the model in a more compact way:

$$
f_{{\vec W}, {b}}(\vec X) = \vec W \cdot \vec X + b
$$

### What Is a Dot Product?

The **dot product** is a mathematical operation that takes two vectors and multiplies their corresponding elements. After multiplying the elements, you **sum** the results. So:

$$
\vec W \cdot \vec X = w_1 X_1 + w_2 X_2 + ... + w_n X_n
$$

Then, we add the bias term **$b$** to get the final prediction.

# What‚Äôs Next: Vectorization

Now that you understand multiple linear regression, the next step is to **optimize** how we implement it. There's a neat technique called **vectorization** that will make your code shorter, faster, and more efficient. It‚Äôs especially useful when working with large datasets and complex algorithms.

## Vectorization: The Key to Faster and More Efficient Machine Learning Code

When implementing a machine learning algorithm, one powerful technique you should know about is **vectorization**. Using vectorization will not only make your code **shorter** and **easier to read**, but it will also make it run **much faster**. This is because vectorization allows you to take advantage of optimized numerical linear algebra libraries and even specialized hardware like **GPUs** (Graphics Processing Units), which are designed to handle large amounts of data quickly.

Let‚Äôs break down what vectorization means and how it works with a concrete example.

> **Note** : You may have heard of ***GPU, NVIDIA, CUDA***.   
> - **GPU** stands for Graphic Processing Unit, which is a specialized processor originally designed to accelerate rendering of complex graphics, especially for gaming and visual applications. However, GPUs are now widely used for parallel computing tasks due to their ability to perform many operations simultaneously.
> - **NVIDIA** is a leading company that designs and manufactures GPUs. They are well-known for their contributions to both gaming and general-purpose computing on GPUs (GPGPU), which includes applications like machine learning, scientific computation, and AI.
> - **CUDA**, which stands for Compute Unified Device Architecture, is a parallel computing platform and programming model created by NVIDIA. CUDA also makes it easy for developers to take advantage of all the latest GPU architecture innovations. 

## Example: Compute a Prediction with Parameters

Imagine you have a model with parameters **$W$** and **$b$**:

- **$\vec W$** is a vector of numbers, let‚Äôs say it has three values: **$w_1$**, **$w_2$**, and **$w_3$**.
- **$\vec X$** is another vector of features, also with three values: **$X_1$**, **$X_2$**, and **$X_3$**.

In this case, **n = 3**, meaning both **$\vec W$** and **$\vec X$** have 3 components. If you're writing the code in **Python**, you can use the popular **NumPy** library, which is great for numerical operations(And you already know about it from the previous chapter). Here's an example of how you might define these vectors in Python:

```python
import numpy as np
w = np.array([w1, w2, w3])
x = np.array([x1, x2, x3])
b = 50
```

## Non-Vectorized Implementation

Now, let‚Äôs compute the prediction without using vectorization. In this case, for each $X_j$, you multiply it by the corresponding $X_j$ and sum the results. This can be done manually:

$$
f = w_0 \cdot X_0 + w_1 \cdot X_1 + w_2 \cdot X_2 + b
$$

But what if n is not 3? What if it's 1,000, or 100,000? Writing all those terms would be inefficient, not just for you as a coder, but also for the computer to process.

A more flexible way to do this without vectorization is by using a for loop:

```python
f = 0
for j in range(3):  
    f += w[j] * X[j]
f += b 
```

This code works for any **n**, but it's still not the most efficient. When **n** gets large, the computation slows down.

## Introducing Vectorization

To speed things up, we can use vectorization. In mathematical terms, we are performing a dot product between the vectors $W$ and $X$, and then adding the bias $b$.

The formula looks like this:

 $$ùëì_{W,X}=W \cdot X +ùëè$$

But in Python, with NumPy, you can compute this in one line of code using the np.dot() function:

```python
f = np.dot(W, X) + b
```

This is the vectorized implementation of the same operation. It‚Äôs shorter and more efficient!

## Why Vectorization is Faster

So, why is this single line of vectorized code so much faster than the for-loop code? The secret lies in how computers handle these operations behind the scenes. When you use vectorized functions like `np.dot()`, NumPy is able to take advantage of parallel processing.

Think of it like this: if you were trying to build a house, doing all the work yourself (sequentially) would take much longer than if you had a team of workers (parallel processing) each handling different tasks at the same time.

Even though your code is shorter, the real speedup comes from your computer‚Äôs ability to run multiple operations simultaneously. This is especially true if you're using a ***GPU***. ***GPUs*** were originally designed for rendering graphics quickly, but their architecture also makes them very good at performing many mathematical operations in parallel, like those needed for vectorized computations.

## Recap: The Benefits of Vectorization

1. Shorter Code: With vectorization, you can replace a long, complex loop with a single, simple line of code. This makes your code easier to write and much more readable.

2. Faster Execution: Vectorized code runs much faster, especially for large datasets. This is because modern libraries like NumPy are optimized to take full advantage of your computer‚Äôs parallel processing capabilities.

3. Scalability: When n is small, the performance difference might not be noticeable. But as n grows, the performance boost from vectorization becomes significant.

## What Happens Behind the Scenes?

Let‚Äôs first look at how a **for-loop** operates without vectorization. Imagine we‚Äôre iterating through 16 elements, from **j = 0** to **j = 15**. Without vectorization, the loop processes one element at a time. Here‚Äôs how it works in steps:

- At **time-step $t_0$**, it processes the first element (index 0).
- At the next time-step, it moves to the next element (index 1), and so on.
- This continues until the 16th step when the final element is processed.

In other words, it computes everything **one step at a time**.

Now, let‚Äôs compare that with **vectorized code**. In a vectorized implementation, the computer can grab **all the elements of the vectors** (say, **$W$** and **$X$**) and compute their values **in parallel**. The computer multiplies each pair of **$W$** and **$X$** elements at the same time. After that, it efficiently sums these values using specialized hardware.

This is the key reason why vectorized code is so much faster. Instead of processing each operation **sequentially**, the computer uses parallel processing to handle **all** operations in one step, making the entire process much quicker.

### Why Does This Matter?

You might be thinking, "Why does this matter?" Well, it matters **a lot**, especially when you‚Äôre dealing with **large datasets** or **complex machine learning models**. Training these models requires a lot of calculations, and without vectorization, those calculations can take hours or even days or **forever**! But with vectorization, those same calculations might only take minutes!

That‚Äôs why using vectorization is essential to building efficient machine learning algorithms that can scale well to handle large amounts of data.

## Vectorization in Action: Linear Regression with Multiple Features

Let‚Äôs now look at a **real-world example**: implementing multiple linear regression using vectorization. 

Imagine you have a dataset with **16 input features** and corresponding parameters **$w_1$ through $w_16$** (the weights), as well as a parameter **$b$** (the bias).

### Non-Vectorized Approach

Without vectorization, you might compute the updates for each of the 16 parameters manually. For example, you would($d_j$ is derivative of the cost function):

- Update **$w_1$** as: $w_1$ = $w_1$ - 0.1 * $d_1$
- Update **$w_2$** as: $w_2$ = $w_2$ - 0.1 * $d_2$
- And so on, until you reach **$w_{16}$**.

This would be tedious to write and inefficient to run. You might use a **for-loop** to handle this:

```python
for j in range(16):  
    w[j] = w[j] - 0.1 * d[j]
```

This loop calculates each update step sequentially, which can slow things down if you have a lot of features or data.

### Vectorized Approach

Now let‚Äôs look at the vectorized version. Instead of looping through each element, you can perform the entire operation in one step by writing:

```python
w = w - 0.1 * d
```

Here, $\vec W$ and $\vec d$ are both NumPy arrays, and the code runs much faster because it uses the computer‚Äôs parallel processing capabilities. Instead of updating each weight individually, the computer updates all 16 weights in parallel.

This difference in performance might not seem like a big deal for just 16 features, but imagine again you had thousands of features or were working with a large dataset. The speedup from vectorization could mean the difference between your code running in minutes or hours.

# Implementing Gradient Descent for Multiple Linear Regression with Vectorization

Now that you‚Äôve learned about **gradient descent**, **multiple linear regression**, and **vectorization**, it‚Äôs time to bring all of these concepts together to implement **gradient descent for multiple linear regression using vectorization**.

## A Quick Review of Multiple Linear Regression

Previously, we talked about multiple linear regression as having parameters **$w_1$** through **$w_n$** and a bias term **$b$**. Instead of thinking of **$w_1$** through **$w_n$** as separate parameters, we can **collect all the $w$ s into a vector**, which we‚Äôll call **$\vec W$**. So, **$\vec W$** is now a vector of length **n** (where **n** is the number of features). The bias term **$b$** remains a single number.

Now, using vector notation, we can rewrite the model as:

$$
f_{\vec W,b}(\vec X) = \vec W \cdot \vec X + b
$$

Here, the "dot" symbol (¬∑) represents the **dot product** between the vector **$\vec W$** and the vector **$\vec X$** (the features).

## Vectorized Cost Function

Our cost function, previously written as $J(w_1, w_2, ..., w_n, b)$, can now be written more compactly as $J_{\vec W, b}$. Instead of **n** separate parameters, we now have a single vector **$\vec W$** and the number **$b$**. The function $J_{\vec W, b}$ still outputs a single number (the cost), but it now takes a vector as input.

## Gradient Descent for Multiple Features

Here‚Äôs what **gradient descent** looks like for multiple linear regression:

- Repeatedly update each parameter **$w_j$** using the update rule:

$$
w_j = w_j - \alpha \cdot \frac{\partial J_{\vec W, b}}{\partial w_j}
$$

$$
b = b - \alpha \cdot \frac{\partial J_{\vec W, b}}{\partial b}
$$

Where **Œ±** is the learning rate, and $\frac{\partial J_{\vec W, b}}{\partial w_j}$ is the derivative of the cost function **J** with respect to **$w_j$**.

## Derivatives for Gradient Descent

Let‚Äôs look at how the derivative terms work for multiple features. When we had only **one** feature, the update rule for **w** looked like this:

$$
w = w - \alpha \cdot \frac{\partial J}{\partial w}
$$

Now that we have **multiple features**, the update rule becomes:

$$
w_j = w_j - \alpha \cdot \frac{\partial J}{\partial w_j}
$$

For each **j** (from 1 to **n**), we need to update each **$w_j$** using its corresponding derivative. The main difference is that **$\vec W$** and **$\vec X$** are vectors. 

The general rule is that we update **all parameters $w_1$, $w_2$, ..., $w_n$** and then update **$b$**, the bias term, in each iteration of gradient descent.

## Vectorization in Gradient Descent

With **vectorization**, we don‚Äôt need to update each **$w_j$** one at a time. Instead, we can use vectorized operations to update **all the weights** at once. This makes the code more concise and much faster to run.

In **vectorized form**, the gradient descent update rule for all **$w$** becomes:

$$
w_n = w_n - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( f_{\vec{w}, b}(\vec{x}^{(i)}) - y^{(i)} \right) x_n^{(i)}
$$

And for **b**, the update is similar:

$$
b = b - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( f_{\vec{w}, b}(\vec{x}^{(i)}) - y^{(i)} \right)
$$

Simultaneously update **$w_j$ (for j = 1, ...,  n)** and **$b$** in each iteration.  

## The Normal Equation: An Alternative to Gradient Descent

Before we move on, it‚Äôs important to note that there‚Äôs another method for finding **w** and **b** in linear regression, called the **normal equation**. While **gradient descent** is a powerful method for minimizing the cost function $J_{\vec W, b}$, the **normal equation** offers a different approach.

The **normal equation** is a formula that allows us to solve for **$w$** and **$b$** in one step, without using an iterative process like gradient descent. This is possible because linear regression has an exact solution that can be found using advanced linear algebra techniques.

Using a **linear algebra library**, we can compute **$w$** and **$b$** directly by solving the normal equation.

However, the **normal equation** has some limitations:

- It only works for **linear regression**. It **cannot** be generalized to other algorithms like **logistic regression** or **neural networks**.
- The normal equation is slow when the number of features is very large. This is because solving it involves inverting a matrix, which becomes computationally expensive as the number of features grows.