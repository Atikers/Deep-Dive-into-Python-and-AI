# Multiple Linear Regression: Extending to More Features

Now, we're going to take what you've learned about linear regression and make it **faster** and **more powerful** by incorporating multiple features into our model.

## Moving Beyond One Feature

In the basic version of linear regression, we used a **single feature** to predict the output. For example, you might have used **$x$** (the size of the house) to predict **$y$** (the price of the house). The model was written as:

$$
f_{w, b}(x) = w \cdot x + b
$$

But what if we had **more features** to help us predict the price? Features like the **number of bedrooms**, **number of floors**, and the **age of the house** could give us much more information. This leads us to **multiple linear regression**, where we use several features to make predictions.

You learned multiple linear regression in the previous chapter(***Data Analysis with Python***), but now we're going to take it a step further by introducing **vectorization**.

## Introducing Multiple Features

Let‚Äôs introduce some new notation to handle multiple features. Suppose we have four features: 

- **$x_1$**: the size of the house (square feet),
- **$x_2$**: the number of bedrooms,
- **$x_3$**: the number of floors,
- **$x_4$**: the age of the house (years).

For simplicity, we‚Äôll use **$x_j$** to represent each feature, where **$j$** can go from 1 to **n** (the total number of features). So, in this example, **$n$** is 4.

We‚Äôll also use **$X^{(i)}$** to represent the **i-th training example**. This means **$X^{(i)}$** is a **vector** that contains all the features of the **i-th** example. For instance, if the second training example has:

- 1416 square feet,
- 3 bedrooms,
- 2 floors,
- and is 40 years old,

then **$X^{(2)} = [1416, 3, 2, 40]$**.

### Referring to Individual Features

If you want to refer to a specific feature of a training example, use **$ X^{(i)}_j$**. For example, **$X^{(2)}_3$** would be the number of floors in the second training example, which is 2. 


To make things easier to visualize, we sometimes draw an arrow on top of the vector **$\vec X^{(i)}$** to emphasize that it‚Äôs a list of numbers, not just a single value. However, the arrow is optional and more of a visual aid.

> **Note**: The arrow notation is standard in linear algebra. It‚Äôs used to distinguish between **scalars** (single numbers) and **vectors** (lists of numbers). Vector includes direction and magnitude, while scalar only has magnitude. We will learn more about vectors in the next section, ***Mathematics for Machine Learning and Data Science with Python***.

## The Model with Multiple Features

Now that we have **multiple features**, our model becomes more complex. Here‚Äôs what the model looks like with four features:

$$
f_{w, b}(X) = w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + b
$$

Let‚Äôs make this concrete by looking at a possible model for predicting house prices:

$$
f_{w, b}(X) = 0.1 X_1 + 4 X_2 + 10 X_3 - 2 X_4 + 80
$$

### Interpreting the Parameters

Each parameter **$w_j$** has a specific meaning:

- **$w_1 = 0.1$**: For every additional square foot, the house price increases by **$100** (since 0.1 times $1,000 equals $100).
- **$w_2 = 4$**: Each additional bedroom adds **$4,000** to the price.
- **$w_3 = 10$**: Each additional floor adds **$10,000**.
- **$w_4 = -2$**: For each additional year of age, the price **decreases** by **$2,000**.
- **$b = 80$**: The base price starts at **$80,000** (when all features are zero).

So, if you have **n** features, the model is written as:

$$
f_{w, b}(X) = w_1 X_1 + w_2 X_2 + ... + w_n X_n + b
$$

## A More Compact Notation

To simplify things, let‚Äôs introduce **vector notation**. Instead of writing out each **$w_j$** and **$X_j$** individually, we‚Äôll collect all the **$w_j$'s** into a vector **$W$**. This vector contains the parameters **$w_1, w_2, ..., w_n$**. Similarly, we‚Äôll represent the features **$X_1, X_2, ..., X_n$** as a vector **$X$**.

Now, we can rewrite the model in a more compact way:

$$
f_{{\vec W}, {b}}(\vec X) = \vec W \cdot \vec X + b
$$

### What Is a Dot Product?

The **dot product** is a mathematical operation that takes two vectors and multiplies their corresponding elements. After multiplying the elements, you **sum** the results. So:

$$
\vec W \cdot \vec X = w_1 X_1 + w_2 X_2 + ... + w_n X_n
$$

Then, we add the bias term **$b$** to get the final prediction.

# What‚Äôs Next: Vectorization

Now that you understand multiple linear regression, the next step is to **optimize** how we implement it. There's a neat technique called **vectorization** that will make your code shorter, faster, and more efficient. It‚Äôs especially useful when working with large datasets and complex algorithms.

## Vectorization: The Key to Faster and More Efficient Machine Learning Code

When implementing a machine learning algorithm, one powerful technique you should know about is **vectorization**. Using vectorization will not only make your code **shorter** and **easier to read**, but it will also make it run **much faster**. This is because vectorization allows you to take advantage of optimized numerical linear algebra libraries and even specialized hardware like **GPUs** (Graphics Processing Units), which are designed to handle large amounts of data quickly.

Let‚Äôs break down what vectorization means and how it works with a concrete example.

> **Note** : You may have heard of ***GPU, NVIDIA, CUDA***.   
> - **GPU** stands for Graphic Processing Unit, which is a specialized processor originally designed to accelerate rendering of complex graphics, especially for gaming and visual applications. However, GPUs are now widely used for parallel computing tasks due to their ability to perform many operations simultaneously.
> - **NVIDIA** is a leading company that designs and manufactures GPUs. They are well-known for their contributions to both gaming and general-purpose computing on GPUs (GPGPU), which includes applications like machine learning, scientific computation, and AI.
> - **CUDA**, which stands for Compute Unified Device Architecture, is a parallel computing platform and programming model created by NVIDIA. CUDA also makes it easy for developers to take advantage of all the latest GPU architecture innovations. 

## Example: Compute a Prediction with Parameters

Imagine you have a model with parameters **$W$** and **$b$**:

- **$\vec W$** is a vector of numbers, let‚Äôs say it has three values: **$w_1$**, **$w_2$**, and **$w_3$**.
- **$\vec X$** is another vector of features, also with three values: **$X_1$**, **$X_2$**, and **$X_3$**.

In this case, **n = 3**, meaning both **$\vec W$** and **$\vec X$** have 3 components. If you're writing the code in **Python**, you can use the popular **NumPy** library, which is great for numerical operations(And you already know about it from the previous chapter). Here's an example of how you might define these vectors in Python:

```python
import numpy as np
w = np.array([w1, w2, w3])
x = np.array([x1, x2, x3])
b = 50
```

## Non-Vectorized Implementation

Now, let‚Äôs compute the prediction without using vectorization. In this case, for each $X_j$, you multiply it by the corresponding $X_j$ and sum the results. This can be done manually:

$$
f = w_0 \cdot X_0 + w_1 \cdot X_1 + w_2 \cdot X_2 + b
$$

But what if n is not 3? What if it's 1,000, or 100,000? Writing all those terms would be inefficient, not just for you as a coder, but also for the computer to process.

A more flexible way to do this without vectorization is by using a for loop:

```python
f = 0
for j in range(3):  
    f += w[j] * X[j]
f += b 
```

This code works for any **n**, but it's still not the most efficient. When **n** gets large, the computation slows down.

## Introducing Vectorization

To speed things up, we can use vectorization. In mathematical terms, we are performing a dot product between the vectors $W$ and $X$, and then adding the bias $b$.

The formula looks like this:

 $$ùëì_{W,X}=W \cdot X +ùëè$$

But in Python, with NumPy, you can compute this in one line of code using the np.dot() function:

```python
f = np.dot(W, X) + b
```

This is the vectorized implementation of the same operation. It‚Äôs shorter and more efficient!

## Why Vectorization is Faster

So, why is this single line of vectorized code so much faster than the for-loop code? The secret lies in how computers handle these operations behind the scenes. When you use vectorized functions like `np.dot()`, NumPy is able to take advantage of parallel processing.

Think of it like this: if you were trying to build a house, doing all the work yourself (sequentially) would take much longer than if you had a team of workers (parallel processing) each handling different tasks at the same time.

Even though your code is shorter, the real speedup comes from your computer‚Äôs ability to run multiple operations simultaneously. This is especially true if you're using a ***GPU***. ***GPUs*** were originally designed for rendering graphics quickly, but their architecture also makes them very good at performing many mathematical operations in parallel, like those needed for vectorized computations.

## Recap: The Benefits of Vectorization

1. Shorter Code: With vectorization, you can replace a long, complex loop with a single, simple line of code. This makes your code easier to write and much more readable.

2. Faster Execution: Vectorized code runs much faster, especially for large datasets. This is because modern libraries like NumPy are optimized to take full advantage of your computer‚Äôs parallel processing capabilities.

3. Scalability: When n is small, the performance difference might not be noticeable. But as n grows, the performance boost from vectorization becomes significant.

## What Happens Behind the Scenes?

Let‚Äôs first look at how a **for-loop** operates without vectorization. Imagine we‚Äôre iterating through 16 elements, from **j = 0** to **j = 15**. Without vectorization, the loop processes one element at a time. Here‚Äôs how it works in steps:

- At **time-step $t_0$**, it processes the first element (index 0).
- At the next time-step, it moves to the next element (index 1), and so on.
- This continues until the 16th step when the final element is processed.

In other words, it computes everything **one step at a time**.

Now, let‚Äôs compare that with **vectorized code**. In a vectorized implementation, the computer can grab **all the elements of the vectors** (say, **$W$** and **$X$**) and compute their values **in parallel**. The computer multiplies each pair of **$W$** and **$X$** elements at the same time. After that, it efficiently sums these values using specialized hardware.

This is the key reason why vectorized code is so much faster. Instead of processing each operation **sequentially**, the computer uses parallel processing to handle **all** operations in one step, making the entire process much quicker.

### Why Does This Matter?

You might be thinking, "Why does this matter?" Well, it matters **a lot**, especially when you‚Äôre dealing with **large datasets** or **complex machine learning models**. Training these models requires a lot of calculations, and without vectorization, those calculations can take hours or even days or **forever**! But with vectorization, those same calculations might only take minutes!

That‚Äôs why using vectorization is essential to building efficient machine learning algorithms that can scale well to handle large amounts of data.

## Vectorization in Action: Linear Regression with Multiple Features

Let‚Äôs now look at a **real-world example**: implementing multiple linear regression using vectorization. 

Imagine you have a dataset with **16 input features** and corresponding parameters **$w_1$ through $w_16$** (the weights), as well as a parameter **$b$** (the bias).

### Non-Vectorized Approach

Without vectorization, you might compute the updates for each of the 16 parameters manually. For example, you would($d_j$ is derivative of the cost function):

- Update **$w_1$** as: $w_1$ = $w_1$ - 0.1 * $d_1$
- Update **$w_2$** as: $w_2$ = $w_2$ - 0.1 * $d_2$
- And so on, until you reach **$w_{16}$**.

This would be tedious to write and inefficient to run. You might use a **for-loop** to handle this:

```python
for j in range(16):  
    w[j] = w[j] - 0.1 * d[j]
```

This loop calculates each update step sequentially, which can slow things down if you have a lot of features or data.

### Vectorized Approach

Now let‚Äôs look at the vectorized version. Instead of looping through each element, you can perform the entire operation in one step by writing:

```python
w = w - 0.1 * d
```

Here, $\vec W$ and $\vec d$ are both NumPy arrays, and the code runs much faster because it uses the computer‚Äôs parallel processing capabilities. Instead of updating each weight individually, the computer updates all 16 weights in parallel.

This difference in performance might not seem like a big deal for just 16 features, but imagine again you had thousands of features or were working with a large dataset. The speedup from vectorization could mean the difference between your code running in minutes or hours.

# Implementing Gradient Descent for Multiple Linear Regression with Vectorization

Now that you‚Äôve learned about **gradient descent**, **multiple linear regression**, and **vectorization**, it‚Äôs time to bring all of these concepts together to implement **gradient descent for multiple linear regression using vectorization**.

## A Quick Review of Multiple Linear Regression

Previously, we talked about multiple linear regression as having parameters **$w_1$** through **$w_n$** and a bias term **$b$**. Instead of thinking of **$w_1$** through **$w_n$** as separate parameters, we can **collect all the $w$ s into a vector**, which we‚Äôll call **$\vec W$**. So, **$\vec W$** is now a vector of length **n** (where **n** is the number of features). The bias term **$b$** remains a single number.

Now, using vector notation, we can rewrite the model as:

$$
f_{\vec W,b}(\vec X) = \vec W \cdot \vec X + b
$$

Here, the "dot" symbol (¬∑) represents the **dot product** between the vector **$\vec W$** and the vector **$\vec X$** (the features).

## Vectorized Cost Function

Our cost function, previously written as $J(w_1, w_2, ..., w_n, b)$, can now be written more compactly as $J_{\vec W, b}$. Instead of **n** separate parameters, we now have a single vector **$\vec W$** and the number **$b$**. The function $J_{\vec W, b}$ still outputs a single number (the cost), but it now takes a vector as input.

## Gradient Descent for Multiple Features

Here‚Äôs what **gradient descent** looks like for multiple linear regression:

- Repeatedly update each parameter **$w_j$** using the update rule:

$$
w_j = w_j - \alpha \cdot \frac{\partial J_{\vec W, b}}{\partial w_j}
$$

$$
b = b - \alpha \cdot \frac{\partial J_{\vec W, b}}{\partial b}
$$

Where **Œ±** is the learning rate, and $\frac{\partial J_{\vec W, b}}{\partial w_j}$ is the derivative of the cost function **J** with respect to **$w_j$**.

## Derivatives for Gradient Descent

Let‚Äôs look at how the derivative terms work for multiple features. When we had only **one** feature, the update rule for **w** looked like this:

$$
w = w - \alpha \cdot \frac{\partial J}{\partial w}
$$

Now that we have **multiple features**, the update rule becomes:

$$
w_j = w_j - \alpha \cdot \frac{\partial J}{\partial w_j}
$$

For each **j** (from 1 to **n**), we need to update each **$w_j$** using its corresponding derivative. The main difference is that **$\vec W$** and **$\vec X$** are vectors. 

The general rule is that we update **all parameters $w_1$, $w_2$, ..., $w_n$** and then update **$b$**, the bias term, in each iteration of gradient descent.

## Vectorization in Gradient Descent

With **vectorization**, we don‚Äôt need to update each **$w_j$** one at a time. Instead, we can use vectorized operations to update **all the weights** at once. This makes the code more concise and much faster to run.

In **vectorized form**, the gradient descent update rule for all **$w$** becomes:

$$
w_n = w_n - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( f_{\vec{w}, b}(\vec{x}^{(i)}) - y^{(i)} \right) x_n^{(i)}
$$

And for **b**, the update is similar:

$$
b = b - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( f_{\vec{w}, b}(\vec{x}^{(i)}) - y^{(i)} \right)
$$

Simultaneously update **$w_j$ (for j = 1, ...,  n)** and **$b$** in each iteration.  

## The Normal Equation: An Alternative to Gradient Descent

Before we move on, it‚Äôs important to note that there‚Äôs another method for finding **w** and **b** in linear regression, called the **normal equation**. While **gradient descent** is a powerful method for minimizing the cost function $J_{\vec W, b}$, the **normal equation** offers a different approach.

The **normal equation** is a formula that allows us to solve for **$w$** and **$b$** in one step, without using an iterative process like gradient descent. This is possible because linear regression has an exact solution that can be found using advanced linear algebra techniques.

Using a **linear algebra library**, we can compute **$w$** and **$b$** directly by solving the normal equation.

However, the **normal equation** has some limitations:

- It only works for **linear regression**. It **cannot** be generalized to other algorithms like **logistic regression** or **neural networks**.
- The normal equation is slow when the number of features is very large. This is because solving it involves inverting a matrix, which becomes computationally expensive as the number of features grows.

## Feature Scaling and Gradient Descent: Why Scaling Matters

When building machine learning models, one common challenge is handling features that have very different ranges of values. This can cause problems with optimization methods like gradient descent. So, we'll talk about a technique called **feature scaling** that speeds up gradient descent and makes it more effective.

### A Real-World Example: Predicting House Prices

Imagine you're trying to predict the price of a house. Two features you might use are:

- **$x_1$**: The size of the house in square feet (usually ranges from 300 to 2,000 $feet^2$).
- **$x_2$**: The number of bedrooms (ranges from 0 to 5 bedrooms).
- **$y$**: The price of the house (in dollars).

Then, you can write the model as:

$$
\hat {price} = w_1 x_1 + w_2 x_2 + b
$$

Now, consider a house with:

- 2,000 square feet,
- 5 bedrooms,
- a price of $500,000(=$500K)

In machine learning, we want to find the best values for the parameters (weights) associated with these features, say **$w_1$** (for square feet) and **$w_2$** (for bedrooms), as well as the bias **$b$**. These parameters help the model predict house prices based on the features.

Let's look at two possible sets of parameters:

1. **$w_1$ = 50**, **$w_2$ = 0.1**, and **$b$ = 50**.

    In this case, the model predicts:

    $\hat {price}$ = 50 * 2,000 ($feet^2$) + 0.1 * 5 (bedrooms) + 50
   
    This results in a predicted price of **$100,050.5**, which is **way off** from the actual price of $500,000. Clearly, this isn't a good set of parameters.

2. **$w_1$ = 0.1**, **$w_2$ = 50**, and **$b$ = 50**.

    Now the model predicts:

    $\hat {price}$ = 0.1 * 2,000 ($feet^2$) + 50 * 5 (bedrooms) + 50
    
    This results in a predicted price of **$500,000**, which is the correct price. In this case, the parameter values seem much more reasonable.

### Understanding Why This Happens

When you look at the range of the features, you‚Äôll notice that **$x_1$** (size in square feet) can have very large values, up to 2,000, while **$x_2$** (number of bedrooms) has much smaller values, between 0 and 5.

Because **$x_1$** is large, the model needs a **small** parameter value, like 0.1, to avoid making overly large predictions. On the other hand, **$x_2$** is much smaller, so the model needs a **large** parameter value, like 50, to have a noticeable impact on the prediction.

### Gradient Descent and Feature Scaling

Now, let‚Äôs connect this to gradient descent. Gradient descent is an optimization algorithm that tries to minimize the cost function, which tells us how bad the model‚Äôs predictions are(or how good). However, when the features have very different ranges of values, gradient descent can struggle.

Imagine a scatter plot where the **x-axis** represents **$x_1$** (square feet) and the y-axis represents **$x_2$** (number of bedrooms). The values of **$x_1$** are spread out much more widely than those of **$x_2$**. 

As a result, the cost function‚Äôs contour plot looks like an **elongated oval** rather than a **circle**. This creates a problem for gradient descent.

Here‚Äôs a metaphor to help explain: Imagine you‚Äôre hiking in a narrow valley with steep sides. Instead of walking straight downhill, you might zigzag back and forth, making slow progress. This is what gradient descent does when the features aren‚Äôt scaled properly‚Äîit "zigzags" a lot and takes a long time to reach the bottom, or the optimal solution.

## Feature Scaling: Leveling the Playing Field

The solution to this problem is **feature scaling**. When we scale the features, we transform them so that they all take on similar ranges of values. 

For example, we can rescale **$x_1$** (size in square feet) and **$x_2$** (number of bedrooms) to both range from **0 to 1**. After scaling, the cost function‚Äôs contour plot looks more like a **circle**. In this case, gradient descent can take a more direct path to the minimum, making it **much faster and more efficient**.

And you can think of it like this: when the features are scaled, the valley becomes wider and shallower, making it easier to walk straight downhill. 

In other words, computer can handle the data more efficiently when the data is divided into smaller chunks.

### How to Perform Feature Scaling

There are several ways to scale your features:

- **Normalization**: Rescale the values of each feature to range between 0 and 1.
- **Standardization**: Transform the features so they have a mean of 0 and a standard deviation of 1.

> **Note**:  
> ***Normalization*** is very important in ***Deep Learning***, and also in our life! When we observe the data, events, or anything, we need to normalize them to understand them better. We normalize everything in our life to make them easier to understand. For example, when we see the temperature, We can say that the temperature is hot or cold. However, we need to **denormalize** to get the exact temperature. **Denormalization** is an important skill we should learn: we need to  simplify things to make sense of them, but we also need to go back to the real values to understand them fully.

By scaling your features, you can avoid the slow zigzagging of gradient descent and reach the best solution faster.

One simple way to scale features is to **divide by the maximum value** of each feature. For instance:

- **Scaling $x_1$**: If **$x_1$** ranges from 3 to 2,000, you can divide each value of **$x_1$** by 2,000 (the maximum value). The scaled **$x_1$** will now range from approximately **0.0015 to 1**.
  
- **Scaling $x_2$**: Since **$x_2$** ranges from 0 to 5, you can divide each value of **$x_2$** by 5. The scaled **$x_2$** will now range from **0 to 1**.

If you plot the scaled **$x_1$** and **$x_2$** on a graph, the data points will now appear more **compressed** and will have similar ranges, making gradient descent more efficient.

### Mean Normalization

Another common method is **mean normalization**. Instead of just dividing by the maximum value, we can **center the features around zero**, so they have both **negative and positive values**. This often makes gradient descent work even better.

Here‚Äôs how mean normalization works:

1. **Calculate the mean (average)** of each feature. Let‚Äôs call the mean of **$x_1$** as **$\mu_1$**.
2. **Subtract the mean** from each value of **$x_1$**, and then divide by the range of the feature (the maximum minus the minimum). For instance:

$$
x_1^{\text{normalized}} = \frac{x_1 - \mu_1}{2000 - 300}
$$

If **$\mu_1$** is 600, then the mean normalized **$x_1$** will now range from **-0.18 to 0.82**.

For **$x_2$**, you would do the same:

$$
x_2^{\text{normalized}} = \frac{x_2 - \mu_2}{5 - 0}
$$

If **$\mu_2$** is 2.3, the normalized **$x_2$** will range from **-0.46 to 0.54**.

Now, when you plot the data using the mean normalized features, the values will be centered around zero.

### Z-Score Normalization

Another useful scaling technique is called **Z-score normalization**. This method uses the **mean** and **standard deviation** of each feature to rescale it. If you‚Äôve heard of the **normal distribution** or the **bell curve**, Z-score normalization is based on similar principles.

Here‚Äôs how you perform Z-score normalization:

1. **Calculate the mean ($\mu$)** and the **standard deviation ($\sigma$)** for each feature.
2. For each value of **$x_1$**, subtract the mean and divide by the standard deviation:

$$
x_1^{\text{z-score}} = \frac{x_1 - \mu_1}{\sigma_1}
$$

For example, if **$\mu_1 = 600$** and **$\sigma_1 = 450$**, the Z-score normalized **$x_1$** will now range from **-0.67 to 3.1**.

Similarly, for **$x_2$**, if **$\mu_2 = 2.3$** and **$\sigma_2 = 1.4$**, the normalized **$x_2$** will range from **-1.6 to 1.9**.

## Practical Tips for Feature Scaling

When you perform feature scaling, it‚Äôs usually a good idea to aim for a range between **-1 and +1** for each feature, though this range can vary slightly. For instance:

- If a feature ranges from **-0.3 to +0.3**, that‚Äôs perfectly fine.
- If a feature ranges from **0 to 3**, it‚Äôs also okay, though you might still choose to rescale it if needed.
- However, if a feature has a very large range, like **-100 to +100**, it‚Äôs usually a good idea to rescale it to something closer to **-1 to +1**.

## Examples of Feature Ranges

Let‚Äôs consider a few examples to illustrate when you should re-scale:

- **$x_3$**: Suppose **$x_3$** has values between **-100 and +100**. This is a wide range compared to other features, so you‚Äôd likely want to rescale it to make it comparable to the others.
  
- **$x_4$**: If **$x_4$** has very small values, such as **-0.001 to 0.001**, this is also a case where rescaling would help.

- **$x_5$**: Suppose **$x_5$** is a feature like body temperature, which ranges from **98.6 to 105 degrees Fahrenheit**. Although these numbers are fairly close to each other, they‚Äôre still large relative to scaled features, and this could cause **gradient descent** to run slowly. Rescaling **$x_5$** would likely improve performance.

## Should We Always Scale Features?

In general, there‚Äôs almost **no harm** in scaling features. If you‚Äôre unsure whether or not to scale, it‚Äôs usually better to just **go ahead and do it**. Feature scaling can significantly improve the speed of gradient descent, especially when your features have very different ranges.

However, there are a few cases where you might not need to scale features(You have learned these concepts in the previous chapter, ***Data Analysis with Python***, so you should now be able to grasp them easily):

- **Binary Features**: If a feature is binary (0 or 1), you don‚Äôt need to scale it. Binary features are already in a consistent range.

- **Ordinal Features**: If a feature is ordinal (like a rating from 1 to 5), you might not need to scale it. Ordinal features are often already in a consistent range.

- **Categorical Features**: If a feature is categorical (like colors or cities), you might not need to scale it. Categorical features are usually encoded as binary variables, so they don‚Äôt need scaling.