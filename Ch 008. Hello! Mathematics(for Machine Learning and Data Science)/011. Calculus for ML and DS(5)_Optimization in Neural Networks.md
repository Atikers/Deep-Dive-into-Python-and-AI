# Calculus for ML and DS (5) - Optimization in Neural Networks

## Regression with a Perceptron

> **Have you ever wondered how a simple “brain cell” in a neural network can predict something like house prices or car values?** 

A **perceptron**—the fundamental building block of neural networks—can do exactly that. Let’s see how.

### What Is a Perceptron?

A **perceptron** is essentially a **linear model** that combines multiple inputs to produce a single output. In a simple regression setting with two features, such as:
- $x_1$: the **size** of a house,
- $x_2$: the **number of rooms**,

our perceptron’s output (prediction) $\hat{y}$ is:  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

where:
- $w_1$ and $w_2$ are **weights** that tell us how important each feature is,
- $b$ is a **bias** term, letting us shift the overall prediction up or down.

### Why Is It Called a “Perceptron”?
Historically, a perceptron was one of the earliest neural network models—initially used for classification. But for **regression**, the same idea applies: a weighted sum of inputs plus a bias.

### Regression Example

Consider predicting **house prices**:

1. **Inputs**: 
   - $x_1$: Size (square feet).
   - $x_2$: Number of rooms.

2. **Output** - $\hat{y}$: The predicted **price** of the house.

3. **Parameters** - $w_1, w_2, b$: Unknown numbers we want to find so that $\hat{y}$ is close to the **true** price $y$.

#### Expanding to More Features
If you have more features—like the house’s location, age, nearby schools, etc.—the formula simply adds more weights and inputs:  

$$
\hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
$$

But the same principle applies.

### The Loss Function
How do we measure if our perceptron’s predictions are “good”? We use a **loss function**. A common choice for regression is the **mean squared error**. For one data point:  

$$
L(y, \hat{y}) = \tfrac{1}{2}(y - \hat{y})^2
$$

- $(y - \hat{y})$ is the **error** (difference between the true value and the predicted value).
- We **square** it to keep errors positive.
- The factor $\tfrac{1}{2}$ is just for easier math when taking derivatives.

#### Multiple Data Points
If you have multiple training examples, you average or sum these loss terms over all examples. Minimizing that overall loss forces the perceptron to produce outputs $\hat{y}$ that closely match the real values $y$.

### Finding the Best $w_1, w_2, b$

#### Gradient Descent

To **optimize** the perceptron’s parameters - weights $w_1, w_2$ and bias $b$, we want to **minimize** the total loss:  

$$
J(w_1, w_2, b) = \sum_{i}(y^{(i)} - \hat{y}^{(i)})^2 \quad \text{(or the average of such terms).}
$$

1. **Initialize** random guesses for $w_1, w_2, b$.
2. **Compute gradients** (partial derivatives) of the loss with respect to each parameter.
3. **Update** each parameter by moving opposite the gradient:  

$$
w_1 \leftarrow w_1 - \alpha \frac{\partial J}{\partial w_1}, \quad
w_2 \leftarrow w_2 - \alpha \frac{\partial J}{\partial w_2}, \quad
b \leftarrow b - \alpha \frac{\partial J}{\partial b},
$$

- where $\alpha$ is the **learning rate** (how big a step you take each time).
4. **Repeat** until the changes in $w_1, w_2, b$ are very small or you reach a maximum number of steps.

### Putting It All Together
- **Single-Layer Neural Network**: A “perceptron” for regression is basically a linear combination of inputs plus a bias.
- **Loss Function**: $\tfrac{1}{2}(y - \hat{y})^2$ measures how far off predictions are from actual values.
- **Goal**: Tune $(w_1, w_2, b)$ to minimize the sum of those squared errors. 
- **Method**: Gradient descent (or other optimization techniques) iteratively refines the parameters.

### Why It Matters
This perceptron concept extends easily. For deeper or more complex neural networks, each hidden neuron does a similar process—just with more steps and possibly **activation functions**. But the core idea of **weights + bias** plus **gradient-based optimization** remains.

### Key Takeaways
1. **Perceptron for Regression**: 
   - Prediction formula: $\hat{y} = w_1 x_1 + w_2 x_2 + b$.
   - Geometrically, it’s a plane (in 2D inputs) or a hyperplane (in higher dimensions).
2. **Mean Squared Error**:
   - $\tfrac{1}{2}(y - \hat{y})^2$ is a simple, common choice for measuring “wrongness”.
3. **Gradient Descent**:
   - Update $(w_1, w_2, b)$ by following the negative gradient, which points us downhill in loss-land.
4. **Scalability**:
   - Real neural networks have many layers and parameters, but the principle—“take a linear combination, apply an activation, repeat, then do gradient descent”—stays the same.

---

## **Regression with a Perceptron – Deep Dive into Gradient Descent**

You already know that **gradient descent** is a key method to find parameters that minimize a loss function. Because the perceptron for regression (with two inputs) is a straightforward case, it’s a good way to see how each partial derivative fits together. 

### The Setup: A Two-Input Perceptron

#### Prediction Function

A **perceptron** with two features $x_1, x_2$ predicts an output $\hat{y}$ as:  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

where:
- $w_1, w_2$ are **weights** for each feature,
- $b$ is the **bias** term (sometimes noted as $w_0$).

### Loss Function
We measure the error between $\hat{y}$ and the true value $y$. A common choice is the **mean squared error**. For a single data point:  

$$
L(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2
$$

Multiplying by $\frac{1}{2}$ doesn’t change the minimization, but it simplifies derivatives by canceling out a factor of 2 later.

### Goal: Minimize the Loss
We want to find $w_1, w_2, b$ that give the **smallest average loss** over all data points. The tool? **Gradient Descent.**

### Gradient Descent Update Equations
Recall the general form for each parameter $\theta$:  

$$
\theta \leftarrow \theta - \alpha \frac{\partial L}{\partial \theta}
$$

where $\alpha$ is the **learning rate**.

In our case, we have three parameters: $w_1, w_2, b$. So we need:  

$$
w_1 \leftarrow w_1 - \alpha \frac{\partial L}{\partial w_1}, \quad
w_2 \leftarrow w_2 - \alpha \frac{\partial L}{\partial w_2}, \quad
b \leftarrow b - \alpha \frac{\partial L}{\partial b}
$$

### Finding the Partial Derivatives

#### Chain Rule Overview
To get, for instance, $\frac{\partial L}{\partial w_1}$, we note:
1. $L$ depends on $\hat{y}$ via $\frac{1}{2}(y - \hat{y})^2$.
2. $\hat{y}$ depends on $w_1$ via $\hat{y} = w_1 x_1 + w_2 x_2 + b$.

Hence,  

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1}
$$

Similarly for $w_2$ and $b$.

#### Individual Derivatives

1. **$\frac{\partial L}{\partial \hat{y}}$**  
- From $L = \tfrac{1}{2}(y - \hat{y})^2$,  

$$
\frac{\partial L}{\partial \hat{y}} = \tfrac{1}{2} \cdot 2 \cdot (y - \hat{y}) \cdot \bigl(-1\bigr) = -(y - \hat{y})
$$

- because, inner derivative: $\frac{\partial (y - \hat{y})}{\partial \hat{y}} = -1$, and outer derivative: $\frac{\partial (y - \hat{y})^2}{\partial (y - \hat{y})} = 2(y - \hat{y})$

2. **$\frac{\partial \hat{y}}{\partial b}$**
- Since $\hat{y} = w_1 x_1 + w_2 x_2 + b$,  

$$
\frac{\partial \hat{y}}{\partial b} = 1
$$

- because, except for the bias term, all other terms are considered as constants.

3. **$\frac{\partial \hat{y}}{\partial w_1}$**  

$$
\hat{y} = w_1 x_1 + \dots, \text{so with respect to } w_1
$$  

$$
\frac{\partial \hat{y}}{\partial w_1} = x_1
$$

- because, except for the bias term, all other terms are considered as constants.

4. **$\frac{\partial \hat{y}}{\partial w_2}$**  
- Similarly,  

$$
\frac{\partial \hat{y}}{\partial w_2} = x_2
$$

### Putting It Together

1. **$\frac{\partial L}{\partial b}$**
Using the chain rule:  

$$
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial b} = [-(y - \hat{y})] \times 1 = -(y - \hat{y})
$$

2. **$\frac{\partial L}{\partial w_1}$**
Using the chain rule:  

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1} = [-(y - \hat{y})] \times x_1 = -(y - \hat{y}) x_1
$$

3. **$\frac{\partial L}{\partial w_2}$**

$$
\frac{\partial L}{\partial w_2} = [-(y - \hat{y})] \times x_2 = -(y - \hat{y}) x_2
$$

### Gradient Descent Updates
Finally, we can **update** each parameter:

$$
\begin{aligned}
w_1 &\leftarrow w_1 - \alpha [-(y - \hat{y}) x_1] = w_1 + \alpha (y - \hat{y}) x_1\\
w_2 &\leftarrow w_2 - \alpha [-(y - \hat{y}) x_2] = w_2 + \alpha (y - \hat{y}) x_2\\
b &\leftarrow b - \alpha [-(y - \hat{y})] = b + \alpha (y - \hat{y})
\end{aligned}
$$

If you have **many training examples**, you’d sum (or average) these gradients across all data points, then perform the update. Repeating these steps gradually drives the model to predict more accurately.

## Key Takeaways

1. **Chain Rule**: Splitting $\frac{\partial L}{\partial w_i}$ into $\frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_i}$ simplifies the math.
2. **Same Pattern**: Each parameter update “pulls” the weights in a direction that reduces the error $(y - \hat{y})$.
3. **Scalable**: Although we showed just two inputs, the same approach extends to many inputs. Neural networks do this for **every** layer and **thousands** (or millions) of parameters.
4. **Iterations**: You usually repeat gradient descent updates until either the error is minimal or you’ve reached a set number of steps. Each epoch (loop through your dataset) refines $w_1, w_2, b$.

With these derivatives and the gradient descent formula in hand, you can train a perceptron to **minimize errors**—and that’s exactly how deeper neural networks function at each layer!