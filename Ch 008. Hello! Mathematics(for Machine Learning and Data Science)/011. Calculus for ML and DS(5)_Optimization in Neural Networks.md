# Calculus for ML and DS (5) - Optimization in Neural Networks

## Regression with a Perceptron

> **Have you ever wondered how a simple “brain cell” in a neural network can predict something like house prices or car values?** 

A **perceptron**—the fundamental building block of neural networks—can do exactly that. Let’s see how.

### What Is a Perceptron?

A **perceptron** is essentially a **linear model** that combines multiple inputs to produce a single output. In a simple regression setting with two features, such as:
- $x_1$: the **size** of a house,
- $x_2$: the **number of rooms**,

our perceptron’s output (prediction) $\hat{y}$ is:  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

where:
- $w_1$ and $w_2$ are **weights** that tell us how important each feature is,
- $b$ is a **bias** term, letting us shift the overall prediction up or down.

### Why Is It Called a “Perceptron”?
Historically, a perceptron was one of the earliest neural network models—initially used for classification. But for **regression**, the same idea applies: a weighted sum of inputs plus a bias.

### Regression Example

Consider predicting **house prices**:

1. **Inputs**: 
   - $x_1$: Size (square feet).
   - $x_2$: Number of rooms.

2. **Output** - $\hat{y}$: The predicted **price** of the house.

3. **Parameters** - $w_1, w_2, b$: Unknown numbers we want to find so that $\hat{y}$ is close to the **true** price $y$.

#### Expanding to More Features
If you have more features—like the house’s location, age, nearby schools, etc.—the formula simply adds more weights and inputs:  

$$
\hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
$$

But the same principle applies.

### The Loss Function
How do we measure if our perceptron’s predictions are “good”? We use a **loss function**. A common choice for regression is the **mean squared error**. For one data point:  

$$
L(y, \hat{y}) = \tfrac{1}{2}(y - \hat{y})^2
$$

- $(y - \hat{y})$ is the **error** (difference between the true value and the predicted value).
- We **square** it to keep errors positive.
- The factor $\tfrac{1}{2}$ is just for easier math when taking derivatives.

#### Multiple Data Points
If you have multiple training examples, you average or sum these loss terms over all examples. Minimizing that overall loss forces the perceptron to produce outputs $\hat{y}$ that closely match the real values $y$.

### Finding the Best $w_1, w_2, b$

#### Gradient Descent

To **optimize** the perceptron’s parameters - weights $w_1, w_2$ and bias $b$, we want to **minimize** the total loss:  

$$
J(w_1, w_2, b) = \sum_{i}(y^{(i)} - \hat{y}^{(i)})^2 \quad \text{(or the average of such terms).}
$$

1. **Initialize** random guesses for $w_1, w_2, b$.
2. **Compute gradients** (partial derivatives) of the loss with respect to each parameter.
3. **Update** each parameter by moving opposite the gradient:  

$$
w_1 \leftarrow w_1 - \alpha \frac{\partial J}{\partial w_1}, \quad
w_2 \leftarrow w_2 - \alpha \frac{\partial J}{\partial w_2}, \quad
b \leftarrow b - \alpha \frac{\partial J}{\partial b},
$$

- where $\alpha$ is the **learning rate** (how big a step you take each time).
4. **Repeat** until the changes in $w_1, w_2, b$ are very small or you reach a maximum number of steps.

### Putting It All Together
- **Single-Layer Neural Network**: A “perceptron” for regression is basically a linear combination of inputs plus a bias.
- **Loss Function**: $\tfrac{1}{2}(y - \hat{y})^2$ measures how far off predictions are from actual values.
- **Goal**: Tune $(w_1, w_2, b)$ to minimize the sum of those squared errors. 
- **Method**: Gradient descent (or other optimization techniques) iteratively refines the parameters.

### Why It Matters
This perceptron concept extends easily. For deeper or more complex neural networks, each hidden neuron does a similar process—just with more steps and possibly **activation functions**. But the core idea of **weights + bias** plus **gradient-based optimization** remains.

### Key Takeaways
1. **Perceptron for Regression**: 
   - Prediction formula: $\hat{y} = w_1 x_1 + w_2 x_2 + b$.
   - Geometrically, it’s a plane (in 2D inputs) or a hyperplane (in higher dimensions).
2. **Mean Squared Error**:
   - $\tfrac{1}{2}(y - \hat{y})^2$ is a simple, common choice for measuring “wrongness”.
3. **Gradient Descent**:
   - Update $(w_1, w_2, b)$ by following the negative gradient, which points us downhill in loss-land.
4. **Scalability**:
   - Real neural networks have many layers and parameters, but the principle—“take a linear combination, apply an activation, repeat, then do gradient descent”—stays the same.

---

## **Regression with a Perceptron – Deep Dive into Gradient Descent**

You already know that **gradient descent** is a key method to find parameters that minimize a loss function. Because the perceptron for regression (with two inputs) is a straightforward case, it’s a good way to see how each partial derivative fits together. 

### The Setup: A Two-Input Perceptron

#### Prediction Function

A **perceptron** with two features $x_1, x_2$ predicts an output $\hat{y}$ as:  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

where:
- $w_1, w_2$ are **weights** for each feature,
- $b$ is the **bias** term, sometimes noted as $w_0$.

### Loss Function
We measure the error between $\hat{y}$ and the true value $y$. A common choice is the **mean squared error**. For a single data point:  

$$
L(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2
$$

Multiplying by $\frac{1}{2}$ doesn’t change the minimization, but it simplifies derivatives by canceling out a factor of 2 later.

### Goal: Minimize the Loss
We want to find $w_1, w_2, b$ that give the **smallest average loss** over all data points. The tool? **Gradient Descent.**

### Gradient Descent Update Equations
Recall the general form for each parameter $\theta$:  

$$
\theta \leftarrow \theta - \alpha \frac{\partial L}{\partial \theta}
$$

where $\alpha$ is the **learning rate**.

In our case, we have three parameters: $w_1, w_2, b$. So we need:  

$$
w_1 \leftarrow w_1 - \alpha \frac{\partial L}{\partial w_1}, \quad
w_2 \leftarrow w_2 - \alpha \frac{\partial L}{\partial w_2}, \quad
b \leftarrow b - \alpha \frac{\partial L}{\partial b}
$$

### Finding the Partial Derivatives

#### Chain Rule Overview
To get, for instance, $\frac{\partial L}{\partial w_1}$, we note:
1. $L$ depends on $\hat{y}$ via $\frac{1}{2}(y - \hat{y})^2$.
2. $\hat{y}$ depends on $w_1$ via $\hat{y} = w_1 x_1 + w_2 x_2 + b$.

Hence,  

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1}
$$

Similarly for $w_2$ and $b$.

#### Individual Derivatives

1. **$\frac{\partial L}{\partial \hat{y}}$**  
- From $L = \tfrac{1}{2}(y - \hat{y})^2$,  

$$
\frac{\partial L}{\partial \hat{y}} = \tfrac{1}{2} \cdot 2 \cdot (y - \hat{y}) \cdot \bigl(-1\bigr) = -(y - \hat{y})
$$

- because, inner derivative: $\frac{\partial (y - \hat{y})}{\partial \hat{y}} = -1$, and outer derivative: $\frac{\partial (y - \hat{y})^2}{\partial (y - \hat{y})} = 2(y - \hat{y})$

2. **$\frac{\partial \hat{y}}{\partial b}$**
- Since $\hat{y} = w_1 x_1 + w_2 x_2 + b$,  

$$
\frac{\partial \hat{y}}{\partial b} = 1
$$

- because, except for the bias term, all other terms are considered as constants.

3. **$\frac{\partial \hat{y}}{\partial w_1}$**  

$$
\hat{y} = w_1 x_1 + \dots, \text{so with respect to } w_1
$$  

$$
\frac{\partial \hat{y}}{\partial w_1} = x_1
$$

- because, except for the bias term, all other terms are considered as constants.

4. **$\frac{\partial \hat{y}}{\partial w_2}$**  
- Similarly,  

$$
\frac{\partial \hat{y}}{\partial w_2} = x_2
$$

### Putting It Together

1. **$\frac{\partial L}{\partial b}$**
Using the chain rule:  

$$
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial b} = [-(y - \hat{y})] \times 1 = -(y - \hat{y})
$$

2. **$\frac{\partial L}{\partial w_1}$**
Using the chain rule:  

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1} = [-(y - \hat{y})] \times x_1 = -(y - \hat{y}) x_1
$$

3. **$\frac{\partial L}{\partial w_2}$**

$$
\frac{\partial L}{\partial w_2} = [-(y - \hat{y})] \times x_2 = -(y - \hat{y}) x_2
$$

### Gradient Descent Updates
Finally, we can **update** each parameter:

$$
\begin{aligned}
w_1 &\leftarrow w_1 - \alpha [-(y - \hat{y}) x_1] = w_1 + \alpha (y - \hat{y}) x_1\\
w_2 &\leftarrow w_2 - \alpha [-(y - \hat{y}) x_2] = w_2 + \alpha (y - \hat{y}) x_2\\
b &\leftarrow b - \alpha [-(y - \hat{y})] = b + \alpha (y - \hat{y})
\end{aligned}
$$

If you have **many training examples**, you’d sum (or average) these gradients across all data points, then perform the update. Repeating these steps gradually drives the model to predict more accurately.

## Key Takeaways

1. **Chain Rule**: Splitting $\frac{\partial L}{\partial w_i}$ into $\frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_i}$ simplifies the math.
2. **Same Pattern**: Each parameter update “pulls” the weights in a direction that reduces the error $(y - \hat{y})$.
3. **Scalable**: Although we showed just two inputs, the same approach extends to many inputs. Neural networks do this for **every** layer and **thousands** (or millions) of parameters.
4. **Iterations**: You usually repeat gradient descent updates until either the error is minimal or you’ve reached a set number of steps. Each epoch (loop through your dataset) refines $w_1, w_2, b$.

With these derivatives and the gradient descent formula in hand, you can train a perceptron to **minimize errors**—and that’s exactly how deeper neural networks function at each layer!

---

## **Classification with Perceptron**

#### Why Classification?

Previously, we saw how a **perceptron** could solve a **regression** problem by outputting a continuous value, like predicting house prices. But many tasks require a **yes/no** (binary) decision—**classification**. For instance, determine whether an alien sentence expresses a “happy” or “sad” mood.

### The Scenario: Alien Language Classification

Imagine an alien language with just two words: “aack” and “beep.” We record several sentences:
- **Sentence 1**: “Aack aack aack!” → **Happy**  
- **Sentence 2**: “Beep beep!” → **Sad**  
- **Sentence 3**: “Aack beep beep beep!” → **Sad**  
- **Sentence 4**: “Aack beep aack!” → **Happy**

To train a model, we need **numeric features**. For each sentence, we count:
- $x_1$ = number of times “aack” appears,
- $x_2$ = number of times “beep” appears.

Then the **target** (label) is:
- **Happy** = 1
- **Sad** = 0

Visually, you might plot each (x1, x2) point in 2D space. Observing which cluster is “happy” vs. “sad” suggests **a line might separate them**. That’s where the **perceptron** concept comes in.

### The Perceptron for Binary Classification

#### Structure
For classification, the perceptron still does a **linear combination** of features:  

$$
z = w_1 x_1 + w_2 x_2 + b
$$

- $w_1$ and $w_2$ are **weights** telling us how important each word (“aack,” “beep”) is.
- $b$ is the **bias** that shifts the decision boundary.

##### The Key Twist: An **Activation Function**

Unlike regression (where the output was $z$ itself), for **binary** outputs in $\{0,1\}$, we need to **squash** $z$ into a value between 0 and 1. We use the **sigmoid** (or **logistic**) function, $\sigma(z)$, defined by:  

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

- If $z$ is large and positive, $\sigma(z) \approx 1$ (model thinks “happy”).
- If $z$ is large and negative, $\sigma(z) \approx 0$ (model thinks “sad”).
- Values near 0.5 mean “unsure.”

Hence the perceptron’s **output** is:  

$$
\hat{y} = \sigma(z)
$$

```
           (w1 * x1 + w2 * x2 + b)
x1 ----->      SUMMATION        -----> z   --> [ Sigmoid ] --> y_hat
x2 ----->        ( + b )        -----^
                     |
                     b
```

### Classification Logic

1. **Feed Forward**:  
   - We take each sentence’s (x1, x2).  
   - Compute $z = w_1 x_1 + w_2 x_2 + b$.  
   - Apply $\hat{y} = \sigma(z)$.

2. **Interpretation**:  
   - $\hat{y}$ $\approx$ 1 → Model predicts **Happy**  
   - $\hat{y}$ $\approx$ 0 → Model predicts **Sad**

#### Deciding Where the “Boundary” Goes
The perceptron effectively tries to learn a **decision boundary** (a line in 2D) that separates happy from sad points. This line is:  

$$
w_1 x_1 + w_2 x_2 + b = 0
$$

to define which side is “happy” $z>0$ vs. “sad” $z<0$.

### Why Use Sigmoid?

- **Maps Real Numbers to [0,1]**: Perfect for probabilities or binary decisions.
- **Smooth & Differentiable**: We’ll rely on gradient descent to tune $w_1,w_2,b$. Sigmoid’s derivative is well-defined, making learning feasible.  

$$
\sigma(z) = \frac{1}{1 + e^{-z}}, \quad
\frac{d\sigma(z)}{dz} = \sigma(z)[1 - \sigma(z)]
$$

This derivative is used in **backpropagation** to update the perceptron’s weights and bias.

### Putting It All Together

1. **Gather Data**: 
   - Each alien sentence → $(x_1, x_2)$.  
   - Label each as 0 (sad) or 1 (happy).
2. **Define Perceptron**: 
   - Weighted sum: $z = w_1 x_1 + w_2 x_2 + b$.  
   - Sigmoid output: $\hat{y} = \sigma(z)$.
3. **Train** (Gradient Descent):
   - Compare $\hat{y}$ to the true label $y$.  
   - Compute a **loss** (e.g. cross-entropy or MSE for classification).  
   - Calculate derivatives w.r.t. $w_1, w_2, b$.  
   - Update each parameter to reduce the loss.
4. **Classify** New Sentences:
   - When $\hat{y}>0.5$, guess “happy”; otherwise, guess “sad.”

### The Sigmoid Function

#### Sigmoid: The Big Picture

When classifying (e.g., predicting “happy” vs. “sad”), we want model outputs between 0 and 1. The **sigmoid** function, also known as the **logistic** function, does exactly that:  

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

- If $z$ is very large (positive), $\sigma(z)\approx 1$.  
- If $z$ is very large (negative), $\sigma(z)\approx 0$.  
- If $z=0$, $\sigma(0) = \tfrac{1}{1+1} = \tfrac{1}{2}$.

Graphically, it’s an S-shaped curve mapping $-\infty$ to 0, and $+\infty$ to 1:

```
z-axis (input):   -∞          0           +∞
σ(z) (output):     0  -----> 0.5  ----->   1
```

#### Why Do We Need the Sigmoid Derivative?

In machine learning, we minimize a **loss function** using **gradient descent**, which requires **derivatives**. If our model uses $\sigma(z)$ as its output, we need $\frac{d}{dz}\sigma(z)$ to compute the gradient w.r.t. parameters. 

**Good news**: The derivative of the sigmoid is neat and simple:  

$$
\frac{d}{dz}\sigma(z) = \sigma(z) [1 - \sigma(z)]
$$

#### Deriving It Step by Step

Let’s do a detailed derivation. Start from:  

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}
$$

1. **Rewrite**:  

$$
\sigma(z) = (1 + e^{-z})^{-1}
$$

2. **Apply the Chain Rule**:  

$$
\frac{d}{dz}\sigma(z) = \frac{d}{dz} (1 + e^{-z})^{-1} = -1 \cdot (1 + e^{-z})^{-2} \cdot \frac{d}{dz}(1 + e^{-z})
$$

- The exponent $-1$ pulls down, giving the factor $-1 \cdot (1+e^{-z})^{-2}$.
- Next, we multiply by the derivative of the inside $(1 + e^{-z})$.

3. **Derivative of the Inside**:  

$$
\frac{d}{dz} (1 + e^{-z}) = 0 + \frac{d}{dz} (e^{-z}) = -e^{-z}
$$

- Because the derivative of $e^{-z}$ is $-e^{-z}$(the chain rule).

4. **Combine**:  

$$
\frac{d}{dz}\sigma(z) = -1\cdot (1 + e^{-z})^{-2} \cdot (-e^{-z})
$$

- Notice the two negatives multiply to become **plus**:  

$$
= (1 + e^{-z})^{-2} e^{-z}
$$

5. **Rewrite**:  

$$
= \frac{e^{-z}}{(1 + e^{-z})^2}
$$

- This is correct, but we can make it look even more recognizable by some algebraic tricks.

6. **Introduce + Subtract 1** (A helpful trick):  

$$
\frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1 + e^{-z} - 1}{(1 + e^{-z})^2} \quad \text{(not quite final; see next step)}
$$

- Actually, the standard approach is to notice that:  

$$
\frac{e^{-z}}{(1 + e^{-z})^2}
= \frac{1}{1 + e^{-z}} - \frac{1}{(1 + e^{-z})^2}
$$

- Let’s see how.

- A more direct route: Factor out $\tfrac{1}{1 + e^{-z}}$ from the above expression and see how it leaves behind $(1 - \tfrac{1}{1+ e^{-z}})$.

7. **Identify Sigmoid**:

- Recall that:  
   
$$
\sigma(z) = \tfrac{1}{1 + e^{-z}}
$$

- Therefore,  

$$
\frac{d}{dz}\sigma(z) = \sigma(z) (1 - \sigma(z))
$$

- The first $\sigma(z)$ corresponds to $\tfrac{1}{1+ e^{-z}}$.
- The second term $(1 - \sigma(z))$ arises from factoring it out.

#### The Famous Result

Putting it all together:  

$$
\boxed{\frac{d}{dz}\sigma(z) = \sigma(z) [1 - \sigma(z)]}
$$

This identity is **crucial** for backpropagation in neural networks, since it simplifies gradient calculations dramatically.

#### Why It Matters in Practice

- **Efficiency**: Instead of dealing with complicated expressions, we just multiply $\sigma(z)$ by $(1 - \sigma(z))$.
- **Smooth & Differentiable**: The sigmoid is nicely behaved for gradient-based optimization. 
- **Common Activation**: Sigmoid remains popular in output layers for **binary** classification (since it outputs a probability between 0 and 1).

#### Key Takeaways

1. **Sigmoid** “squeezes” any real number into $(0,1)$.
2. **Derivative**: $\sigma'(z) = \sigma(z) [1-\sigma(z)]$ — a simple, elegant form.
3. **Use Cases**:  
   - **Binary classification** output layer.  
   - Anywhere you want a number interpreted as a “probability.”  

Keep this derivative formula handy. In the next steps, we’ll apply it in gradient descent to train **classification perceptrons** effectively!

### Gradient Descent

Now let’s see how **gradient descent** uses a specialized **loss function** (log loss) to update the perceptron’s parameters $w_1, w_2$ and $b$.

If you need a refresher on the log loss function, please refer to:
> 'Ch 008: Mathematics for ML and Data Science'  
> '# Calculus for ML and DS (2) - Optimization'
> '## Optimization of Log-Loss'

#### Recap: The Classification Perceptron

##### Prediction Function

For **two** input features $(x_1, x_2)$, our perceptron predicts:  

$$
z = w_1 x_1 + w_2 x_2 + b, \quad 
\hat{y} = \sigma(z)
$$

where $\sigma(\cdot)$ is the **sigmoid** function:  

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

- **$\hat{y}\approx 1$** → model believes output is (e.g.) “happy.”  
- **$\hat{y}\approx 0$** → model believes output is “sad.”

##### The Goal

We want $\hat{y}$ to match the true label $y\in\{0,1\}$. To measure the difference between $\hat{y}$ and $y$, we use a **loss function** $L(y,\hat{y})$. Our job is to **minimize** that loss by adjusting $w_1, w_2,\text{ and }b$. 

#### The Log Loss (Binary Cross-Entropy)

##### Why Not Just MSE?

For regression, we used mean squared error. But in classification, **log loss** (or **binary cross-entropy**) generally works better:  

$$
L(y,\hat{y}) = -y \ln(\hat{y}) - (1 - y)\ln(1 - \hat{y})
$$

- If $y=1$, the second term vanishes, and the loss is $-\ln(\hat{y})$.  
  - If $\hat{y}$ is close to 1, then $\ln(\hat{y})$ is near 0 → small loss.  
  - If $\hat{y}\approx 0$, $\ln(\hat{y})$ is very negative → large loss.  
- If $y=0$, the first term vanishes, and the loss is $-\ln(1-\hat{y})$.  
  - If $\hat{y}\approx 0$, then $\ln(1-\hat{y})\approx \ln(1)=0$ → small loss.  
  - If $\hat{y}\approx 1$, then $\ln(1-\hat{y})$ is $\ln(0)$ → extremely negative → large loss.

This matches our intuition: we heavily penalize confident but **wrong** predictions. And it aligns with viewing $\hat{y}$ as a **probability**.

#### Gradient Descent Updates

##### The Parameter Update Formula

To find the best $(w_1, w_2, b)$, we do **gradient descent** on the **log loss**:  

$$
\begin{aligned}
w_1 &\leftarrow w_1 - \alpha \frac{\partial}{\partial w_1} L(y,\hat{y}), \\
w_2 &\leftarrow w_2 - \alpha \frac{\partial}{\partial w_2} L(y,\hat{y}), \\
b   &\leftarrow b   - \alpha \frac{\partial}{\partial b}   L(y,\hat{y}),
\end{aligned}
$$

where $\alpha$ is the **learning rate**.

**But** the loss $L$ is expressed in terms of $\hat{y}$, which itself depends on $\sigma(z)$, which depends on $(w_1, w_2, b)$. So we must apply the **chain rule** repeatedly. That’s where the derivative of the sigmoid $\sigma'(z) = \sigma(z) (1-\sigma(z))$ is crucial.

##### Workflow

1. **Initialize** $(w_1, w_2, b)$ to random or small values.
2. **Compute** the output $z = w_1x_1 + w_2x_2 + b$ and $\hat{y}=\sigma(z)$.
3. **Evaluate** log loss $L(y,\hat{y})$.
4. **Compute** partial derivatives of $L$ w.r.t. $w_1, w_2, b$.  
5. **Update** each parameter by subtracting $\alpha \times (\text{derivative})$.
6. **Repeat** until the loss is sufficiently small or you reach a set iteration limit.

#### Why Sigmoid + Log Loss Works So Well

- **Smooth Gradients**: Sigmoid’s derivative is simple $\sigma(z)[1-\sigma(z)]$ and log loss is nicely differentiable.  
- **Natural Probabilistic Interpretation**: $\hat{y}$ can be viewed as $\Pr(\text{happy}|\mathbf{x})$.  
- **Penalizes Over-Confident Mistakes**: The $\ln(\hat{y})$ and $\ln(1-\hat{y})$ terms blow up if the model is confident but wrong.

#### Putting It All Together

1. **Forward Pass**:  
   - $z = w_1 x_1 + w_2 x_2 + b$.  
   - $\hat{y} = \sigma(z)$.  
2. **Loss**:  
   - $L(y,\hat{y}) = -y\ln(\hat{y}) -(1-y)\ln(1-\hat{y})$
3. **Backward Pass** (Derivatives):  
   - Use chain rule with $\sigma'(z)$ and $\ln$ terms.  
4. **Update**:
   - $w_i \leftarrow w_i - \alpha \frac{\partial}{\partial w_i}L, \quad b   \leftarrow b - \alpha \frac{\partial}{\partial b}  L$
5. **Iterate** for all training examples. Over multiple epochs, the model’s parameters converge to minimize log loss.

##### Example

- Suppose $(x_1,x_2)=(1,3)$ with true label $y=0$ (“sad”). 
- If the current perceptron parameters yield $\hat{y}=0.9$, the log loss is large because the model is confidently wrong. 
- Gradient descent will push $w_1,w_2,b$ so next time $\hat{y}$ hopefully drops closer to 0.

#### Key Takeaways

- **Log Loss** (binary cross-entropy) is typically used for classification because it **amplifies** mistakes when the model is very confident but wrong.
- **Sigmoid** + **Log Loss** pairs naturally; their derivatives yield **clean** gradient formulas.
- **Gradient Descent** remains the main update technique: partial derivatives guide each parameter’s correction.
- This single-layer **binary** perceptron generalizes to deeper or multi-class networks—the principle remains:  
  1) compute an output,  
  2) measure a suitable loss,  
  3) do backprop with the chain rule,  
  4) update parameters.

Armed with these fundamentals, you can tackle numerous classification tasks—just define your **feature inputs** $(x_1,x_2,\dots)$, set up your **sigmoid** perceptron, choose **log loss**, and let gradient descent do the heavy lifting!

### Calculating the Derivatives

Now, we want to **train** this perceptron by performing **gradient descent**, which requires computing **partial derivatives** of the loss w.r.t. $w_1$, $w_2$, and $b$. Let’s see how to do this step by step.

#### Overview of the Problem

##### The Model

We have:  

$$
z = w_1 x_1 + w_2 x_2 + b, \quad
\hat{y} = \sigma(z),
$$

where $\sigma(\cdot)$ is the **sigmoid** function,  

$$
\sigma(z) = \tfrac{1}{1 + e^{-z}}
$$

Our **loss function** for a single data point $(x_1,x_2,y)$ is the **log loss** (binary cross-entropy):  

$$
L(y,\hat{y}) = -y\ln(\hat{y}) - (1-y)\ln(1-\hat{y})
$$

We want to find parameter values $(w_1,w_2,b)$ that **minimize** $L(y,\hat{y})$ across the training set.

##### Chain Rule in Action

To do gradient descent, we need $\frac{\partial L}{\partial w_1}$, $\frac{\partial L}{\partial w_2}$, and $\frac{\partial L}{\partial b}$. But $L$ depends on $\hat{y}$ which depends on $z$, and $z$ depends on $(w_1, w_2, b)$. So we apply the **chain rule**:  

$$
\frac{\partial L}{\partial w_1}
= \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1},
\quad
\frac{\partial L}{\partial w_2}
= \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_2},
\quad
\frac{\partial L}{\partial b}
= \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial b}
$$

##### Part 1: $\frac{\partial L}{\partial \hat{y}}$

Given  

$$
L(y,\hat{y}) = -y\ln(\hat{y}) - (1-y)\ln(1-\hat{y})
$$

the partial derivative w.r.t. $\hat{y}$ is:  

$$
\frac{\partial L}{\partial \hat{y}}
= -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}
= \frac{-y(1-\hat{y}) + (1-y) \hat{y}}{\hat{y}(1-\hat{y})}
= \frac{-y + y\hat{y} + \hat{y} - y\hat{y}}{\hat{y}(1-\hat{y})}
= \frac{-\,y + \hat{y}}{\hat{y}(1-\hat{y})}
= -\frac{y - \hat{y}}{\hat{y}(1-\hat{y})}
$$

Often seen as:  

$$
\frac{\partial L}{\partial \hat{y}} = -\frac{y - \hat{y}}{\hat{y}(1-\hat{y})}
$$

##### Part 2: $\frac{\partial \hat{y}}{\partial w_1}, \frac{\partial \hat{y}}{\partial w_2}, \frac{\partial \hat{y}}{\partial b}$

Remember, $\hat{y}=\sigma(z)$ and $z = w_1 x_1 + w_2 x_2 + b$. Using the **sigmoid derivative** $\sigma'(z)=\sigma(z)(1-\sigma(z))$, we get:  

$$
\tfrac{\partial \hat{y}}{\partial w_1} 
= \sigma'(z) \tfrac{\partial z}{\partial w_1}
= \hat{y}(1-\hat{y}) x_1
$$  

$$
\tfrac{\partial \hat{y}}{\partial w_2} = \hat{y}(1-\hat{y}) x_2
$$  

$$
\tfrac{\partial \hat{y}}{\partial b} = \hat{y}(1-\hat{y})
$$

#### Combine via Chain Rule

For each parameter, multiply $\frac{\partial L}{\partial \hat{y}}$ by $\frac{\partial \hat{y}}{\partial \text{parameter}}$:  

##### $\frac{\partial L}{\partial b}$

$$
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial b} = (-\frac{y-\hat{y}}{\hat{y}(1-\hat{y})})
\times (\hat{y}(1-\hat{y}))
= -(y-\hat{y})
$$

##### $\frac{\partial L}{\partial w_1}$  

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1} = (-\frac{y-\hat{y}}{\hat{y}(1-\hat{y})}) \times (\hat{y}(1-\hat{y})x_1) = -(y-\hat{y})x_1
$$

##### $\frac{\partial L}{\partial w_2}$  

$$
\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_2} = (-\frac{y-\hat{y}}{\hat{y}(1-\hat{y})}) \times (\hat{y}(1-\hat{y})x_2) = -(y-\hat{y})x_2
$$

#### Applying These in Gradient Descent

Once we have these derivatives, each **gradient descent** step is:  

$$
\begin{aligned}
w_1 &\leftarrow w_1 - \alpha \frac{\partial L}{\partial w_1}
= w_1 - \alpha (-(y-\hat{y})x_1) = w_1 + \alpha(y-\hat{y})x_1,\\
w_2 &\leftarrow w_2 - \alpha \frac{\partial L}{\partial w_2} = w_2 + \alpha(y-\hat{y})x_2,\\
b   &\leftarrow b - \alpha \frac{\partial L}{\partial b}
= b   + \alpha(y-\hat{y})
\end{aligned}
$$

**Interpretation**:
- If $y=\hat{y}$ (prediction correct), then $y-\hat{y}=0$ → minimal updates.
- If $y\neq \hat{y}$ (prediction off), $|y-\hat{y}|$ is large, so bigger parameter corrections.

#### Key Takeaways

1. **Chain Rule**:  
   - $L \to \hat{y} \to z \to (w_1,w_2,b)$ requires partial derivatives at each link.
   - The **sigmoid derivative** $\sigma'(z)=\sigma(z)[1-\sigma(z)]$ greatly simplifies the math.

2. **Final Form**:  
   - $\frac{\partial L}{\partial b}=-(y-\hat{y})$,  
   - $\frac{\partial L}{\partial w_1}=-(y-\hat{y})x_1$,  
   - $\frac{\partial L}{\partial w_2}=-(y-\hat{y})x_2$.

3. **Gradient Descent**:  
   - Update each parameter by subtracting $\alpha\times(\text{derivative})$.
   - Repeatedly apply for all data points until the log loss stops decreasing.

4. **Logic**:  
   - If the model’s output $\hat{y}$ is higher than the true label $y=0$, it lowers the parameters to reduce $\hat{y}$.  
   - If the model’s output $\hat{y}$ is lower than the true label $y=1$, it raises the parameters to increase $\hat{y}$.  

With these derivatives in hand, **training** a classification perceptron becomes a direct extension of the procedure used for **linear regression**—just with a **sigmoid** activation and **log loss** in the loop.

---

## **Classification with a Neural Network**



### Minimizing Log-Loss

---

## **Gradient Descent and Backpropagation**


