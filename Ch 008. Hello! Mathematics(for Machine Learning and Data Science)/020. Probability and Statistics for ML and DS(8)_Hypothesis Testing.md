# Probability and Statistics for ML and DS(8)_Hypothesis Testing
---
## Hypothesis Testing: Making Data-Driven Decisions

> *Have You Ever Wondered How Your Spam Filter Works? Ever checked your email and noticed how most spam messages get filtered automatically? How does your email provider know which messages are genuine and which are trying to sell you something? This is where hypothesis testing comes into play—a powerful tool that helps us make decisions based on evidence.*

### Hypothesis Objects: Decision-Making Frameworks

In our object-oriented journey through statistics, we've explored probability distributions and how they help us understand randomness. Now, we'll see how to use these concepts to make decisions with confidence.

Hypothesis testing can be viewed as creating a decision-making framework with two competing hypothesis objects:

```
HypothesisTest {
    nullHypothesis: Hypothesis,      // The default assumption
    alternativeHypothesis: Hypothesis,  // What we're trying to prove
    evidenceThreshold: Number,        // How much evidence we need
    
    evaluateEvidence(data): Decision  // Method to reach a conclusion
}
```

Each hypothesis represents a possible state of reality, and our job is using data to choose between them.

### The Null and Alternative Hypotheses: Two Competing Objects

Let's define our two main hypothesis objects:

1. **Null Hypothesis $H_0$**: The default assumption or status quo. It's what we assume to be true until proven otherwise.

2. **Alternative Hypothesis $H_1$**: The competing claim we're investigating—often what we're actually interested in proving.

These two hypotheses have key properties:
- They must be mutually exclusive (they can't both be true)
- They must be exhaustive (one must be true)
- They must be testable with data

### The Email Spam Filter: Hypothesis Testing in Action

Consider a spam filter deciding whether an email is legitimate (ham) or unwanted (spam):

```
EmailClassifier {
    nullHypothesis: "Email is ham (legitimate)",
    alternativeHypothesis: "Email is spam (unwanted)",
    
    classifyEmail(emailContent): "Ham" or "Spam"
}
```

Notice something interesting here: the null hypothesis is that the email is legitimate (ham). Why? Because the consequences of incorrectly labeling a legitimate email as spam (losing important information) are usually worse than letting a spam email into your inbox.

### The Asymmetry in Hypothesis Decision-Making

Here's where hypothesis testing gets interesting—there's a fundamental asymmetry in how decisions are made:

1. If we gather sufficient evidence against $H_0$, we **reject it** and accept $H_1$
2. If we don't have enough evidence against $H_0$, we **fail to reject it**—**but this doesn't prove $H_0$ is true**

This asymmetry is similar to how our legal system works. A defendant is presumed innocent (null hypothesis) until proven guilty (alternative hypothesis) beyond reasonable doubt (evidence threshold).

Do you remember **the courtroom analogy**? Let's review it:

#### The Courtroom Analogy

Let's imagine a courtroom trial. In this trial, a person is prosecuted of committing a crime. Here's how the courtroom scenario relates to null hypothesis testing:  

- **Null Hypothesis, $H_0$**: In the courtroom, the default assumption is that the prosecuted person is innocent. This is like the null hypothesis in statistics, where we assume there's no effect or no difference between groups.

- **Alternative Hypothesis, $H_1$**: The prosecutor believes the prosecuted person is guilty and presents evidence to prove it. This is like the alternative hypothesis in statistics, where we believe there is an effect or a difference, and we try to provide evidence for it.

- **Evidence (Data)**: The prosecutor presents evidence to show that the prosecuted person is guilty. Similarly, in statistics, we collect data to test whether the null hypothesis should be rejected in favor of the alternative hypothesis.

- **The Judge/Jury (Statistical Test)**: The judge or jury evaluates the evidence. In null hypothesis testing, the statistical test evaluates the data to see if it's strong enough to reject the null hypothesis.

- **Decision (Reject or Fail to Reject)**: If the evidence is strong enough(=low P-value), the jury might decide the prosected person guilty. In statistics, this is like rejecting the null hypothesis and concluding there’s a significant effect or difference.

- If the evidence is not strong enough, the jury cannot convict, meaning the prosecuted person **remains** innocent. In statistics, this is like failing to reject the null hypothesis, meaning we don't have enough evidence to conclude there’s an effect or difference.

- **Important Note**: Just like in a courtroom, where a "not guilty" verdict doesn't necessarily mean the person is innocent (just that there wasn't enough evidence to prove guilt), **failing to reject the null hypothesis doesn't mean it's true. It just means there isn't enough evidence against it.**

- **Summary:**
  - The null hypothesis, $H_0$, is like assuming someone is innocent.
  - The alternative hypothesis, $H_1$, is like believing someone is guilty.
  - Evidence (data) is collected to test these assumptions.
  - The decision (reject or fail to reject) is based on whether the evidence is strong enough to change the assumption.

### Evidence Collection: The Heart of Hypothesis Testing

For our spam filter, evidence comes from examining the email's properties:
- Sender information
- Subject line content
- Email body text
- Presence of suspicious links
- Unusual formatting

When the email contains phrases like "earn extra cash," "risk free," "dear friend," or "act immediately," these serve as evidence against the null hypothesis, suggesting the email might be spam.

### Beyond Email: Hypothesis Testing Everywhere

This framework extends far beyond spam filtering:

**Medical Diagnosis:**
- H₀: Patient does not have the disease
- H₁: Patient has the disease
- Evidence: Test results, symptoms, medical history

**Quality Control:**
- H₀: The product batch meets quality standards 
- H₁: The product batch is defective
- Evidence: Sample testing results

**Financial Fraud Detection:**
- H₀: The transaction is legitimate
- H₁: The transaction is fraudulent
- Evidence: Transaction patterns, amount, location, timing

### Connecting to Our Previous Knowledge

Remember how we discussed probability distributions? They play a crucial role in hypothesis testing by helping us quantify the likelihood of our evidence under different hypotheses. When we calculate how likely or unlikely our observed data would be if the null hypothesis were true, we're using these distributions.

### The Hypothesis Testing Process

At its core, hypothesis testing follows these steps:

1. Define your null hypothesis, $H_0$, and alternative hypothesis, $H_1$
2. Collect data
3. Calculate how likely your data would be if $H_0$ were true
4. If this likelihood is below your threshold, reject $H_0$ in favor of $H_1$
5. Otherwise, fail to reject $H_0$

### Summary

Hypothesis testing is about making decisions under uncertainty, **not about proving absolute truth**. The null hypothesis is our default position, and **we only move away from it when the evidence is compelling enough.**

> **Remember: We assume the status quo until the data convinces us otherwise.**

---

## Type I and Type II Errors: The Cost of Being Wrong

### What Happens When Our Decision-Making System Makes a Mistake?

> ***Have you ever deleted an important email because your spam filter thought it was junk? Or perhaps found an obvious scam sitting in your inbox?***

These mistakes represent the two fundamental error types in hypothesis testing—and understanding them is crucial for making better decisions with data.

### The Error Object: Modeling Mistakes in Our Decision Framework

In our `HypothesisTest` object from before, we need to add an important property—the possibility of error:

```
HypothesisTest {
    nullHypothesis: Hypothesis,     // Default assumption
    alternativeHypothesis: Hypothesis, // What we're investigating
    evidenceThreshold: Number,      // Decision boundary
    
    possibleErrors: {
        typeI Error,    // Rejecting H₀ when it's true
        typeII Error    // Failing to reject H₀ when it's false
    },
    
    evaluateEvidence(data): Decision
}
```

Each error type represents a different kind of mistake our decision system can make.

### The Two Fundamental Errors: False Alarms and Missed Detections

Let's explore these two error types using our spam filter example:

1. **Type I Error (False Positive)**: Rejecting $H_0$ when it's actually true
   - In our spam example: Marking a legitimate email as spam
   - Also called a "false alarm"
   - Mathematical notation: P(Reject H₀ | H₀ is true)

2. **Type II Error (False Negative)**: Failing to reject H₀ when it's actually false
   - In our spam example: Letting a spam email into your inbox
   - Also called a "missed detection"
   - Mathematical notation: P(Fail to reject H₀ | H₀ is false)

### The Decision Matrix: Visualizing Errors

Looking at the decision matrix in the image:

- When reality is [$H_0$ True] (legitimate email) and we [Reject $H_0$] (mark as spam), we make a **Type I error**
- When reality is [$H_0$ False] (spam email) and we [Don't reject $H_0$] (mark as legitimate), we make a **Type II error**
- The other two combinations represent correct decisions
- Here is the image of the decision matrix table:

![Decision Matrix](images/image-19.jpg)

### Error Tradeoffs: You Can't Minimize Both

Here's the challenging part of hypothesis testing: there's an inherent tradeoff between **Type I and Type II errors**. It's like a seesaw—pushing down on one side raises the other.

If we make our spam filter extremely strict (low threshold for marking as spam):
- We'll catch almost all spam emails (low Type II error)
- But we'll also mark many legitimate emails as spam (high Type I error)

If we make our spam filter very lenient (high threshold for marking as spam):
- Almost all legitimate emails will reach the inbox (low Type I error)
- But many spam emails will also get through (high Type II error)

### Significance Level: Setting Our Error Tolerance

The maximum probability of making a Type I error that we're willing to accept is called the **significance level**, denoted by the Greek letter alpha $α$.

Common values include:
- $α = 0.05$ (5% chance of Type I error)
- $α = 0.01$ (1% chance of Type I error)

Setting $α$ is a critical design decision that balances the two types of errors for your specific application.

### Beyond Email: Error Types Across Domains

This error framework applies to countless decision scenarios:

**Medical Testing:**
- H₀: Patient doesn't have the disease
- Type I Error: Diagnosing a healthy person with the disease (false positive)
- Type II Error: Missing the disease in a sick patient (false negative)
- Which is worse? **It depends on the disease and treatment!**

**Criminal Justice:**
- $H_0$: Defendant is innocent
- Type I Error: Convicting an innocent person
- Type II Error: Letting a guilty person go free
- Our system is designed to minimize Type I errors ("innocent until proven guilty" - "무죄추정의 원칙")

**Quality Control:**
- $H_0$: Product batch meets standards
- Type I Error: Rejecting a good batch (wasting money)
- Type II Error: Accepting a defective batch (harming customers)
- The balance depends on the product and consequences of each error

### Object-Oriented Error Management Strategies

Error management involves implementing specific methods to control these error types:

1. **Sample Size Increase**: Larger samples generally reduce both error types
2. **Threshold Adjustment**: Moving the decision boundary to favor one error type over another
3. **Feature Engineering**: Finding better evidence to distinguish between hypotheses
4. **Cost Function Optimization**: Weighting errors according to their consequences

Remember our probability distributions? They help us calculate the likelihood of each error type. The area under the curve beyond our threshold represents our Type I error rate, while the overlap between distributions creates our Type II error rate.

This connects back to our concept of distributions as objects with methods to calculate probabilities—now we're using those probabilities to make decisions with quantified risks.

### The Practical Impact of Errors

In real systems, we often weight these errors based on their consequences:

- In email filtering: Missing a legitimate email (Type I) is usually worse than letting spam through (Type II)
- In cancer screening: Missing cancer (Type II) is usually worse than a false alarm (Type I)
- In fraud detection: The cost of investigating a legitimate transaction (Type I) versus the loss from fraud (Type II)

### Summary

Every time we make a decision based on limited data, we risk being wrong in one of two ways. **We can't eliminate both types of errors completely—reducing one typically increases the other**. The art of hypothesis testing is finding the right balance for your specific situation.

> **Remember: Every decision system must balance false alarms against missed detections—there is no perfect detector.**