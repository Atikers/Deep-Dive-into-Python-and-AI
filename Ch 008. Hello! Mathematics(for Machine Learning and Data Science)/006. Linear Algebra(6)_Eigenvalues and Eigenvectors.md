# ***Linear Algebra(6)_Eigenvalues and Eigenvectors***

## **Bases in Linear Algebra**

> **Have you ever wondered how we can pinpoint any location on a map using just a few directions (like north-south and east-west)?**

In linear algebra, that idea is captured by the concept of a *basis*.

### **Intuitive Picture**

Imagine you’re exploring a city. You have two main streets: one runs north-south, and the other east-west. If you can move along these two streets in any amount—forward or backward—you can reach every possible location in that city grid. In this city analogy, those two streets act like *basis vectors*. 

Similarly, in 2D linear algebra, a basis is typically two vectors (arrows) that can “generate” or “reach” every point in the plane through some combination of those vectors. If you can’t get everywhere—say the two vectors overlap or point in the same direction—then you’re stuck moving along one line, which means you don’t have a full basis.

### **Formal Definition**

A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ is called a **basis** for a vector space if:

1. They are **linearly independent**. Informally, none of the vectors can be written as a “stretch” or “shrink” of another.
2. They **span** the entire space. In other words, every point (vector) in the space can be written as a linear combination of the basis vectors:  

$$
\mathbf{p} = a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_k \mathbf{v}_k
$$

- for some scalars $a_1, a_2, \ldots, a_k$ (which can be any real numbers).

In 2D, this boils down to having two vectors that do not lie on the same line. In 3D, you’d need three vectors that don’t lie in the same plane, and so on.

### **Why Does This Matter?**

1. **Coordinate Systems**: A basis is like a skeleton for a space. Once you’ve picked a basis, any vector in that space can be described by how much you move along each basis vector. This is why the traditional $x$ and $y$ axes in a plane are considered a standard basis: any point $(x, y)$ is $x$ steps in the horizontal direction plus $y$ steps in the vertical direction.

2. **Simplicity and Building Blocks**: Think of an artist mixing primary colors—red, blue, and yellow—to get any color on the palette. The primary colors form a type of “basis,” because you can combine them in different proportions to produce a wide range of hues. Likewise, a basis in linear algebra provides the fundamental “directions” you need to build any vector.

3. **Choice of Basis**: You aren’t stuck with just one option. Any two linearly independent vectors in 2D can form a basis. Different choices can sometimes simplify problems. For example, if you choose a basis aligned with certain symmetries, your equations might become easier to handle.

### **Examples**

1. **Standard 2D Basis**: The usual vectors $(1,0)$ and $(0,1)$ in the plane. Clearly, they span the plane: any point $(x,y)$ can be reached by $x(1,0) + y(0,1)$.

2. **Tilted Basis**: Suppose you have vectors $(2,1)$ and $(-1,3)$. These still form a basis as long as they’re not multiples of one another. You can still reach all points in the plane by walking some amount in each direction.

3. **Non-Basis**: Two vectors that line up—like $(1,2)$ and $(2,4)$—are multiples of each other and only cover a single line. You’d never be able to reach any point off that line.

### **Connecting to AI and Beyond**

In more advanced AI topics, especially in deep learning or machine learning, changing from one basis to another can make certain problems simpler to understand or solve. For instance, when dealing with huge datasets, we often want to select a more *convenient* basis that captures the key patterns (like using Principal Component Analysis, where we change the basis to emphasize the directions of greatest variance).

Think of it a bit like surgery in medicine: the surgeon picks the best approach or “angle” to operate so that the procedure becomes simpler and clearer. In the same way, switching to the right basis can reveal structures and patterns in your data that are hidden in the original coordinate system.

### **Key Takeaways**

- A **basis** is a set of independent vectors that span the entire space.  
- Any vector in the space can be written as a combination of those basis vectors.  
- Different bases can be chosen depending on what is most useful for a given problem.  
- In AI, changing bases can help us see data in a new light and simplify computations.

Stay curious, and remember that much like choosing the best vantage point to observe a painting or performance, choosing the right basis in linear algebra can give you a deeper, more intuitive view of your data.

---

## **Span in Linear Algebra**

> **Imagine you have a toolbox with a few types of tools—maybe a hammer, a screwdriver, and a wrench. Do you ever think about all the tasks you can accomplish *just* with those tools?**

In linear algebra, the concept of span captures a similar idea: how many “places” (or solutions) can you reach by combining a given set of vectors?

### **Motivation & Intuition**

In everyday life, we combine movements or actions to reach goals. For instance:
- **Navigation**: Going north-south or east-west to reach any location on a city grid.
- **Color Mixing**: Using primary colors (red, blue, yellow) in different proportions to create a full palette.
- **Forces in Physics**: Combining forces (vectors) in different directions to find the net force on an object.

All these scenarios hint at how “few directions” can generate a wide variety of outcomes. In linear algebra, the **span** of a set of vectors is precisely the collection of all points (or vectors) you can produce by linearly combining those directions.

### **Formal Definition of Span**

Given vectors  

$$
\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k
$$ 

in a vector space (like a 2D or 3D space), the **span** of these vectors is the set of all possible linear combinations:  

$$
\{a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_k \mathbf{v}_k \mid a_1, a_2, \ldots, a_k \in \mathbb{R} \}
$$

- **Linear Combination**: Taking each vector and scaling it by some real number $a_i$, then adding the results together.  
- **All Real Numbers**: Each $a_i$ can be zero, positive, or negative, allowing you to “walk” in each vector’s direction (forwards, backwards, or stand still in that direction).

#### **Example in 2D**

- Suppose $\mathbf{v}_1 = (1,0)$ and $\mathbf{v}_2 = (0,1)$.  
- Their span is *every* point $(x,y)$ in the plane, because any $(x,y)$ can be written as  

$$
x(1,0) + y(0,1) = (x,y)
$$

- This is the standard set of axes you see on a typical Cartesian plane.

#### **Example in 3D**

- Consider $\mathbf{v}_1 = (1,0,0), \mathbf{v}_2 = (0,1,0)$, and $\mathbf{v}_3 = (0,0,1)$.  
- Their span is the entire 3D space—every point $(x,y,z)$—because you can create any 3D vector by combining these three “direction” vectors appropriately.

### **Spanning Lines, Planes, and Spaces**

To grasp span more concretely, let’s see how the number of vectors relates to the shape they span:

1. **One Vector**: Spans a line (unless the vector is the zero vector, which spans only a single point).  
2. **Two Independent Vectors**: Spans a plane (in 2D or as a plane “slice” in 3D).  
3. **Three Independent Vectors**: In 3D, three vectors that don’t all lie in the same plane can span the whole space.

#### **When Vectors Fail to Span**

If multiple vectors are “all in the same line,” you can’t cover an entire plane. Similarly, if three vectors in 3D all lie in the same plane, you can’t reach points outside that plane. The span would be just that plane, not the whole 3D space.

### **Minimal Spanning Set and the Basis Concept**

A **spanning set** means you can generate the entire space (or subspace), but sometimes you might have redundant vectors. A **basis** is a *minimal* spanning set—no extra directions that can already be formed by the others.

- **Minimal** means that if you remove *any* vector from a basis, you lose the ability to span the entire space.  

#### **Example of Redundancy**

- In 2D, having three vectors that all lie in the same plane still spans the plane, but one vector is redundant because two independent vectors were enough. Hence, that three-vector set isn’t a basis (it’s not minimal).

### **The Link Between Span and Linear Independence**

**Linear Independence** is the condition that no vector in your set can be formed by combining the others. If even one vector can be generated by the rest, the set is **linearly dependent**.

- A set of vectors forms a **basis** if and only if it:
  1. **Spans** the space.
  2. Is **linearly independent**.

Thus, checking **span** ensures you cover the whole space, and checking **independence** ensures you aren’t using extra “duplicate” directions.

### **Dimensional Insights**

A space’s **dimension** is the number of vectors in *any* of its bases. Here are some key facts:

- A **line** (1D) has dimension 1. Any single nonzero vector can form a basis for that line.  
- A **plane** (2D) has dimension 2. Any two linearly independent vectors can form a basis.  
- **3D space** has dimension 3. You need three linearly independent vectors to form a basis, and so on.

In more advanced settings (like AI and data science), you might work in *very* high-dimensional spaces (e.g., a 100-dimensional feature space).

### **Practical Analogies from Other Fields**

1. **Physics (Force Vectors)**: In classical mechanics, the net force on an object is the vector sum of forces applied to it. If you have two distinct directions of force in a 2D plane, you can produce any resultant force in that plane by choosing appropriate magnitudes. The set of force directions might span the plane of possible net forces.

2. **Art (Color Mixing)**: If you consider red, blue, and yellow as your “vectors,” mixing them in various proportions spans a wide range of colors on your canvas. However, if one color can already be made by mixing the other two, it’s redundant.

3. **Medicine (Treatment Plans)**: Imagine each “direction” as a particular treatment approach (e.g., physical therapy, medication, diet). If one of those treatments is essentially a combination of the others, it doesn’t add a fundamentally new approach. The total “span” of treatments might be all possible ways to combine them for a given condition.

4. **Engineering (Vibration Modes)**: In structural engineering, different “modes” of vibration can combine to produce the total motion of a system. If certain modes are not truly distinct, they’re dependent on others and don’t increase the total possible motion patterns.

### **Step-by-Step Example: Checking Span in 2D**

Let’s say we have two vectors in 2D:  

$$
\mathbf{v}_1 = (2,1), \quad \mathbf{v}_2 = (-1,3)
$$

1. **Can they span the plane?**  
  - We ask: can any point $(x,y)$ be written as  
   
$$
\alpha (2,1) + \beta (-1,3) = (x,y)\;?
$$  

$$
2\alpha - \beta = x
$$  

$$
\alpha + 3\beta = y
$$
   
  - If for every $(x,y)$ you can solve for $\alpha$ and $\beta$, then $\mathbf{v}_1$ and $\mathbf{v}_2$ span the plane. Typically, this works if the determinant of the coefficient matrix is nonzero (which means $\mathbf{v}_1$ and $\mathbf{v}_2$ are not multiples of each other).

2. **Check for linear dependence**  
  - If $\mathbf{v}_2$ were just a scalar multiple of $\mathbf{v}_1$, you’d see the system only has solutions for certain $(x,y)$ that lie on one line. That would mean no full coverage of the plane—hence, no spanning of 2D.

### **Importance in AI and Data Science**

1. **Dimensionality Reduction**: Techniques like **Principal Component Analysis (PCA)** find a smaller set of “directions” (principal components) that still *span* most of the meaningful variance in your data. This helps when you have lots of features (dimensions) but want to compress them without losing essential information.

2. **Feature Selection**: If some features in a dataset are linear combinations of others, they might be redundant. By identifying a minimal spanning set of features (a basis for your data space), you reduce complexity.

3. **Neural Networks**: Even in deep learning, you’ll often deal with transformations (like matrix multiplications). Understanding how spans and bases work helps you see how data passes from one layer to another (through transformations that can change how you view the space).

### **Summary & Key Points**

- **Span** is the set of all linear combinations of given vectors.  
- If you can reach the entire space, the vectors *span* that space.  
- **Basis** is a minimal set that spans the space (no vector can be omitted).  
- **Dimension** is the number of vectors in *any* basis of that space.  
- **Linear independence** prevents redundancy—no vector is a linear combination of the others.

**Big Picture**: Think of a span as your “toolbox.” You want just enough tools to handle every possible “building/repair project” (i.e., every point in your space), but not so many that you’re carrying around unnecessary duplicates. That’s essentially the difference between any spanning set and a basis.

---

## **Eigenbases**

> **Have you ever wondered if there is a way to see a linear transformation as just “stretching” along certain special directions, rather than tilting or shearing in complicated ways?**

When we talk about *bases* in linear algebra, we usually start with the “standard” or “fundamental” basis vectors. For the 2D plane, these are $(1,0)$ and $(0,1)$. Any vector in 2D can be expressed as a combination of these two basic directions. 

However, there’s a particular choice of basis that can make certain transformations look *astonishingly simple*. We call it an **eigenbasis**. In this basis, a linear transformation acts like pure stretching along each of the basis directions—no rotation, no extra skew. These special directions are called **eigenvectors**, and the amounts by which the vectors get stretched are called **eigenvalues**.

### **A Quick Example: Stretching a Parallelogram**

Imagine you have a matrix  

$$
A = \begin{pmatrix}
2 & 1 \\
0 & 3
\end{pmatrix}
$$ 

that defines a linear transformation in 2D. If you apply $A$ to the unit square defined by $(1,0)$ and $(0,1)$, you will end up with a parallelogram. In more familiar coordinates:

- The vector $(1,0)$ is mapped to $(2,0)$.  
- The vector $(0,1)$ is mapped to $(1,3)$.  

Visually, our neat little square is stretched and slanted, so the resulting shape is a parallelogram.

But what if you choose a different set of two vectors as your basis? Suppose you pick $(1,0)$ and $(1,1)$. Under $A$, these become:

- $(1,0)$ goes to $(2,0)$.  
- $(1,1)$ goes to $(3,3)$.  

So, you end up with another parallelogram—yet something special happens: each side of that parallelogram remains parallel to its “original” side. In other words, you’ve found directions that are being purely _stretched_ by the matrix. One direction is stretched by a factor of $2$, while the other is stretched by a factor of $3$. When both vectors are simply stretched (without extra skew between them), those vectors are eigenvectors, and $2$ and $3$ are the corresponding eigenvalues.

### **Why Is This So Useful?**

1. **Simplicity of Computation**  
   In the eigenbasis, applying the transformation $A$ boils down to just multiplying each coordinate by its respective eigenvalue. For example, if a point $(x,y)$ in your new basis has coordinates $(a,b)$ with respect to the eigenvectors, then after applying $A$, it becomes $(\lambda_1 \cdot a, \lambda_2 \cdot b )$, where $\lambda_1$ and $\lambda_2$ are the eigenvalues. No mixing, no extra addition.

2. **Connections to Other Fields**  
   - **Physics**: In vibration analysis (think of a guitar string or a drumhead), certain “modes” (eigenvectors) can vibrate independently with different frequencies (eigenvalues).  
   - **Engineering**: Stress and strain in materials can be described by principal directions—again, an eigenbasis concept.  
   - **Medicine**: In imaging (like MRI or CT scans), data compression techniques can rely on “principal directions” to reduce dimensionality.  
   - **Art**: When scaling or distorting images, finding principal axes that stay pure in direction can help with uniform transformations or stylized effects.

3. **Principal Component Analysis (PCA)**  
   One of the biggest uses of eigenvectors and eigenvalues in machine learning is **PCA (Principal Component Analysis)**. PCA finds directions (principal components) along which data has the greatest variance, and these directions are essentially eigenvectors of the data’s covariance matrix. By re-expressing data in an eigenbasis, you can see which directions are most “significant” and potentially reduce the dimensionality of your dataset without losing the key patterns.

### How It Ties Together

- An **eigenbasis** is simply a basis made up of eigenvectors of a matrix $A$.  
- Each vector in this basis is stretched by its corresponding eigenvalue under the transformation.  
- By diagonalizing (or simplifying) the matrix in the eigenbasis, we make our computations more straightforward.  

Think of it like looking at a transformation through “special glasses” that let you see each direction as a perfect stretch—no complicated distortion. It’s often a powerful tool in many branches of science, engineering, and data analysis.

Next time you see a matrix, ask yourself: does it have a set of directions it simply stretches? If so, finding that eigenbasis can turn a seemingly complex problem into something much simpler. In upcoming sections, we’ll dig deeper into how to compute these eigenvectors and eigenvalues, and how they lead to major applications like dimension reduction and PCA.

### **Key Takeaway**

An eigenbasis reshapes the way you see transformations. Once you find it, linear algebra problems can become much simpler—like turning a bumpy path into a smooth highway for both theory and computation.

---

## **Eigenvalues and Eigenvectors**

> **Have you ever noticed that when you apply a certain linear transformation (like a matrix in 2D), some directions seem to “line up” perfectly—almost as if they just get stretched while keeping their original orientation, whereas other directions get slanted or rotated?**

In linear algebra, those “special” directions are connected to the idea of **eigenvectors** and **eigenvalues**. Learning these concepts is a milestone for anyone delving into AI or data science, particularly because they are at the heart of **Principal Component Analysis (PCA)** and other advanced techniques. Let’s explore what they are and why they are so powerful.

### **A 2D Example**

Let’s consider the 2×2 matrix(again):  

$$
A = \begin{pmatrix}
2 & 1 \\
0 & 3
\end{pmatrix}
$$

We’ll look at how $A$ acts on different 2D vectors. Some vectors will be stretched in a neat, direct way; others will be skewed or rotated. 

1. **Vector $(1,0)$**  
- Applying $A$:  
   
$$
A \begin{pmatrix} 1 \\
0 \end{pmatrix} 
= \begin{pmatrix} 2 \\
0 \end{pmatrix}
$$  
   - We see that it **does not** change direction (it’s still pointing purely along the horizontal axis).  
   - We can rewrite  
   
$$
\begin{pmatrix} 2 \\
0 \end{pmatrix}
$$
   
$$
2 \times \begin{pmatrix} 1 \\
0 \end{pmatrix}
$$  
   - So $(1,0)$ is an **eigenvector** with **eigenvalue** $2$

2. **Vector $(1,1)$**  
- Applying $A$:  
   
$$
A \begin{pmatrix} 1 \\
1 \end{pmatrix}
= \begin{pmatrix} 3 \\
3 \end{pmatrix}
$$
   - Again, the result points along the same diagonal direction as $(1,1)$ itself.  
   - It is effectively  
   
$$
3 \times \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$
   - So $(1,1)$ is another **eigenvector** with **eigenvalue** $3$.

3. **Vector $(-1,2)$**  
- Applying $A$:  
   
$$
A \begin{pmatrix} -1 \\
2 \end{pmatrix}
= \begin{pmatrix} 0 \\
6 \end{pmatrix}
$$
   - Notice that $(0,6)$ does **not** point in the same direction as $(-1,2)$.  
   - There is no single scalar $\alpha$ such that  
   
$$
\alpha \times \begin{pmatrix} -1 \\
2 \end{pmatrix}
= \begin{pmatrix} 0 \\
6 \end{pmatrix}
$$
   - Therefore, $(-1,2)$ is **not** an eigenvector.

### **Formal Definition**

The above observations hint at the general definition. A vector $\mathbf{v}$ is called an **eigenvector** of a matrix $A$ if:  

$$
A \mathbf{v} = \lambda \mathbf{v}
$$

where $\lambda$ is a scalar value called the **eigenvalue** associated with $\mathbf{v}$.  

- In our example, $(1,0)$ and $(1,1)$ satisfy this condition:  

$$
A \begin{pmatrix} 1 \\
0 \end{pmatrix}
= 2 \begin{pmatrix} 1 \\
0 \end{pmatrix} \quad A \begin{pmatrix} 1 \\
1 \end{pmatrix}
= 3 \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$

- Thus, **$(1,0)$ with eigenvalue $2$** and **$(1,1)$ with eigenvalue $3$** are two eigenvalue-eigenvector pairs.

### **Why Are Eigenvectors So Special?**

1. **Matrix Multiplication vs. Scalar Multiplication**  
   - **Matrix multiplication** in 2D typically involves several operations (in this small case, $2\times 2$ yields 8 multiplications, 4 additions).  
   - **Scalar multiplication** is far simpler—just multiply each component by a single number (2 operations in 2D).  
   - When you’re working **along an eigenvector**, the transformation $A$ acts like a single scalar $\lambda$. This makes your life **much easier** in higher dimensions.

2. **Eigenbasis Simplifies Computations**  
   - Suppose your matrix $A$ has two linearly independent eigenvectors in 2D (as in our example). These eigenvectors can form a new basis—**eigenbasis**.  
   - **Any vector** in the plane can be expressed as a linear combination of these two eigenvectors.  
   - When you apply $A$ to a vector written in the eigenbasis, each component just gets multiplied by its corresponding eigenvalue—**no big matrix multiplications required**.

### **A Step-by-Step Computation Shortcut**

Let’s see how this helps even with a non-eigenvector like $(-1,2)$:

1. **Express $(-1,2)$ in the Eigenbasis**  
- Our eigenbasis might be $\{(1,0), (1,1)\}$.  
- If you can solve for scalars $a$ and $b$ such that  
   
$$
\begin{pmatrix} -1 \\
2 \end{pmatrix} = a \begin{pmatrix} 1 \\
0 \end{pmatrix} + b \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$

- then you’ve found the coordinates of $(-1,2)$ in the eigenbasis.  
- In our example, it turns out $a=-3$ and $b=2$ do the job, so  

$$
\begin{pmatrix} -1 \\
2 \end{pmatrix} = -3 \begin{pmatrix} 1 \\
0 \end{pmatrix} + 2 \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$

2. **Apply $A$ to Each Eigenvector**  
- We already know $A(1,0) = 2(1,0)$ and $A(1,1) = 3(1,1)$.  

3. **Combine Results**  
- We have  

$$
A \begin{pmatrix} -1 \\
2 \end{pmatrix} = A \Bigl( -3 \begin{pmatrix} 1 \\
0 \end{pmatrix} + 2 \begin{pmatrix} 1 \\
1 \end{pmatrix} \Bigr) = -3 A \begin{pmatrix} 1 \\
0 \end{pmatrix} + 2 A \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$  

- By plugging in the eigenvector relationships, you get:  

$$
= -3 \bigl(2 \begin{pmatrix} 1 \\
0 \end{pmatrix}\bigr) + 2 \bigl(3 \begin{pmatrix} 1 \\
1 \end{pmatrix}\bigr) = \begin{pmatrix} 0 \\
6 \end{pmatrix}
$$  

Hence, instead of redoing the entire matrix multiplication, you took advantage of the shortcut that **on eigenvectors**, $A$ is just a scalar.

**Caution**: You still need to initially find how $(-1,2)$ is represented in the eigenbasis. For large problems, that can itself be a significant computation (e.g., you might need the inverse of the eigenbasis matrix). So eigenvectors don’t magically remove **all** the work, but they can shift it to a more convenient step.

### **Key Takeaways**

1. **Definition**: Eigenvalues $\lambda$ and eigenvectors $\mathbf{v}$ satisfy  

$$
A \mathbf{v} = \lambda \mathbf{v}
$$

2. **Interpretation**: Eigenvectors are directions in which $A$ acts as a **pure stretch**. The factor of stretching is the eigenvalue.  
3. **Eigenbasis**: If enough eigenvectors exist to form a basis, you can rewrite any vector in terms of these eigenvectors, turning a big matrix multiplication problem into simpler scalar multiplications.  

**Question for Reflection**: Next time you see a transformation—be it a distortion of an image or a data transformation in machine learning—ask yourself: are there “natural directions” where this transformation is just a scaling? If so, you’ve just discovered its eigenvectors and eigenvalues. Embracing those can make your work far more efficient, especially for high-dimensional tasks.

> **Remember**: Learning eigenvalues and eigenvectors opens the door to efficient algorithms, clearer geometric intuition, and a powerful way to rewrite and simplify linear transformations.

---

## **Calculating Eigenvalues and Eigenvectors**

> **If you’ve ever stretched or squeezed a shape, have you noticed certain directions might remain “pure” stretches without shearing or rotating? How can we formally identify those directions and how much they stretch?** 

That’s where *eigenvalues* and *eigenvectors* come in.

### **Why Calculate Them?**

- **Simplicity**: Finding eigenvalues and eigenvectors can reveal the essential “stretch” factors of a matrix, making many problems easier.  
- **Applications in AI**: Principal Component Analysis, spectral clustering, and various machine learning methods all rely on computing eigenvalues and eigenvectors.  
- **Insights**: They help us see if a system (like a dynamical system) is stable, if it amplifies signals, or if it compresses them.

### **The Key Equations**

1. **Eigenvalue Equation**  
- A vector $\mathbf{v}$ is an eigenvector of a matrix $\mathbf{A}$ if  
   
$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}
$$

- where $\lambda$ is the eigenvalue. In words, $\mathbf{v}$ keeps its **direction** under $\mathbf{A}$—it’s simply scaled by $\lambda$.

2. **Characteristic Polynomial**  
- To find $\lambda$, rewrite the eigenvalue equation as  
   
$$
(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}
$$

- For a *nonzero* eigenvector $\mathbf{v}$ to exist, the matrix $(\mathbf{A} - \lambda \mathbf{I})$ must be **singular**. Equivalently,  
   
$$
\det(\mathbf{A} - \lambda \mathbf{I}) = 0
$$

- The resulting polynomial in $\lambda$ is the **characteristic polynomial**. Its roots are the eigenvalues.

### **Step-by-Step: A 2×2 Example**

Let’s take a simple 2×2 matrix:  

$$
\mathbf{A} = \begin{pmatrix}
2 & 1 \\
0 & 3
\end{pmatrix}
$$

1. **Write $(\mathbf{A} - \lambda \mathbf{I})$**  

$$
\mathbf{A} - \lambda \mathbf{I} = \begin{pmatrix}
2 - \lambda & 1 \\
0 & 3 - \lambda
\end{pmatrix}
$$

2. **Take the Determinant**  

$$
\det(\mathbf{A} - \lambda \mathbf{I}) = (2 - \lambda)(3 - \lambda) - (1 \cdot 0) = (2 - \lambda)(3 - \lambda)
$$

3. **Set It Equal to Zero**  

$$
(2 - \lambda)(3 - \lambda) = 0 \quad \Rightarrow \quad \lambda = 2 \quad \text{or} \quad \lambda = 3
$$

These are the **eigenvalues**. Next, we find the corresponding **eigenvectors**:

1. **For $\lambda = 2$**: Solve $(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \mathbf{0}$.  

$$
\begin{pmatrix}
2 - 2 & 1 \\
0 & 3 - 2
\end{pmatrix} = \begin{pmatrix}
0 & 1 \\
0 & 1
\end{pmatrix}
$$
- This matrix sends $(x,y)$ to $(y, y)$. For it to be $(0,0)$, we need $y = 0$. So any vector of the form $(x, 0)$ is a solution. A convenient choice is $(1,0)$.

2. **For $\lambda = 3$**: Solve $(\mathbf{A} - 3\mathbf{I})\mathbf{v} = \mathbf{0}$.  

$$
\begin{pmatrix}
2 - 3 & 1 \\
0 & 3 - 3
\end{pmatrix} = \begin{pmatrix}
-1 & 1 \\
0 & 0
\end{pmatrix}
$$

- This requires $-x + y = 0 \Rightarrow y = x$. So one valid vector is $(1,1)$.

So the eigenvalues are $2$ and $3$, with corresponding eigenvectors $(1,0)$ and $(1,1)$ (or any nonzero scalar multiples).

### **A 3×3 Example in Brief**

For a 3×3 matrix, say  

$$
\mathbf{B} = \begin{pmatrix}
2 & 1 & -1 \\
1 & 0 & -3 \\
-1 & -3 & 0
\end{pmatrix}
$$

we form  

$$
\mathbf{B} - \lambda \mathbf{I} = \begin{pmatrix}
2-\lambda & 1 & -1 \\
1 & -\lambda & -3 \\
-1 & -3 & -\lambda
\end{pmatrix}
$$

Then,  

$$
\det(\mathbf{B} - \lambda \mathbf{I}) = 0
$$

gives a *cubic* characteristic polynomial. Solve for $\lambda$ (there might be up to 3 real or complex roots), then find the eigenvectors by substituting each root back into $(\mathbf{B} - \lambda \mathbf{I})\mathbf{v} = \mathbf{0}$.

### **Infinite Solutions and Eigenvectors**

Why do we get infinitely many solutions for each eigenvalue? Because eigenvectors are defined **up to a scalar multiple**—if $(1,0)$ is an eigenvector, so is $(5,0)$ or $(-3,0)$. They all lie on the same line, which is the direction that remains unchanged (up to scaling).

### **What If the Matrix Isn’t Square?**

- **Determinants** are only defined for square matrices.  
- Therefore, **eigenvalues** and **eigenvectors** only make sense for square matrices.  
- For rectangular matrices, we have related concepts (like **singular values** and **singular vectors** in the singular value decomposition), but they’re not called eigenvalues/eigenvectors.

### **Avoiding Common Pitfalls**

1. **Multiplicity**: An eigenvalue can appear more than once (repeated root). Sometimes, one eigenvalue has more than one independent eigenvector (or *less* than expected, which complicates diagonalization).  

2. **Complex Eigenvalues**: For real-valued matrices, you might still get complex solutions for $\lambda$. In that case, your eigenvectors live in a complex vector space.  
3. **Zero Eigenvalue**: This indicates $\mathbf{A}$ is singular. The presence of $\lambda = 0$ implies a non-invertible matrix.

### **Why This Matters (Again)**

- **Diagonalization**: If a matrix has a full set of $n$ independent eigenvectors (for an $n \times n$ matrix), it can be turned into a simpler diagonal form. This makes many operations (like exponentiating the matrix) much easier.  
- **PCA & Machine Learning**: In analyzing data, finding eigenvalues of the *covariance matrix* pinpoints directions of greatest variance.  
- **Physical Interpretations**: In vibrations or quantum mechanics, eigenvalues can represent fundamental frequencies or energy levels.

### **Final Roadmap for Calculation**

To recap the general process:

1. **Write** $(\mathbf{A} - \lambda \mathbf{I})$.  
2. **Compute** the determinant to get the characteristic polynomial.  
3. **Factor/Solve** for eigenvalues $\lambda_i$.  
4. **For each $\lambda_i$**, solve $(\mathbf{A} - \lambda_i \mathbf{I})\mathbf{v} = \mathbf{0}$ to get the eigenvector(s).  
5. **Normalize (Optional)**: Often we pick a convenient scalar multiple (like making the vector have length 1 or some integer ratio).

### **Key Takeaways**

- **Eigenvalues**: The “stretch factors” by which each eigenvector is scaled.  
- **Eigenvectors**: Special **directions** that remain aligned with themselves under the matrix.  
- **Characteristic Polynomial**: The bridge to finding these special values.  
- **Infinite Solutions**: Each eigenvalue yields a line (or plane, etc.) of possible eigenvectors.  

- **Only Square Matrices**: You need a determinant to define eigenvalues, so the matrix must be square.

By following these steps, you can quickly break down seemingly complicated linear transformations and uncover the elegantly simple stretching directions hidden within—exactly what’s used in advanced AI methods like principal component analysis.

---

## **A Deeper Dive into Eigenvalues and Eigenvectors**

Many people find eigenvalues and eigenvectors confusing and have trouble understanding them intuitively. To help clarify these concepts, I’ve created an additional section.

### **Setting the Stage: Matrices as Transformations**

When you see a matrix in a typical math or AI course, it’s easy to get stuck in an “array of numbers” mindset. But the key to truly understanding eigenvalues and eigenvectors is to **interpret** a matrix as a **linear transformation**:

- In **2D**, each 2×2 matrix describes how you move or distort points on a plane.  
- In **3D**, each 3×3 matrix describes how you transform all points in 3D space.

Viewing matrices this way transforms abstract symbols into **actions** on space—stretching, compressing, reflecting, rotating, and more.

### **Where Eigenvectors Come In**

Most vectors get “moved” or “rotated” off of their original lines when you apply a matrix. However, certain **special vectors** stay on their own span (i.e., the line from the origin through that vector) even after the transformation. These are **eigenvectors**.

#### **Staying on the Same Line**

- If you apply the matrix $A$ to an eigenvector $\mathbf{v}$, the result $A\mathbf{v}$ will lie **somewhere** along the same line as $\mathbf{v}$.  
- Mathematically, this “stay-on-the-line” condition is expressed by  

$$
A\mathbf{v} = \lambda \mathbf{v}
$$

where $\lambda$ is a scalar. We call $\lambda$ the **eigenvalue** associated with the eigenvector $\mathbf{v}$.

#### **Interpreting the Eigenvalue**

- If $\lambda = 3$, $\mathbf{v}$ is **stretched** to triple its length.  
- If $\lambda = 0.5$, $\mathbf{v}$ is **squished** to half its length.  
- If $\lambda$ is **negative**, that indicates a **flip** (the direction is reversed) **and** a scale by the absolute value of $\lambda$.  

In 2D examples, you might find that one line is stretched by $\lambda_1$ and another is stretched by $\lambda_2$. In 3D, you could have multiple eigenvalues, each associated with a different direction in space.

### **Concrete Examples**

#### **A Simple 2D Matrix**
Let’s say $A$ sends:
- the vector $(1,0)$(i-hat) to $(3,0)$.  
- the vector $(0,1)$(j-hat) to $(1,2)$.

That means  

$$
A = \begin{pmatrix}
3 & 1 \\
0 & 2
\end{pmatrix}
$$

where the first column is the image of $\mathbf{i}$-hat and the second column is the image of $\mathbf{j}$-hat.

1. **Observing a potential eigenvector:**  
- $\mathbf{i}$-hat $(1,0)$ is actually mapped to $(3,0)$, which stays on the $x$-axis. So $\mathbf{i}$-hat is an eigenvector with eigenvalue 3.  
2. **Finding another eigenvector:**  
- By solving $\det(A - \lambda I) = 0$, you might find $\lambda = 2$ is another eigenvalue. A corresponding eigenvector could be  
   
$$\begin{pmatrix} -1 \\
1 \end{pmatrix}
$$

- which remains on its own diagonal line, scaled by 2.

#### **Rotation in 3D**
- A rotation matrix around some axis doesn’t change any point **exactly** on that axis (the axis is “fixed”), meaning vectors along that axis are eigenvectors with $\lambda = 1$.  
- Every other vector is rotated off of its line, so no other real eigenvectors exist. This underscores the idea that finding **one** axis of rotation is simpler than juggling a full 3×3 matrix.

#### **Rotation in 2D (90°)**
- A pure 90° rotation in the plane has **no real** eigenvectors. Every vector is turned off its original line, meaning no direction remains purely “scaled.”  
- Algebraically, you see this when the polynomial $\det(A - \lambda I)$ yields **complex roots** ($\pm i$), signaling no real eigenvalues.

#### **Shear**
- In a shear, say  

$$
\begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}
$$

- every vector except those on the $x$-axis gets “pushed” diagonally.  
- The $x$-axis vectors remain where they are (so they’re eigenvectors with $\lambda=1$). However, there may **not** be a second distinct eigenvector to span the plane.  

#### **Uniform Scaling**

- If you have a matrix like  

$$
\begin{pmatrix}
2 & 0 \\
0 & 2
\end{pmatrix}
$$

- it **scales everything** by 2, so literally **any** vector is an eigenvector with eigenvalue 2.  

### **The Algebra Behind Finding Them**

1. **Rewrite the condition $A\mathbf{v} = \lambda \mathbf{v}$ as**  

$$
(A - \lambda I)\mathbf{v} = \mathbf{0}
$$

2. **Non-zero $\mathbf{v}$ requires $\det(A - \lambda I) = 0$.**  
   This means the transformation $A - \lambda I$ is **singular**—it “collapses” the space to a lower dimension, leaving at least one non-zero vector in its null space.  
3. **Solve for $\lambda$** by making $\det(A - \lambda I) = 0$. These are your **eigenvalues**.  
4. **For each eigenvalue $\lambda$,** solve $(A - \lambda I)\mathbf{v} = \mathbf{0}$ to find the corresponding **eigenvector(s)**.

### **Eigenbasis and Diagonalization**

If a 2D or 3D matrix has enough eigenvectors to form a **basis** (i.e., a full set of independent eigenvectors spanning the entire space), then in that new coordinate system, the matrix becomes **diagonal**:  

$$
\begin{pmatrix}
\lambda_1 & 0 & 0 \\
0 & \lambda_2 & 0 \\
0 & 0 & \lambda_3
\end{pmatrix}
\quad\text{(or just 2×2 for 2D)}
$$

**Why is that awesome?**
- **Matrix powers**: Computing $A^{100}$ is a breeze when $A$ is diagonal; you just raise each diagonal entry to the 100th power.  
- **Interpretation**: A diagonal matrix is simply a transformation that independently scales each basis vector.  
- **Simplicity**: Many tasks in machine learning (like PCA) or other numeric algorithms rely on diagonalization or related techniques (like the Singular Value Decomposition) to simplify computations.

**Note**: Not every matrix can be fully diagonalized (e.g., a shear matrix may have only one eigenvector). But when you **can** diagonalize a matrix, it’s a powerful tool.

### **Why Does This Matter?**

1. **3D Rotations**: As mentioned, eigenvectors can reveal an **axis of rotation**, which is invaluable for geometric understanding or 3D computations.  
2. **Principal Component Analysis (PCA)**: In machine learning, finding principal axes (eigenvectors of the covariance matrix) helps reduce dimensionality and identify the main “directions” of variation in data.  
3. **Repeated Transformations**: Whether you’re doing iterative methods in computational math or analyzing repeated transformations in a simulation, eigen-decomposition can drastically simplify what might otherwise be an **intractable** process.

### **Putting It All Together**

- **Eigenvectors** are directions in which a transformation acts by simple scaling (including potential flips), rather than any rotation.
- **Eigenvalues** are the factors of that scaling.
- **Diagonalization** (or more advanced forms like Jordan normal form) leverages eigenvectors for deep simplifications of matrix operations.

In essence, **eigenvalues** and **eigenvectors** clarify a transformation’s most **natural** or **fundamental** properties: how it *really* behaves, beyond the specifics of any single coordinate system. This is why they show up everywhere in advanced mathematics, physics, and AI: they isolate the transformation’s essential actions, making complex problems more approachable and more elegant to solve.

### **Further Exploration**

- **Complex Eigenvalues**: Rotations in 2D yield complex eigenvalues with no real eigenvectors, but they still matter for analyzing transformations in the complex plane.  
- **Defective Matrices**: Some matrices have fewer eigenvectors than the dimension of the space (like certain shears), requiring a broader framework (Jordan forms) to handle them.  
- **Applications**: Eigen-decomposition is a bedrock for everything from solving differential equations to Google’s PageRank algorithm and modern neural network initialization strategies.

Ultimately, eigenvalues and eigenvectors are a **bridge** between the numeric representation of a transformation (the matrix) and its **geometric meaning** (stretch, flip, rotate, or reduce). Grasping this bridge unlocks a remarkable amount of power in both theoretical and applied problems.

---

## **On the Number of Eigenvalues**

> **If we pick a 3×3 matrix, do we always get three different eigenvalues—and does that automatically guarantee three separate eigenvectors?** 

Let’s investigate how *repeated eigenvalues* might complicate things.

### **A Quick Refresher**

- An **eigenvalue** $\lambda$ of a square matrix $\mathbf{A}$ is a solution to  

$$
\det(\mathbf{A} - \lambda \mathbf{I}) = 0
$$

- The corresponding **eigenvectors** are nonzero vectors $\mathbf{v}$ that satisfy  

$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}
$$

- The **number of times** a particular eigenvalue appears as a root of the characteristic polynomial is called its **algebraic multiplicity**.  
- The number of **independent** eigenvectors you actually get for that eigenvalue is called its **geometric multiplicity**, and it can be *less* than the algebraic multiplicity.

### **Repeated Eigenvalues in 3×3 Matrices**

#### **Example: Two Distinct Directions for the Same Eigenvalue**

Suppose you have a 3×3 matrix $\mathbf{A}$ whose characteristic polynomial yields eigenvalues $\lambda_1 = 4$, $\lambda_2 = 2$, and again $\lambda_3 = 2$ (so “2” is repeated). One might worry that having the eigenvalue 2 twice means we won’t get enough eigenvectors. But in **some** cases, you still can:

1. **Eigenvalue 4**: You solve $(\mathbf{A} - 4\mathbf{I})\mathbf{v} = \mathbf{0}$ and get a vector $\mathbf{v}_1$.  
2. **Eigenvalue 2 (First Direction)**: You plug $\lambda = 2$ into $(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \mathbf{0}$ and find one eigenvector $\mathbf{v}_2$.  
3. **Eigenvalue 2 (Second Direction)**: Interestingly, because the space of solutions might be 2D (or bigger) for that repeated root, you can find **another** eigenvector $\mathbf{v}_3$ that isn’t just a scalar multiple of $\mathbf{v}_2$.

Hence, you end up with **three** linearly independent eigenvectors, even though one eigenvalue is repeated.

#### **Example: Only One Direction for a Repeated Eigenvalue**

In other situations, you might again get $\lambda_1 = 4$, $\lambda_2 = 2$, $\lambda_3 = 2$, *but* when you solve for $\mathbf{v}$ corresponding to $\lambda=2$, you discover that **only one** independent eigenvector emerges. All the solutions for that eigenvalue lie along a single line in 3D space—meaning the geometric multiplicity of “2” is just 1.

This time, you’ll find:
- **Eigenvalue 4** gives you one independent direction.  
- **Eigenvalue 2** also appears twice in the polynomial, but yields **only one** unique direction.  

So in total, you have just **two** independent eigenvectors for a 3D matrix. You can’t construct a basis of three eigenvectors—thus, the matrix is *not* diagonalizable, and you don’t have a full *eigenbasis*.

### **General Patterns**

1. **2×2 Matrix**:
   - If the two eigenvalues are **different**, you’ll have two distinct eigenvectors—enough for a full eigenbasis in 2D.  

   - If there’s a **repeated** eigenvalue (e.g., both eigenvalues are 3), you might find:
     - **Two** independent eigenvectors, or  
     - **One** eigenvector (then you can’t form a full basis).

2. **3×3 Matrix**:
   - **Three Distinct Eigenvalues**: Always at least one eigenvector per eigenvalue, so you get three independent eigenvectors.  
   - **One Eigenvalue Repeated Twice**: You might have **two** or **three** total independent eigenvectors.  
   - **One Eigenvalue Repeated Three Times**: You could end up with 1, 2, or 3 independent eigenvectors for that single eigenvalue.

The crucial point is the distinction between **algebraic multiplicity** (how many times the eigenvalue shows up in the characteristic polynomial) and **geometric multiplicity** (how many independent vectors correspond to it). A matrix is **diagonalizable** if and only if, for every eigenvalue, **geometric multiplicity = algebraic multiplicity**.

### **Why Care About Repeated Eigenvalues?**

- **Basis Formation**: Repeated eigenvalues might or might not give enough independent directions to span your entire space.  
- **Diagonalization**: If you can’t find a full set of independent eigenvectors, you can’t write $\mathbf{A} = \mathbf{PDP}^{-1}$ in diagonal form.  
- **Complexity in Systems**: In some physical or dynamical systems, repeated eigenvalues can indicate resonance or degenerate modes of behavior.  
- **Eigenvalue Decomposition in AI**: PCA or other methods rely on large covariance matrices where repeated eigenvalues might cluster or hamper direct interpretations of principal components.

### **Key Takeaways**

1. **Eigenvalues can repeat**, and each repeated eigenvalue may provide fewer (or equal) independent eigenvectors than its multiplicity.  
2. **Different outcomes**: Sometimes a repeated eigenvalue yields multiple distinct directions; other times it yields just one.  
3. **Full Eigenbasis**: Only possible when the *total* number of independent eigenvectors adds up to the dimension of your space (e.g., 2 for 2D, 3 for 3D).  
4. **Practical Check**: If you suspect a repeated eigenvalue, solve $(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = \mathbf{0}$ carefully. You might find multiple independent solutions—or just one line of solutions.

Remember: an eigenvalue’s algebraic multiplicity (how many times it appears as a root) doesn’t necessarily match its geometric multiplicity (how many independent vectors it provides). That difference lies at the heart of whether a matrix is easily diagonalizable or not!

---

## **Dimensionality Reduction and Projection**

> **Have you ever wished you could compress a huge spreadsheet of data into a smaller one—without completely losing the valuable information hidden inside?**

### **Why Reduce Dimensionality?**

When dealing with high-dimensional data (i.e., many columns or features), there are two main reasons to reduce the number of dimensions:

1. **Manageability**  
   - Some datasets have hundreds or thousands of features, making them computationally expensive to process.  
   - Smaller datasets are more efficient to store and analyze.
2. **Visualization**  
   - It’s easier to visually explore data when working in just 2D or 3D.  
   - Most common charts (e.g., scatter plots, bar charts) focus on at most a few dimensions at a time.  
   - Reducing to fewer dimensions can reveal patterns more clearly.

#### **A Naïve Approach: Deleting Columns**

A quick way to reduce dimensions is to simply remove columns. While this certainly makes your data *skinnier*, you lose all information in those deleted features. This might cause you to miss out on crucial insights.

**Enter PCA**: *Principal Component Analysis* is a technique that **systematically** reduces dimensions while trying to preserve as much **variation** (or information) in the data as possible.

### **The Idea of Projections**

Before diving into PCA, let’s understand how data can be **projected** onto fewer dimensions:

1. **Current Data**: Suppose you have a table with several features (columns). In 2D, you might have an $(x,y)$ coordinate for each data point.
2. **Target Subspace**: You want to “project” these points onto a **line**, or in higher cases, onto a lower-dimensional subspace (a plane, etc.).

#### **A 2D Example**

Imagine you have points in the plane like $(1, 1)$, $(1.2, 1.6)$, $(-0.5, 0.2)$, $(-1.3, -0.6)$, and you wish to project them onto the line $y = x$.

##### **Why Use the Line $y = x$ as an Example?**

1. **A Simple Illustration of Projection**  

   - The line $y = x$ is an easy one to visualize in a 2D plane. By projecting points onto that diagonal, you can show exactly how a higher-dimensional point (two coordinates) collapses into a single number.  
   - It’s not necessarily the “best” line in terms of capturing data variance (that’s what PCA is all about), but it **is** a straightforward line that demonstrates how projection actually works.  

2. **Preparing for the General Concept**  
   - In PCA, you look for directions (lines in 2D, planes or hyperplanes in higher dimensions) that maximize variance. The idea of “sliding” each point onto some subspace is the **core** mechanism.  
     - More Variance $\Rightarrow$ More Information : Imagine you have a bunch of 3D objects scattered on a table in a dark room. You shine a flashlight from various angles to cast their shadows onto a wall (which represents a 2D projection). If you angle your flashlight so that the shadows spread out broadly on the wall, you can distinguish which objects are where. Each object casts a shadow in a slightly different place, so you see more “detail” about how the objects differ. This corresponds to maximizing variance—the direction of the light (i.e., how you project) captures the biggest differences among objects. If you shine the flashlight from a bad angle, maybe nearly parallel to the table, the shadows overlap heavily, or look like one thin line. You lose a ton of information—how many objects are there, how far apart they are, etc. This is like picking a low-variance direction. Everything looks almost the same from that viewpoint, and you can’t tell them apart. In PCA terms, you’re choosing the “angle” (the principal component direction) that creates the widest spread of shadows—so the data points (your objects) look more distinct in the lower-dimensional space.
     - When I say variance in this context, I mean the spread of the data along a particular direction. A high-variance direction is one where your data points differ from each other the most. Intuitively:
       - If you keep the direction where data is spread out the most, you’ll capture the largest share of the overall “differences” in the data.
       - Conversely, if you pick a direction with low variance, all the data points will be bunched close together along that axis—meaning they all look very “similar” from that viewpoint. You learn less about how the data actually varies.
       - By projecting onto directions of maximum variance, PCA ensures you retain the most information about how your data differs across observations.
     - There’s another way to see why “max variance” is crucial:
       - Imagine you want to compress the data into fewer dimensions (e.g., from 10 features down to 2) while losing as little information as possible.
       - The “information loss” can be thought of as reconstruction error—how far off you’d be if you tried to reconstruct the original data using only those 2 dimensions.
       - Mathematically, picking the direction of maximum variance is equivalent to minimizing how much data is “squashed” or “distorted” when you project it.
     - In other words, focusing on maximum variance directions is just another way of saying you want to keep the directions that preserve most of the unique differences between points.

   - Using $y = x$ provides a clear, concrete practice run: we can easily compute dot products with a known vector $\mathbf{v} = (1,1)$. Then once people grasp that concept, it’s easier to generalize to “any line” or “any set of directions” in higher dimensions.

3. **Visual Familiarity**  
   - The line $y = x$ looks like a 45° diagonal when you plot it. Most people quickly understand that projecting onto this diagonal means each point’s two coordinates get collapsed into a single axis along that diagonal. It’s **visually** simpler than picking a more arbitrary line like $y = 2x + 3$.

#### **Dot Product Trick**

- The line $y = x$ can be represented by the vector $\mathbf{v} = (1,1)$.  
- However, $\mathbf{v}$ is not a **unit** vector. its length is $\sqrt{2}$.  
- To avoid “stretching,” we first **normalize** $\mathbf{v}$ by dividing by its length $||\mathbf{v}||_2 = \sqrt{2}$. This yields:  

$$
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 \\
1
\end{pmatrix}
$$

- For each data point $(x,y)$, we do a **dot product** with the normalized vector:  

$$
(x,y) \cdot \frac{1}{\sqrt{2}}(1,1) = \frac{x + y}{\sqrt{2}}
$$

- This single number $\frac{x+y}{\sqrt{2}}$ tells us the location of the projected point along the $y = x$ line.  

- Another example:  
```python
# Without normalization
Original point: (3,4)
Projection value: 3 + 4 = 7
# This is too large! The point (3,4) has actual length of 5,
# but we're getting 7 which is distorted

# With normalization
Original point: (3,4)
Projection value: (3 + 4)/√2 ≈ 4.95
# Much better! This value is closer to the true length of 5
```

Result: Instead of needing two coordinates $(x,y)$, each point is now described by just **one** coordinate! You can think of this as a **compression** of the data.

### **Generalizing to More Dimensions**

1. **Original Data Matrix $\mathbf{A}$**  
- Has $r$ rows (observations) and $c$ columns (features).  

2. **Projection Matrix $\mathbf{V}$**  
- Suppose you want to project onto $k$ directions (where $k < c$).  
- Each of these $k$ directions is a **normalized** vector of length $c$.  
- Stack these $k$ direction vectors as columns in $\mathbf{V}$, so $\mathbf{V}$ is $c \times k$.  

3. **Projected Data**  
- The new, “compressed” data $\mathbf{A}_\mathbf{p}$ is given by  
   
$$
\mathbf{A}_\mathbf{p} = \mathbf{A} \mathbf{V}
$$

- Dimensions of $\mathbf{A}_\mathbf{p}$: $r \times k$.  
- You still have $r$ observations, but only $k$ features instead of $c$.

### **Why It Works**

- By choosing these $k$ directions cleverly (PCA will show you how), you preserve the *most* essential structure or variance in your dataset.  

- This is much better than blindly deleting columns, since you combine existing features into *fewer* new features that capture the most important patterns.

### **Real-World Examples**

1. **Customer Data**  

   - Maybe you have dozens of demographic and usage statistics. You want to find the main “axes” of variation—like *age vs. spending habits*, or *account age vs. login frequency*.  
   - Projecting onto those axes compresses your dataset while keeping the major correlations.

2. **Image Compression**  
   - Large images have thousands of pixels (features). You can project them into a smaller space that still retains most of the visual essence.  

3. **Text Analytics**  
   - Documents with thousands of word-frequency features can be projected onto fewer dimensions that still capture main “topics” or themes.

### **Key Takeaways Before PCA**

- **Dimensionality Reduction**: Slashes the number of features while often retaining meaningful relationships.  
- **Projection**: Maps data onto lower-dimensional subspaces, e.g., a line, a plane, or a more general $k$-dimensional subspace.  
- **Normalization**: Ensure vectors you project onto are unit-length, so you don’t artificially stretch or shrink the data.  
- **Matrix Multiplication**: Once you pick your projection vectors, the entire operation is a simple matrix multiplication $\mathbf{A}_\mathbf{p} = \mathbf{A}\mathbf{V}$.

Next up, we’ll see **how** to pick those “best” directions for preserving information—hint: it’s all about **variance** and eigenvalues, which leads us to **Principal Component Analysis (PCA)**.

---

## **Motivating PCA**

> **Have you ever tried fitting a wide painting onto a narrow wall and found yourself rotating or tilting it to keep the most crucial part visible? In much the same way, **Principal Component Analysis (PCA)** aims to reduce the “width” of your data (dimensions) while preserving its “most important” information.**

### **Idea: Projecting 2D to 1D**

Suppose you have data points in a 2D plane—each dot represents an observation with two features $(x, y)$. Reducing this dataset from 2D to 1D means projecting each point onto some chosen line. For instance:

1. **Projecting onto the $x$-axis**  
   - Each point’s position becomes $(x, 0)$.  
   - You end up with a 1D line of points, but do you keep much of the variation in the $y$ direction?

2. **Projecting onto the $y$-axis**  
   - Each point’s position becomes $(0, y)$.  
   - Some of the differences in the $x$ direction might get “squashed.”

3. **Projecting onto Another Line**  
   - For example, onto the line $y + x = 0$ (or equivalently the direction vector (1, -1)).  
   - Different lines capture different aspects of your data—some might preserve more “spread” of the points than others.

### **Centering the Data**

Often, before projecting, we **center** the data around $(0,0)$. This means subtracting the mean of $x$ from all $x$-values and the mean of $y$ from all $y$-values. Centering helps ensure that the chosen projection is purely about the data’s variance, not about an offset from the origin.

#### **Preserving “Spread” = Preserving Information**

Once you pick a line and project all points onto it, your points will typically cluster or spread out along that 1D axis. 

- **High Spread**: If the points lie broadly apart after projection, you retain more of the original differences among them.  
- **Low Spread**: If points clump together, you lose more detail, and different observations might appear nearly identical on the line.

#### **Example Visuals**

- **$x$-Axis Projection**: Might show moderate spread if $x$ values differ greatly.  
- **$y$-Axis Projection**: If $y$ values don’t vary much, the projection collapses many points into a small range.  
- **Diagonal or Custom Line**: Could align better with the data’s natural shape, preserving the largest variation.

### **Ranking Projections by Spread**

After projecting, you can measure how “spread out” the data is—often by the **variance** of the projected points. If you compare several potential lines:

1. **Line with Maximum Variance**: The 1D projection that keeps the data points as far apart as possible (i.e., maximum spread).  
2. **Lines with Lower Variance**: Might compress the data more and lose details.

**Key Insight**: PCA will **automatically** find the direction(s) in which your data has the greatest variance, ensuring you preserve the largest possible amount of information when dropping dimensions.

### **Benefits of Dimensionality Reduction**

1. **Smaller Datasets**: Fewer features to store, process, and manage.  
2. **Minimize Information Loss**: PCA finds the “best” directions to project onto, maintaining maximum variance.  
3. **Easier Visualization**: Once you reduce to 2D or 3D, you can create scatter plots or other charts to spot patterns more clearly.

### **Looking Ahead**

This brief overview shows *why* projecting onto certain directions matters. The next step is seeing **how** PCA decides which directions capture the most variance—by looking at:

- **Covariance Matrices** and **Variance** calculations.  

- **Eigenvalues** and **Eigenvectors** of these matrices.  

By connecting the “spread” of your data to these mathematical tools, PCA systematically identifies the principal components—those directions along which your data “varies” the most. Stay curious and enjoy discovering how just a bit of linear algebra can unlock big insights from high-dimensional data!

---

## **Variance and Covariance**

> **Have you ever looked at a set of numbers—like exam scores or daily temperatures—and wondered “How spread out are these values, and do two different sets of values move together or in opposite directions?”**

### **Mean: The Foundation**

Before we dive into **variance** and **covariance**, let’s recall the **mean**. For a set of $n$ data points, $x_1, x_2, \dots, x_n$, the mean $\mu_x$ is  

$$
\mu_x = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

- Intuitively, the mean is the “center of mass” of our data.  

In 2D points $(x_i, y_i)$, the mean is $(\mu_x, \mu_y)$:  

$$
\mu_x = \frac{1}{n}\sum_{i=1}^n x_i, 
\quad
\mu_y = \frac{1}{n}\sum_{i=1}^n y_i
$$

### **Variance: Measuring Spread**

#### **Definition**

**Variance** of a single variable $x$ measures *how far* the data points spread out from their mean $\mu_x$. Formally, the sample variance is  

$$
\text{Var}(x) = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \mu_x)^2
$$

- You might see this called the **average squared distance from the mean**.  
- Why “squared”? It penalizes points further away more heavily and avoids canceling out positive/negative differences.

#### **Example**

Suppose $x$ is [10, 4, 11, 14, 6] and $n = 5$.  
1. **Mean**: $\mu_x = 9$  
2. **Differences**:  
   - (10 - 9) = 1, (4 - 9) = -5, (11 - 9) = 2, (14 - 9) = 5, (6 - 9) = -3  
3. **Squared Differences**: $1^2, (-5)^2, 2^2, 5^2, (-3)^2 = 1, 25, 4, 25, 9$ 
4. **Sum**: 1 + 25 + 4 + 25 + 9 = 64
5. **Divide by $n-1$**: 64 / 4 = 16  

Hence, $\text{Var}(x) = 16$. A bigger variance means data is more spread out.

> **Note**: The denominator is $n-1$ because we’re estimating the variance from a sample of the population. If you have the entire population, you’d use $n$ instead. When you’re using the entire population, the denominator is $n$. It is related to the **degrees of freedom** in the sample. Refer to [ch 006.Hello! Machine Learning](025.Beyond Supervised Learning(14)) for more details.

### **Covariance: Measuring Joint Variation**

#### **Motivation**

What if you have two variables, $x$ and $y$? They might each have their own variance, but do they *move together*?  

- If $x$ goes up when $y$ goes up, we might see a **positive** relationship (e.g., height vs. shoe size).  
- If $x$ goes up when $y$ goes down, we might see a **negative** relationship (e.g., speed vs. travel time).  
- If they don’t follow a clear trend, the relationship might be near **zero**.

**Covariance** captures the *direction* of the linear relationship between two variables.

#### **Formula**

For two variables $x$ and $y$, their sample covariance is  

$$
\text{Cov}(x,y) = \frac{1}{n - 1}\sum_{i=1}^{n} (x_i - \mu_x)(y_i - \mu_y)
$$

- Notice it’s very similar to variance, except we multiply the “centered” $x$ values by the “centered” $y$ values.  
- If $x_i$ and $y_i$ tend to deviate from their means in the *same* direction (both above or both below), the product is positive overall.  
- If they deviate in *opposite* directions (one above its mean, the other below), the product is negative.

#### **Interpretation**

- **Positive Covariance**: As $x$ increases, $y$ also tends to increase (points slant *upward*).  
- **Negative Covariance**: As $x$ increases, $y$ tends to decrease (points slant *downward*).  
- **Covariance near 0**: $x$ and $y$ show no clear linear trend.

### **Visualizing Covariance Quadrants**

Imagine a scatter plot of points $(x_i, y_i)$ with mean $(\mu_x, \mu_y)$. Centering your data around this mean splits the plane into four quadrants:

1. **Upper-Right Quadrant**: $x_i > \mu_x$ and $y_i > \mu_y$ (positive $\times$ positive).  
2. **Upper-Left Quadrant**: $x_i < \mu_x$ but $y_i > \mu_y$ (negative $\times$ positive).  
3. **Lower-Left Quadrant**: $x_i < \mu_x$ and $y_i < \mu_y$ (negative $\times$ negative).  
4. **Lower-Right Quadrant**: $x_i > \mu_x$ but $y_i < \mu_y$ (positive $\times$ negative).

Points in quadrants 1 or 3 contribute *positively* to $\text{Cov}(x,y)$. Points in quadrants 2 or 4 contribute *negatively*. The overall sum (and then division by $n-1$) tells you if you have *more* positive or negative contributions on average.

### **Recap and Why It Matters for PCA**

- **Variance** shows how a single feature is spread.  
- **Covariance** captures how two features change together (or against each other).

#### **Connection to PCA**

Principal Component Analysis uses the **covariance matrix** of your dataset to find directions in which your data varies the most (i.e., has the largest variance). By combining variance (the “spread” of each variable) and covariance (how variables move together), PCA identifies the “principal components” that best represent your data’s overall patterns.

**In short**: Variance and covariance are building blocks for understanding data spread and relationships—a crucial step before we dive deeper into how PCA selects its principal components.

Stay curious, and remember:  
- **Variance** answers “How far apart are the data from the mean?”  
- **Covariance** answers “When one variable shifts from its mean, does another tend to do the same—or the opposite?”

These insights are vital for advanced data analysis, especially in AI and machine learning, where extracting meaningful patterns from high-dimensional data can make all the difference.

---

## **Covariance Matrices**

> **Have you ever collected data with multiple features—like height, weight, and age—and wondered how to capture *all* pairwise relationships (e.g., which features vary together or oppose each other) in one place?**

### **Why Build a Covariance Matrix?**

We’ve seen how **variance** measures the spread of a single variable and **covariance** measures how pairs of variables move together (positively, negatively, or not at all). But what if you have several variables—say $x$, $y$, and $z$—and you want *all* of these relationships in one snapshot?

That’s where the **covariance matrix** comes in. It’s a square matrix storing:

- Along the **diagonal**: The variances of each individual variable (e.g., $\text{Var}(x)$, $\text{Var}(y)$, …).  
- In the **off-diagonal** entries: The covariances between every pair (e.g., $\text{Cov}(x,y)$, $\text{Cov}(y,x)$, etc.).

Hence, a covariance matrix succinctly organizes how each variable relates to itself (spread) and to each other variable.

### **The 2D Example**

#### **Intuitive Understanding**

Suppose you have two variables, $x$ and $y$. From previous lessons, you know:

- $\text{Var}(x)$ is the spread of the $x$ values,  
- $\text{Var}(y)$ is the spread of the $y$ values,  
- $\text{Cov}(x,y)$ measures how $x$ and $y$ move in tandem (positive, negative, or none).

A **2×2 covariance matrix** for $(x,y)$ then looks like this:  

$$
\mathbf{C} = \begin{pmatrix}
\text{Var}(x) & \text{Cov}(x,y) \\
\text{Cov}(y,x) & \text{Var}(y)
\end{pmatrix}
$$

Because $\text{Cov}(x,y) = \text{Cov}(y,x)$, the matrix is *symmetric*. The diagonal entries $\text{Cov}(x,x)$ and $\text{Cov}(y,y)$ are just the *variances* of $x$ and $y$, respectively.

#### **Example Datasets**

Imagine three scatter plots, each with the same $\text{Var}(x)$ and $\text{Var}(y)$ but different trends:

1. **Negative Trend**: Covariance $= -2$.  
2. **No Trend**: Covariance $= 0$.  
3. **Positive Trend**: Covariance $= +2$.

Corresponding covariance matrices might be:  

$$
\begin{pmatrix}
3 & -2 \\
-2 & 1
\end{pmatrix}
$$  

$$
\begin{pmatrix}
3 & 0 \\
0 & 1
\end{pmatrix}
$$  

$$
\begin{pmatrix}
3 & 2 \\
2 & 1
\end{pmatrix}
$$

Here, the diagonal (3 and 1) represent the variances of $x$ and $y$, while the off-diagonal shows how $x$ and $y$ co-vary.

### **General Formula in Matrix Form**

#### Data in Matrix $\mathbf{A}$

Let’s say you have $n$ observations and $d$ features. You place them in a matrix $\mathbf{A}$ of shape $n \times d$:

- Each **row** is an observation.  

- Each **column** is one variable (feature).

For example, with $d=2$ (features $x$ and $y$), $\mathbf{A}$ might look like:  

$$
\mathbf{A} = \begin{pmatrix}
x_1 & y_1 \\
x_2 & y_2 \\
\vdots & \vdots \\
x_n & y_n \end{pmatrix}
$$

#### Mean Matrix $\boldsymbol{\mu}$

Compute the mean of each column (variable):  

$$
\mu_x = \frac{1}{n}\sum_{i=1}^n x_i, \quad
\mu_y = \frac{1}{n}\sum_{i=1}^n y_i
$$

Then create a matrix $\boldsymbol{\mu}$ of the same shape as $\mathbf{A}$, where each entry in column $x$ is $\mu_x$, and each entry in column $y$ is $\mu_y$:  

$$
\boldsymbol{\mu} = \begin{pmatrix}
\mu_x & \mu_y \\
\mu_x & \mu_y \\
\vdots & \vdots \\
\mu_x & \mu_y
\end{pmatrix}
$$

#### **Covariance Matrix Calculation**

The covariance matrix $\mathbf{C}$ is given by:  

$$
\mathbf{C} = \frac{1}{n - 1} (\mathbf{A} - \boldsymbol{\mu})^\top (\mathbf{A} - \boldsymbol{\mu})
$$

1. **Center the Data**: $(\mathbf{A} - \boldsymbol{\mu})$ subtracts each column’s mean from that column.  
2. **Transpose**: $(\mathbf{A} - \boldsymbol{\mu})^\top$ switches rows and columns.  
3. **Multiply**: The result is a $d \times d$ matrix (for $d$ variables) containing all pairwise covariances, including each variable’s variance on the diagonal.

### **Walking Through a Concrete Example**

Suppose you have $n=8$ observations of $(x,y)$:

| x  | y  |
|----|----|
| 10 | 5  |
| 12 | 3  |
|  6 | 9  |
|  6 | 4  |
|  5 | 11 |
| 14 | 2  |
|  8 | 1  |
|  3 | 13 |

1. **Find Means**:  
  - $\mu_x = 8$  
  - $\mu_y = 6$

2. **Center the Data** $(\mathbf{A} - \boldsymbol{\mu})$:

   $x - \mu_x$ | $y - \mu_y$
   |------------|-------------|
   | 2          | -1          |
   | 4          | -3          |
   | -2         | 3           |
   | -2         | -2          |
   | -3         | 5           |
   | 6          | -4          |
   | 0          | -5          |
   | -5         | 7           |

3. **Transpose & Multiply**:  

$$
\mathbf{C} = \frac{1}{8-1}
\begin{pmatrix}
2 & 4 & -2 & -2 & -3 & 6 & 0 & -5 \\
-1 & -3 & 3 & -2 & 5 & -4 & -5 & 7
\end{pmatrix}
\begin{pmatrix}
2 & -1 \\
4 & -3 \\
-2 & 3 \\
-2 & -2 \\
-3 & 5 \\
6 & -4 \\
0 & -5 \\
-5 & 7
\end{pmatrix}
$$

4. **Resulting 2×2 Matrix** might look like:  

$$
\mathbf{C} = \begin{pmatrix}
14 & -11.86 \\
-11.86 & 19.71
\end{pmatrix}
$$

   - Diagonal entries $\approx 14$ and $\approx 19.71$ are **variances** of $x$ and $y$.  
   - Off-diagonal $-11.86$ is their **covariance** (negative slope in the scatter plot).

This matches our intuition: $x$ and $y$ both have moderate variance, and they trend *downwards* together (negative covariance).

### **Extending to More Than Two Variables**

With $d$ variables (e.g., $x, y, z$), the covariance matrix is $d \times d$. Each diagonal entry is a variance - $\text{Var}(x)$, $\text{Var}(y)$, $\text{Var}(z)$, etc. - and each off-diagonal is a covariance - $\text{Cov}(x,y)$, $\text{Cov}(y,z)$, etc. - The same matrix multiplication approach works:  

$$
\mathbf{C} = \frac{1}{n - 1}(\mathbf{A} - \boldsymbol{\mu})^\top (\mathbf{A} - \boldsymbol{\mu})
$$

where $\mathbf{A}$ is now $n \times d$ and $\boldsymbol{\mu}$ is $n \times d$.

### **Why Covariance Matrices Matter for PCA**

**Principal Component Analysis (PCA)** seeks directions in your data that have maximum variance. By looking at the **eigenvalues** and **eigenvectors** of the **covariance matrix**:

- **Eigenvalues** reveal how much variance is captured by each principal component.  
- **Eigenvectors** give the directions (axes) along which the data varies most.

In other words, the covariance matrix is central to PCA’s power: once you have $\mathbf{C}$, you can transform your dataset into a coordinate system that emphasizes the largest spreads (principal components).

### **Key Takeaways**

1. A **covariance matrix** is a square matrix that organizes:
   - Variances on the diagonal,  

   - Covariances off the diagonal.
2. **Matrix Form**:  

$$
\mathbf{C} = \frac{1}{n - 1} (\mathbf{A} - {\mu})^T (\mathbf{A} - {\mu})
$$

3. It captures *all* pairwise linear relationships between your variables.  
4. **PCA** uses the covariance matrix to find the principal axes of variation in high-dimensional datasets.

Imagine it as a big “cheat sheet” for how each feature in your data is spread out and how it correlates (positively or negatively) with every other feature. With this matrix in hand, we’ll unlock the final steps toward performing PCA!

---

## **PCA-Overview: Putting It All Together**

> **How can we systematically find the “best” line (or plane, or more general subspace) onto which to project our data so that we *maximize* the variance and preserve as much information as possible?**

### **The Big Picture**

- **Goal**: Compress your dataset from a higher dimension (many features) down to fewer dimensions—while keeping the “spread” (variance) as large as possible(Think of "shadow" analogy)
- **Solution**: **Principal Component Analysis (PCA)** uses three key ideas:

  1. **Projections** to reduce dimensions.
  2. **Eigenvectors and eigenvalues** to identify “principal axes” (directions of maximum variance).
  3. **Covariance matrix** to capture all variable-to-variable relationships.

### **Recap of Key Concepts**

1. **Covariance Matrix**: 
- A square matrix that organizes all variances and covariances for your features. 
- Always *symmetric* (transpose equals itself).

2. **Eigenvectors**:
- A direction that is scaled (stretched/shrunk) by the matrix, without being rotated or sheared.
- An eigenvector is a vector whose direction remains unchanged (it is only scaled) when a matrix (or linear transformation) is applied to it. 
- Consider a matrix **A**. If **v** is an eigenvector of **A**, then applying **A** to **v** results in:  
     
$$
A\mathbf{v} = \lambda \mathbf{v}
$$

- Here, v represents a direction in space. Under the transformation by A, although the length (or magnitude) of v may change, its direction remains the same—it is merely stretched, shrunk, or possibly flipped. This property means that even though most vectors might change direction when transformed, eigenvectors always lie along the same line through the origin.

3. **Eigenvalues**:
- The scale factor (stretch) associated with that eigenvector.
- The eigenvalue is the scalar factor by which the eigenvector is scaled during the transformation.
- In the equation:  

$$
A\mathbf{v} = \lambda \mathbf{v}
$$

- The scalar $\lambda$ is the eigenvalue associated with the eigenvector **v**. It tells you how much **v** is stretched or shrunk:

- the scalar $\lambda$ is the eigenvalue associated with the eigenvector **v**. It tells you how much **v** is stretched or shrunk:
  - If $\lambda = 2$, the eigenvector is stretched to twice its original length.
  - If $\lambda = 0.5$, the eigenvector is reduced to half its original length.
  - If $\lambda$ is negative, the eigenvector is not only scaled by the absolute value but also flipped to the opposite direction.

> ***Intuitive Meaning of Finding an Eigenvector of a Matrix***  
> When we say we are "finding an eigenvector V of a matrix A," what we are really doing is uncovering a **special direction in space** that remains unrotated under the transformation applied by A. In more intuitive terms:  
> **Invariant Direction**: Most vectors change both their length and direction when you apply the matrix A. However, an eigenvector is unique because its direction remains unchanged—even though its magnitude might be stretched, compressed, or even flipped. Think of it as discovering a line (or axis) in the space that is “immune” to rotational distortions caused by A.  
> **Pure Scaling**: When you apply A to an eigenvector V, the result is simply a scaled version of V. The eigenvalue tells you exactly how much the vector is scaled.
> **Geometric Analogy**: Imagine you have a flexible grid drawn on a rubber sheet. When you apply the transformation A, almost every line in the grid twists and turns. However, there might be one line along which the grid only stretches or shrinks without changing its direction. This line represents the eigenvector of A. Discovering it tells you a lot about the inherent structure of the transformation—identifying a "preferred" direction that A respects.

4. **Projections**:
   - “Dropping” data onto a line (1D) or a plane (2D) by matrix multiplication. 
   - Allows you to go from $d$ features down to fewer new “principal components.”

### **The Process Step-by-Step (2D Example)**

Imagine you have a 2D dataset—points $(x,y)$—and want to project onto *one* dimension:

#### **Center the Data**
1. Compute the mean $(\mu_x, \mu_y)$ of your dataset.
2. Subtract these means from every point so that your data is centered around $(0,0)$.

#### **Compute the Covariance Matrix**
1. Calculate $\text{Var}(x)$, $\text{Var}(y)$, and $\text{Cov}(x,y)$.  
2. Form the $2\times2$ covariance matrix  

$$
\mathbf{C} = \begin{pmatrix}
\text{Var}(x) & \text{Cov}(x,y) \\
\text{Cov}(y,x) & \text{Var}(y)
\end{pmatrix}
$$

#### Find Eigenvalues and Eigenvectors of $\mathbf{C}$
1. **Eigenvectors** indicate the directions (lines) in which the variance is pure scaling, no shearing.  
2. **Eigenvalues** tell you how large that scaling factor is—in the context of PCA, it corresponds to how much *variance* exists along that eigenvector.

> **Key Insight**: The eigenvector with the **largest** eigenvalue indicates the *principal axis* where your data has the most spread.

#### **Project the Data onto the “Best” Eigenvector**
1. Pick the eigenvector with the biggest eigenvalue - call it $\mathbf{v}_1$.  
2. Project each data point onto $\mathbf{v}_1$.  
3. Now each observation is described by just *one* coordinate (its position on $\mathbf{v}_1$). 

#### **(Optional) Discard Smaller Directions**
- If you only want 1D output, ignore the second eigenvector (and smaller eigenvalue).  
- That “discarded” dimension has smaller variance, so you lose the least information.

Result? You’ve **reduced** from 2D to 1D (one column/feature) while keeping maximum variance possible in that new dimension.

### **A Concretely Illustrated 2D Example**

Suppose:

- $\text{Var}(x) = 9, \text{Var}(y) = 3, \text{Cov}(x,y) = 4$  
- Then  

$$
\mathbf{C} = \begin{pmatrix}
9 & 4 \\
4 & 3
\end{pmatrix}
$$

- You find two eigenvalues: $\lambda_1 = 11$ and $\lambda_2 = 1$, with corresponding eigenvectors $\mathbf{v}_1 = (2, 1)$ and $\mathbf{v}_2 = (-1, 2)$.
- **Larger(est) eigenvalue** = 11, so $\mathbf{v}_1=(2,1)$ is the “best” direction for capturing variance.
- Project data onto $\mathbf{v}_1$. Your dataset goes from $(x,y)$ pairs to a single coordinate along $(2,1)$.

Visually, you see points flatten down onto a single line—losing 1 dimension but retaining the “richest” spread of the original data.

### **Extending to Higher Dimensions**

What if you have $d$ features (say $d=9$)?

1. **Center** your data around the mean (now you have $n$ rows, $d$ columns).  
2. **Build** the $d \times d$ covariance matrix.  
3. **Find Eigenvalues/Vectors**: You’ll get $d$ eigenvalues $\lambda_1, \ldots, \lambda_d$ and corresponding eigenvectors $\mathbf{v}_1, \ldots, \mathbf{v}_d$.  
   - Sort them from largest to smallest eigenvalue.  
4. **Select** however many dimensions you want to keep (e.g., keep 2 or 3). 
   - Suppose you keep $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$.  
5. **Project** your data onto those $k$ eigenvectors:
   - This yields a new dataset with $k$ columns (the principal components), each capturing a direction of large variance.
   - You’ve gone from $d$ features down to $k$, hopefully preserving most of the original variance.

### **Why Does This Work?**

- **Maximizing Variance**: Eigenvectors of the covariance matrix directly correspond to directions of the largest “spread” in your data.
- **Uncorrelated Axes**: Eigenvectors for a symmetric matrix (like the covariance matrix) are orthogonal. So each “principal component” is independent from the others.
  - **Property of Symmetric Matrices**: The covariance matrix is a symmetric matrix. One important property of symmetric matrices is that eigenvectors corresponding to distinct eigenvalues are always **orthogonal**. This means that for any two eigenvectors **v₁** and **v₂**, the dot product equals zero.
  - **Orthogonality and Uncorrelation**: When eigenvectors are orthogonal, they are geometrically perpendicular to each other. In the context of PCA, this orthogonality implies that the principal components (which are the eigenvectors) are statistically **uncorrelated**. In simpler terms, a change in one principal component does not affect the others. Thus, each principal component captures a unique aspect of the data's variance without overlapping with the others.
  - **Significance in PCA**: PCA aims to reduce the dimensionality of the dataset by projecting the data onto a new set of axes that capture the maximum variance. Since the eigenvectors of the covariance matrix are orthogonal:
    - Each principal component represents an independent direction in the data.
    - The data's variance along each axis is distinct, meaning that the components do not share redundant information.
- **Dimensionality Reduction**: By discarding directions with smaller eigenvalues, you lose less important “variance” and keep the main structure.

### **Final Thoughts and Significance**

1. **Data Compression**: PCA is ideal when you have many features but suspect some are redundant or correlated.  
2. **Noise Reduction**: Lower-eigenvalue directions often correspond to minor fluctuations or noise.  
3. **Visualization**: Reducing to 2 or 3 principal components can reveal patterns in a scatter plot or 3D plot.  
4. **Building Blocks**: PCA is widely used in machine learning pipelines (feature selection, speedup computations, etc.).

#### **In a Nutshell**
1. *Center* the data.  
2. *Compute* the covariance matrix.  
3. *Calculate* eigenvectors and eigenvalues.  
4. *Choose* the top eigenvectors (largest eigenvalues).  
5. *Project* data onto those vectors → fewer columns, maximum possible variance.

**That’s PCA**: elegantly combining linear algebra concepts—covariance, eigenvalues, eigenvectors—with the geometric notion of projection to simplify and illuminate your data.

### **Key Takeaway**

Principal Component Analysis is a powerful, mathematically grounded method to discover the “directions of largest variance” in your dataset and reduce its dimensionality. By preserving the largest eigenvalues, you retain the richest structure your data has to offer.

---

## **PCA-Why It Works**

> **You’ve seen that if you take the **covariance matrix** of your data, find its **eigenvalues** and **eigenvectors**, and then project your data onto the directions of the largest eigenvalues, you preserve the maximum variance. But *why* do eigenvalues and eigenvectors reveal directions of greatest spread?**

### **The Setting: Covariance as a “Stretch” Transformation**

Imagine a 2D dataset with two features, $x$ and $y$. Once you’ve centered the data (subtracted mean values), you get its **covariance matrix** $\mathbf{C}$. Conceptually, you can interpret $\mathbf{C}$ as a linear transformation that “stretches” the plane:

1. A **vector** $\mathbf{v}$ in the plane is mapped by $\mathbf{C}$ to another vector $\mathbf{C}\mathbf{v}$.  
2. If you imagine taking all possible direction vectors on a circle of radius 1 (unit vectors), applying $\mathbf{C}$ will transform that circle into some **ellipse**. 

**Why a circle of radius 1?**  
- We only care about *directions*, not magnitudes. Any bigger circle would map to a larger but *similar* ellipse, telling you the same “stretch” directions.

### **From a Circle to an Ellipse**

1. **Start with a unit circle**: All points where $\|\mathbf{v}\|=1$.  
2. **Apply $\mathbf{C}$**: Each point is sent to $\mathbf{C}\mathbf{v}$.  

3. **Result**: Those points form an ellipse. Some directions in the circle get lengthened more than others.

In other words, $\mathbf{C}$ does not rotate your vectors in a uniform way; it *stretches* them by different factors, depending on the direction.

### **Major vs. Minor Axes of the Ellipse**

When you look at the resulting ellipse:

- The **longest** axis is where the transformation has the largest “stretch.”  
- The **shortest** axis is where the transformation has the smallest stretch.

#### **Eigenvectors and Eigenvalues**

1. **Eigenvector** = A direction $\mathbf{v}$ that **only gets scaled** by $\mathbf{C}$—no shearing or rotation. In formula terms:  

$$
\mathbf{C} \mathbf{v} = \lambda \mathbf{v}
$$

2. **Eigenvalue** $\lambda$ = The **amount** of that scaling (“stretch”).

For the covariance matrix in 2D:

- You get **two** eigenvectors (often perpendicular for a symmetric matrix) and their corresponding eigenvalues.  
- The vector with the **largest** eigenvalue points along the **major axis** of the ellipse (the direction of maximum stretching).  
- The vector with the **smallest** eigenvalue points along the **minor axis** (the direction of least stretching).

Hence, if the largest eigenvalue is 11, it means any vector along that eigenvector direction is scaled by **11**—the biggest factor you can get. Another direction might only be scaled by 5 or 9, etc.

### **Concrete Example of Stretch Factors**

Let’s see how some sample vectors in the unit circle might transform:

- **Covariance matrix**:  

$$
\mathbf{C} = 
\begin{pmatrix}
9 & 4 \\
4 & 3
\end{pmatrix}
$$

- Suppose it has eigenvectors $\mathbf{v}_1 = (2, 1)$ with eigenvalue $\lambda_1 = 11$, and $\mathbf{v}_2 = (-1, 2)$ with eigenvalue $\lambda_2 = 1$.

#### **Checking Specific Vectors**

1. **Vector** $(0,1)$ – a simple direction.  
   - Transformed to $\mathbf{C} (0,1) = (4,3)$.  

   - Original length: $1$. New length: $\sqrt{4^2 + 3^2} = \sqrt{16 + 9} = 5$.  
   - Stretch factor = 5, which is *less* than 11.

2. **Vector** $(1,0)$ – another simple direction.  
   - Transformed to $\mathbf{C}(1,0) = (9,4)$.  
   - Original length: $1$. New length: $\sqrt{9^2 + 4^2} = \sqrt{81 + 16} = \sqrt{97} \approx 9.85$.  
   - Stretch factor = 9.85, still *less* than 11.

Only the eigenvector $(2,1)$ is scaled exactly by **11**—the maximum possible. **Any** other direction will be somewhere between 1 and 11. That is why $(2,1)$ is the direction of greatest stretch.

### **Tying It to PCA**

1. **Covariance matrix** $\mathbf{C}$ describes how your data is spread in all directions.  
2. **Eigenvectors** of $\mathbf{C}$ are the special directions (principal components) where $\mathbf{C}$ only stretches vectors—no shearing.  
3. **Largest eigenvalue** = direction of greatest stretch, a.k.a. maximum variance.  

#### **Why Project onto the Biggest Eigenvectors?**

- By projecting onto the eigenvector(s) with the highest eigenvalue(s), you focus on directions where data is most spread out.  
- In dimensionality reduction:  

  - If you keep just 1 dimension in 2D, choose the eigenvector with the biggest eigenvalue.  
  - If you have many dimensions (e.g., 9) and want to reduce to 2, keep the two eigenvectors with the largest eigenvalues, because each indicates a principal axis of maximal variance.

### **Key Insights**

1. **Covariance** $\rightarrow$ “Spread.” The matrix $\mathbf{C}$ transforms a unit circle into an ellipse in 2D (or a sphere into an ellipsoid in higher dimensions).  
2. **Eigenvectors** $\rightarrow$ Directions that are purely stretched, not rotated.  
3. **Eigenvalues** $\rightarrow$ Amount of stretch in each eigenvector direction.  
4. **Maximizing Variance** = **Finding** the direction with the largest stretch = **Largest eigenvalue**.

Hence, PCA’s method of choosing the eigenvector(s) with the largest eigenvalue(s) is exactly how we identify where the data “lives” most broadly—giving us the **principal component(s)**.

**In Short**:  
- The covariance matrix describes how points are spread.  
- Its eigenvectors show how it stretches space (the major or minor axes of the “spread” ellipse).  
- The biggest eigenvalue = biggest stretch = the direction of maximum variance.  
- That is precisely why PCA picks that direction to retain the richest structure in a lower-dimensional representation.

---

## **PCA-Mathematical Formulation**

Now that you understand the intuition behind PCA, how do we tie it all together into a concise set of formulas? 

Below is a step-by-step process, showing how you go from a dataset with multiple features to a lower-dimensional representation that preserves as much variance as possible.

### **The Setup**

You have $n$ observations (rows) and $d$ variables (columns). For concreteness, let's say you have $d = 5$ variables (features) $x_1, x_2, x_3, x_4, x_5$. Your dataset:  

$$
\mathbf{X} = \begin{pmatrix}
x_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\
x_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
x_{n1} & x_{n2} & x_{n3} & x_{n4} & x_{n5}
\end{pmatrix}
$$

- Each **row**: One observation.  
- Each **column**: One variable.

**Goal**: Reduce from $d = 5$ features down to, say, $k=2$ principal components.

### **Center the Data**

Compute the **mean** of each column (feature):  

$$
\mu_j = \frac{1}{n}\sum_{i=1}^{n} x_{ij} \quad \text{for } j = 1, 2, \dots, 5
$$

Then subtract these means from each column in $\mathbf{X}$, forming the **centered** matrix $(\mathbf{X} - \boldsymbol{\mu})$:  

$$
(\mathbf{X} - \boldsymbol{\mu}) = \begin{pmatrix}
x_{11} - \mu_1 & x_{12} - \mu_2 & \cdots & x_{15} - \mu_5 \\
x_{21} - \mu_1 & x_{22} - \mu_2 & \cdots & x_{25} - \mu_5 \\
\vdots        & \vdots        & \ddots & \vdots         \\
x_{n1} - \mu_1 & x_{n2} - \mu_2 & \cdots & x_{n5} - \mu_5
\end{pmatrix}
$$

### **Calculate the Covariance Matrix**

Recall the covariance matrix $\mathbf{C}$ is given by:  

$$
\mathbf{C} = \frac{1}{n - 1} (\mathbf{X} - \boldsymbol{\mu})^\mathsf{T} (\mathbf{X} - \boldsymbol{\mu})
$$

- $\mathbf{C}$ is a $5 \times 5$ matrix in this example.  
- The diagonal entries are variances of $x_1, x_2, \dots, x_5$, and the off-diagonals are pairwise covariances.

### **Find Eigenvalues and Eigenvectors**

Solve the **eigenvalue equation** for $\mathbf{C}$:  

$$
\mathbf{C}\,\mathbf{v} = \lambda\,\mathbf{v}
$$

You’ll get **five** eigenvalues $\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5$ (not necessarily distinct), and corresponding eigenvectors $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4, \mathbf{v}_5$.

Sort them so that $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_5$. The largest eigenvalues correspond to the directions (eigenvectors) of greatest variance.

### **Construct the Projection Matrix**

If you want to reduce to $k=2$ dimensions, **keep only** the top two eigenvectors. Denote them by $\mathbf{v}_1$ and $\mathbf{v}_2$ - associated with $\lambda_1$ and $\lambda_2$. It’s common to normalize each eigenvector to unit length, so:  

$$
\mathbf{V} = \begin{pmatrix}
\frac{\mathbf{v}_1}{\|\mathbf{v}_1\|_2} & \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|_2}
\end{pmatrix}
$$

- This $\mathbf{V}$ is a $d \times 2$ matrix (in our example, $5 \times 2$).  
- Each column is one principal component direction.

### **Project the Centered Data**

Multiply the centered data by $\mathbf{V}$:  

$$
\mathbf{X}_\text{PCA} = (\mathbf{X} - {\mu}) \mathbf{V}
$$

- **Dimensions**:  
  - $(\mathbf{X} - \boldsymbol{\mu})$ is $n \times 5$
  - $\mathbf{V}$ is $5 \times 2$
  - so $\mathbf{X}_\text{PCA}$ is $n \times 2$

Hence, **$\mathbf{X}_\text{PCA}$** is your new dataset with just **2 columns**—the first two principal components. You’ve reduced from 5D to 2D in a way that maximally preserves variance.

### **Putting It All Together**

1. **Form** the matrix $\mathbf{X}$ from your data ($n$ rows, $d$ columns).  
2. **Center** it by subtracting each column’s mean.  
3. **Compute** the covariance matrix $\mathbf{C}$.  
4. **Eigen-Decompose** $\mathbf{C}$ to find eigenvalues (sorted) and eigenvectors.  
5. **Select** the top $k$ eigenvectors (largest eigenvalues).  
6. **Project** the centered data onto those $k$ eigenvectors to get $\mathbf{X}_\text{PCA}$.

This procedure is the core of **Principal Component Analysis** for dimensionality reduction. You can choose any number of principal components $k$—from 1 up to $d$—depending on how much of the variance you need to retain.