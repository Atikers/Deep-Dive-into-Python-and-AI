# ***Linear Algebra(6)_Eigenvalues and Eigenvectors***

## **Bases in Linear Algebra**

> **Have you ever wondered how we can pinpoint any location on a map using just a few directions (like north-south and east-west)?**

In linear algebra, that idea is captured by the concept of a *basis*.

### **Intuitive Picture**

Imagine you’re exploring a city. You have two main streets: one runs north-south, and the other east-west. If you can move along these two streets in any amount—forward or backward—you can reach every possible location in that city grid. In this city analogy, those two streets act like *basis vectors*. 

Similarly, in 2D linear algebra, a basis is typically two vectors (arrows) that can “generate” or “reach” every point in the plane through some combination of those vectors. If you can’t get everywhere—say the two vectors overlap or point in the same direction—then you’re stuck moving along one line, which means you don’t have a full basis.

### **Formal Definition**

A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ is called a **basis** for a vector space if:

1. They are **linearly independent**. Informally, none of the vectors can be written as a “stretch” or “shrink” of another.
2. They **span** the entire space. In other words, every point (vector) in the space can be written as a linear combination of the basis vectors:  

$$
\mathbf{p} = a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_k \mathbf{v}_k
$$

- for some scalars $a_1, a_2, \ldots, a_k$ (which can be any real numbers).

In 2D, this boils down to having two vectors that do not lie on the same line. In 3D, you’d need three vectors that don’t lie in the same plane, and so on.

### **Why Does This Matter?**

1. **Coordinate Systems**: A basis is like a skeleton for a space. Once you’ve picked a basis, any vector in that space can be described by how much you move along each basis vector. This is why the traditional $x$ and $y$ axes in a plane are considered a standard basis: any point $(x, y)$ is $x$ steps in the horizontal direction plus $y$ steps in the vertical direction.

2. **Simplicity and Building Blocks**: Think of an artist mixing primary colors—red, blue, and yellow—to get any color on the palette. The primary colors form a type of “basis,” because you can combine them in different proportions to produce a wide range of hues. Likewise, a basis in linear algebra provides the fundamental “directions” you need to build any vector.

3. **Choice of Basis**: You aren’t stuck with just one option. Any two linearly independent vectors in 2D can form a basis. Different choices can sometimes simplify problems. For example, if you choose a basis aligned with certain symmetries, your equations might become easier to handle.

### **Examples**

1. **Standard 2D Basis**: The usual vectors $(1,0)$ and $(0,1)$ in the plane. Clearly, they span the plane: any point $(x,y)$ can be reached by $x(1,0) + y(0,1)$.

2. **Tilted Basis**: Suppose you have vectors $(2,1)$ and $(-1,3)$. These still form a basis as long as they’re not multiples of one another. You can still reach all points in the plane by walking some amount in each direction.

3. **Non-Basis**: Two vectors that line up—like $(1,2)$ and $(2,4)$—are multiples of each other and only cover a single line. You’d never be able to reach any point off that line.

### **Connecting to AI and Beyond**

In more advanced AI topics, especially in deep learning or machine learning, changing from one basis to another can make certain problems simpler to understand or solve. For instance, when dealing with huge datasets, we often want to select a more *convenient* basis that captures the key patterns (like using Principal Component Analysis, where we change the basis to emphasize the directions of greatest variance).

Think of it a bit like surgery in medicine: the surgeon picks the best approach or “angle” to operate so that the procedure becomes simpler and clearer. In the same way, switching to the right basis can reveal structures and patterns in your data that are hidden in the original coordinate system.

### **Key Takeaways**

- A **basis** is a set of independent vectors that span the entire space.  
- Any vector in the space can be written as a combination of those basis vectors.  
- Different bases can be chosen depending on what is most useful for a given problem.  
- In AI, changing bases can help us see data in a new light and simplify computations.

Stay curious, and remember that much like choosing the best vantage point to observe a painting or performance, choosing the right basis in linear algebra can give you a deeper, more intuitive view of your data.

---

## **Span in Linear Algebra**

> **Imagine you have a toolbox with a few types of tools—maybe a hammer, a screwdriver, and a wrench. Do you ever think about all the tasks you can accomplish *just* with those tools?**

In linear algebra, the concept of span captures a similar idea: how many “places” (or solutions) can you reach by combining a given set of vectors?

### **Motivation & Intuition**

In everyday life, we combine movements or actions to reach goals. For instance:
- **Navigation**: Going north-south or east-west to reach any location on a city grid.
- **Color Mixing**: Using primary colors (red, blue, yellow) in different proportions to create a full palette.
- **Forces in Physics**: Combining forces (vectors) in different directions to find the net force on an object.

All these scenarios hint at how “few directions” can generate a wide variety of outcomes. In linear algebra, the **span** of a set of vectors is precisely the collection of all points (or vectors) you can produce by linearly combining those directions.

### **Formal Definition of Span**

Given vectors  

$$
\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k
$$ 

in a vector space (like a 2D or 3D space), the **span** of these vectors is the set of all possible linear combinations:  

$$
\{a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_k \mathbf{v}_k \mid a_1, a_2, \ldots, a_k \in \mathbb{R} \}
$$

- **Linear Combination**: Taking each vector and scaling it by some real number $a_i$, then adding the results together.  
- **All Real Numbers**: Each $a_i$ can be zero, positive, or negative, allowing you to “walk” in each vector’s direction (forwards, backwards, or stand still in that direction).

#### **Example in 2D**

- Suppose $\mathbf{v}_1 = (1,0)$ and $\mathbf{v}_2 = (0,1)$.  
- Their span is *every* point $(x,y)$ in the plane, because any $(x,y)$ can be written as  

$$
x(1,0) + y(0,1) = (x,y)
$$

- This is the standard set of axes you see on a typical Cartesian plane.

#### **Example in 3D**

- Consider $\mathbf{v}_1 = (1,0,0), \mathbf{v}_2 = (0,1,0)$, and $\mathbf{v}_3 = (0,0,1)$.  
- Their span is the entire 3D space—every point $(x,y,z)$—because you can create any 3D vector by combining these three “direction” vectors appropriately.

### **Spanning Lines, Planes, and Spaces**

To grasp span more concretely, let’s see how the number of vectors relates to the shape they span:

1. **One Vector**: Spans a line (unless the vector is the zero vector, which spans only a single point).  
2. **Two Independent Vectors**: Spans a plane (in 2D or as a plane “slice” in 3D).  
3. **Three Independent Vectors**: In 3D, three vectors that don’t all lie in the same plane can span the whole space.

#### **When Vectors Fail to Span**

If multiple vectors are “all in the same line,” you can’t cover an entire plane. Similarly, if three vectors in 3D all lie in the same plane, you can’t reach points outside that plane. The span would be just that plane, not the whole 3D space.

### **Minimal Spanning Set and the Basis Concept**

A **spanning set** means you can generate the entire space (or subspace), but sometimes you might have redundant vectors. A **basis** is a *minimal* spanning set—no extra directions that can already be formed by the others.

- **Minimal** means that if you remove *any* vector from a basis, you lose the ability to span the entire space.  

#### **Example of Redundancy**

- In 2D, having three vectors that all lie in the same plane still spans the plane, but one vector is redundant because two independent vectors were enough. Hence, that three-vector set isn’t a basis (it’s not minimal).

### **The Link Between Span and Linear Independence**

**Linear Independence** is the condition that no vector in your set can be formed by combining the others. If even one vector can be generated by the rest, the set is **linearly dependent**.

- A set of vectors forms a **basis** if and only if it:
  1. **Spans** the space.
  2. Is **linearly independent**.

Thus, checking **span** ensures you cover the whole space, and checking **independence** ensures you aren’t using extra “duplicate” directions.

### **Dimensional Insights**

A space’s **dimension** is the number of vectors in *any* of its bases. Here are some key facts:

- A **line** (1D) has dimension 1. Any single nonzero vector can form a basis for that line.  
- A **plane** (2D) has dimension 2. Any two linearly independent vectors can form a basis.  
- **3D space** has dimension 3. You need three linearly independent vectors to form a basis, and so on.

In more advanced settings (like AI and data science), you might work in *very* high-dimensional spaces (e.g., a 100-dimensional feature space).

### **Practical Analogies from Other Fields**

1. **Physics (Force Vectors)**: In classical mechanics, the net force on an object is the vector sum of forces applied to it. If you have two distinct directions of force in a 2D plane, you can produce any resultant force in that plane by choosing appropriate magnitudes. The set of force directions might span the plane of possible net forces.

2. **Art (Color Mixing)**: If you consider red, blue, and yellow as your “vectors,” mixing them in various proportions spans a wide range of colors on your canvas. However, if one color can already be made by mixing the other two, it’s redundant.

3. **Medicine (Treatment Plans)**: Imagine each “direction” as a particular treatment approach (e.g., physical therapy, medication, diet). If one of those treatments is essentially a combination of the others, it doesn’t add a fundamentally new approach. The total “span” of treatments might be all possible ways to combine them for a given condition.

4. **Engineering (Vibration Modes)**: In structural engineering, different “modes” of vibration can combine to produce the total motion of a system. If certain modes are not truly distinct, they’re dependent on others and don’t increase the total possible motion patterns.

### **Step-by-Step Example: Checking Span in 2D**

Let’s say we have two vectors in 2D:  

$$
\mathbf{v}_1 = (2,1), \quad \mathbf{v}_2 = (-1,3)
$$

1. **Can they span the plane?**  
  - We ask: can any point $(x,y)$ be written as  
   
$$
\alpha (2,1) + \beta (-1,3) = (x,y)\;?
$$  

$$
2\alpha - \beta = x
$$  

$$
\alpha + 3\beta = y
$$
   
  - If for every $(x,y)$ you can solve for $\alpha$ and $\beta$, then $\mathbf{v}_1$ and $\mathbf{v}_2$ span the plane. Typically, this works if the determinant of the coefficient matrix is nonzero (which means $\mathbf{v}_1$ and $\mathbf{v}_2$ are not multiples of each other).

2. **Check for linear dependence**  
  - If $\mathbf{v}_2$ were just a scalar multiple of $\mathbf{v}_1$, you’d see the system only has solutions for certain $(x,y)$ that lie on one line. That would mean no full coverage of the plane—hence, no spanning of 2D.

### **Importance in AI and Data Science**

1. **Dimensionality Reduction**: Techniques like **Principal Component Analysis (PCA)** find a smaller set of “directions” (principal components) that still *span* most of the meaningful variance in your data. This helps when you have lots of features (dimensions) but want to compress them without losing essential information.

2. **Feature Selection**: If some features in a dataset are linear combinations of others, they might be redundant. By identifying a minimal spanning set of features (a basis for your data space), you reduce complexity.

3. **Neural Networks**: Even in deep learning, you’ll often deal with transformations (like matrix multiplications). Understanding how spans and bases work helps you see how data passes from one layer to another (through transformations that can change how you view the space).

### **Summary & Key Points**

- **Span** is the set of all linear combinations of given vectors.  
- If you can reach the entire space, the vectors *span* that space.  
- **Basis** is a minimal set that spans the space (no vector can be omitted).  
- **Dimension** is the number of vectors in *any* basis of that space.  
- **Linear independence** prevents redundancy—no vector is a linear combination of the others.

**Big Picture**: Think of a span as your “toolbox.” You want just enough tools to handle every possible “building/repair project” (i.e., every point in your space), but not so many that you’re carrying around unnecessary duplicates. That’s essentially the difference between any spanning set and a basis.

---

## **Eigenbases**

> **Have you ever wondered if there is a way to see a linear transformation as just “stretching” along certain special directions, rather than tilting or shearing in complicated ways?**

When we talk about *bases* in linear algebra, we usually start with the “standard” or “fundamental” basis vectors. For the 2D plane, these are $(1,0)$ and $(0,1)$. Any vector in 2D can be expressed as a combination of these two basic directions. 

However, there’s a particular choice of basis that can make certain transformations look *astonishingly simple*. We call it an **eigenbasis**. In this basis, a linear transformation acts like pure stretching along each of the basis directions—no rotation, no extra skew. These special directions are called **eigenvectors**, and the amounts by which the vectors get stretched are called **eigenvalues**.

### **A Quick Example: Stretching a Parallelogram**

Imagine you have a matrix  

$$
A = \begin{pmatrix}
2 & 1 \\
0 & 3
\end{pmatrix}
$$ 

that defines a linear transformation in 2D. If you apply $A$ to the unit square defined by $(1,0)$ and $(0,1)$, you will end up with a parallelogram. In more familiar coordinates:

- The vector $(1,0)$ is mapped to $(2,0)$.  
- The vector $(0,1)$ is mapped to $(1,3)$.  

Visually, our neat little square is stretched and slanted, so the resulting shape is a parallelogram.

But what if you choose a different set of two vectors as your basis? Suppose you pick $(1,0)$ and $(1,1)$. Under $A$, these become:

- $(1,0)$ goes to $(2,0)$.  
- $(1,1)$ goes to $(3,3)$.  

So, you end up with another parallelogram—yet something special happens: each side of that parallelogram remains parallel to its “original” side. In other words, you’ve found directions that are being purely _stretched_ by the matrix. One direction is stretched by a factor of $2$, while the other is stretched by a factor of $3$. When both vectors are simply stretched (without extra skew between them), those vectors are eigenvectors, and $2$ and $3$ are the corresponding eigenvalues.

### **Why Is This So Useful?**

1. **Simplicity of Computation**  
   In the eigenbasis, applying the transformation $A$ boils down to just multiplying each coordinate by its respective eigenvalue. For example, if a point $(x,y)$ in your new basis has coordinates $(a,b)$ with respect to the eigenvectors, then after applying $A$, it becomes $(\lambda_1 \cdot a, \lambda_2 \cdot b )$, where $\lambda_1$ and $\lambda_2$ are the eigenvalues. No mixing, no extra addition.

2. **Connections to Other Fields**  
   - **Physics**: In vibration analysis (think of a guitar string or a drumhead), certain “modes” (eigenvectors) can vibrate independently with different frequencies (eigenvalues).  
   - **Engineering**: Stress and strain in materials can be described by principal directions—again, an eigenbasis concept.  
   - **Medicine**: In imaging (like MRI or CT scans), data compression techniques can rely on “principal directions” to reduce dimensionality.  
   - **Art**: When scaling or distorting images, finding principal axes that stay pure in direction can help with uniform transformations or stylized effects.

3. **Principal Component Analysis (PCA)**  
   One of the biggest uses of eigenvectors and eigenvalues in machine learning is **PCA (Principal Component Analysis)**. PCA finds directions (principal components) along which data has the greatest variance, and these directions are essentially eigenvectors of the data’s covariance matrix. By re-expressing data in an eigenbasis, you can see which directions are most “significant” and potentially reduce the dimensionality of your dataset without losing the key patterns.

### How It Ties Together

- An **eigenbasis** is simply a basis made up of eigenvectors of a matrix $A$.  
- Each vector in this basis is stretched by its corresponding eigenvalue under the transformation.  
- By diagonalizing (or simplifying) the matrix in the eigenbasis, we make our computations more straightforward.  

Think of it like looking at a transformation through “special glasses” that let you see each direction as a perfect stretch—no complicated distortion. It’s often a powerful tool in many branches of science, engineering, and data analysis.

Next time you see a matrix, ask yourself: does it have a set of directions it simply stretches? If so, finding that eigenbasis can turn a seemingly complex problem into something much simpler. In upcoming sections, we’ll dig deeper into how to compute these eigenvectors and eigenvalues, and how they lead to major applications like dimension reduction and PCA.

### **Key Takeaway**

An eigenbasis reshapes the way you see transformations. Once you find it, linear algebra problems can become much simpler—like turning a bumpy path into a smooth highway for both theory and computation.

---

## **Eigenvalues and Eigenvectors**

> **Have you ever noticed that when you apply a certain linear transformation (like a matrix in 2D), some directions seem to “line up” perfectly—almost as if they just get stretched while keeping their original orientation, whereas other directions get slanted or rotated?**

In linear algebra, those “special” directions are connected to the idea of **eigenvectors** and **eigenvalues**. Learning these concepts is a milestone for anyone delving into AI or data science, particularly because they are at the heart of **Principal Component Analysis (PCA)** and other advanced techniques. Let’s explore what they are and why they are so powerful.

### **A 2D Example**

Let’s consider the 2×2 matrix(again):  

$$
A = \begin{pmatrix}
2 & 1 \\
0 & 3
\end{pmatrix}
$$

We’ll look at how $A$ acts on different 2D vectors. Some vectors will be stretched in a neat, direct way; others will be skewed or rotated. 

1. **Vector $(1,0)$**  
- Applying $A$:  
   
$$
A \begin{pmatrix} 1 \\
0 \end{pmatrix} 
= \begin{pmatrix} 2 \\
0 \end{pmatrix}
$$  
   - We see that it **does not** change direction (it’s still pointing purely along the horizontal axis).  
   - We can rewrite  
   
$$
\begin{pmatrix} 2 \\
0 \end{pmatrix}
$$
   
$$
2 \times \begin{pmatrix} 1 \\
0 \end{pmatrix}
$$  
   - So $(1,0)$ is an **eigenvector** with **eigenvalue** $2$

2. **Vector $(1,1)$**  
- Applying $A$:  
   
$$
A \begin{pmatrix} 1 \\
1 \end{pmatrix}
= \begin{pmatrix} 3 \\
3 \end{pmatrix}
$$
   - Again, the result points along the same diagonal direction as $(1,1)$ itself.  
   - It is effectively  
   
$$
3 \times \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$
   - So $(1,1)$ is another **eigenvector** with **eigenvalue** $3$.

3. **Vector $(-1,2)$**  
- Applying $A$:  
   
$$
A \begin{pmatrix} -1 \\
2 \end{pmatrix}
= \begin{pmatrix} 0 \\
6 \end{pmatrix}
$$
   - Notice that $(0,6)$ does **not** point in the same direction as $(-1,2)$.  
   - There is no single scalar $\alpha$ such that  
   
$$
\alpha \times \begin{pmatrix} -1 \\
2 \end{pmatrix}
= \begin{pmatrix} 0 \\
6 \end{pmatrix}
$$
   - Therefore, $(-1,2)$ is **not** an eigenvector.

### **Formal Definition**

The above observations hint at the general definition. A vector $\mathbf{v}$ is called an **eigenvector** of a matrix $A$ if:  

$$
A \mathbf{v} = \lambda \mathbf{v}
$$

where $\lambda$ is a scalar value called the **eigenvalue** associated with $\mathbf{v}$.  

- In our example, $(1,0)$ and $(1,1)$ satisfy this condition:  

$$
A \begin{pmatrix} 1 \\
0 \end{pmatrix}
= 2 \begin{pmatrix} 1 \\
0 \end{pmatrix} \quad A \begin{pmatrix} 1 \\
1 \end{pmatrix}
= 3 \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$

- Thus, **$(1,0)$ with eigenvalue $2$** and **$(1,1)$ with eigenvalue $3$** are two eigenvalue-eigenvector pairs.

### **Why Are Eigenvectors So Special?**

1. **Matrix Multiplication vs. Scalar Multiplication**  
   - **Matrix multiplication** in 2D typically involves several operations (in this small case, $2\times 2$ yields 8 multiplications, 4 additions).  
   - **Scalar multiplication** is far simpler—just multiply each component by a single number (2 operations in 2D).  
   - When you’re working **along an eigenvector**, the transformation $A$ acts like a single scalar $\lambda$. This makes your life **much easier** in higher dimensions.

2. **Eigenbasis Simplifies Computations**  
   - Suppose your matrix $A$ has two linearly independent eigenvectors in 2D (as in our example). These eigenvectors can form a new basis—**eigenbasis**.  
   - **Any vector** in the plane can be expressed as a linear combination of these two eigenvectors.  
   - When you apply $A$ to a vector written in the eigenbasis, each component just gets multiplied by its corresponding eigenvalue—**no big matrix multiplications required**.

### **A Step-by-Step Computation Shortcut**

Let’s see how this helps even with a non-eigenvector like $(-1,2)$:

1. **Express $(-1,2)$ in the Eigenbasis**  
- Our eigenbasis might be $\{(1,0), (1,1)\}$.  
- If you can solve for scalars $a$ and $b$ such that  
   
$$
\begin{pmatrix} -1 \\
2 \end{pmatrix} = a \begin{pmatrix} 1 \\
0 \end{pmatrix} + b \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$

- then you’ve found the coordinates of $(-1,2)$ in the eigenbasis.  
- In our example, it turns out $a=-3$ and $b=2$ do the job, so  

$$
\begin{pmatrix} -1 \\
2 \end{pmatrix} = -3 \begin{pmatrix} 1 \\
0 \end{pmatrix} + 2 \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$

2. **Apply $A$ to Each Eigenvector**  
- We already know $A(1,0) = 2(1,0)$ and $A(1,1) = 3(1,1)$.  

3. **Combine Results**  
- We have  

$$
A \begin{pmatrix} -1 \\
2 \end{pmatrix} = A \Bigl( -3 \begin{pmatrix} 1 \\
0 \end{pmatrix} + 2 \begin{pmatrix} 1 \\
1 \end{pmatrix} \Bigr) = -3 A \begin{pmatrix} 1 \\
0 \end{pmatrix} + 2 A \begin{pmatrix} 1 \\
1 \end{pmatrix}
$$  

- By plugging in the eigenvector relationships, you get:  

$$
= -3 \bigl(2 \begin{pmatrix} 1 \\
0 \end{pmatrix}\bigr) + 2 \bigl(3 \begin{pmatrix} 1 \\
1 \end{pmatrix}\bigr) = \begin{pmatrix} 0 \\
6 \end{pmatrix}
$$  

Hence, instead of redoing the entire matrix multiplication, you took advantage of the shortcut that **on eigenvectors**, $A$ is just a scalar.

**Caution**: You still need to initially find how $(-1,2)$ is represented in the eigenbasis. For large problems, that can itself be a significant computation (e.g., you might need the inverse of the eigenbasis matrix). So eigenvectors don’t magically remove **all** the work, but they can shift it to a more convenient step.

### **Key Takeaways**

1. **Definition**: Eigenvalues $\lambda$ and eigenvectors $\mathbf{v}$ satisfy  

$$
A \mathbf{v} = \lambda \mathbf{v}
$$

2. **Interpretation**: Eigenvectors are directions in which $A$ acts as a **pure stretch**. The factor of stretching is the eigenvalue.  
3. **Eigenbasis**: If enough eigenvectors exist to form a basis, you can rewrite any vector in terms of these eigenvectors, turning a big matrix multiplication problem into simpler scalar multiplications.  

**Question for Reflection**: Next time you see a transformation—be it a distortion of an image or a data transformation in machine learning—ask yourself: are there “natural directions” where this transformation is just a scaling? If so, you’ve just discovered its eigenvectors and eigenvalues. Embracing those can make your work far more efficient, especially for high-dimensional tasks.

> **Remember**: Learning eigenvalues and eigenvectors opens the door to efficient algorithms, clearer geometric intuition, and a powerful way to rewrite and simplify linear transformations.

---

## **Calculating Eigenvalues and Eigenvectors**

> **If you’ve ever stretched or squeezed a shape, have you noticed certain directions might remain “pure” stretches without shearing or rotating? How can we formally identify those directions and how much they stretch?** 

That’s where *eigenvalues* and *eigenvectors* come in.

### **Why Calculate Them?**

- **Simplicity**: Finding eigenvalues and eigenvectors can reveal the essential “stretch” factors of a matrix, making many problems easier.  
- **Applications in AI**: Principal Component Analysis, spectral clustering, and various machine learning methods all rely on computing eigenvalues and eigenvectors.  
- **Insights**: They help us see if a system (like a dynamical system) is stable, if it amplifies signals, or if it compresses them.

### **The Key Equations**

1. **Eigenvalue Equation**  
- A vector $\mathbf{v}$ is an eigenvector of a matrix $\mathbf{A}$ if  
   
$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}
$$

- where $\lambda$ is the eigenvalue. In words, $\mathbf{v}$ keeps its **direction** under $\mathbf{A}$—it’s simply scaled by $\lambda$.

2. **Characteristic Polynomial**  
- To find $\lambda$, rewrite the eigenvalue equation as  
   
$$
(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}
$$

- For a *nonzero* eigenvector $\mathbf{v}$ to exist, the matrix $(\mathbf{A} - \lambda \mathbf{I})$ must be **singular**. Equivalently,  
   
$$
\det(\mathbf{A} - \lambda \mathbf{I}) = 0
$$

- The resulting polynomial in $\lambda$ is the **characteristic polynomial**. Its roots are the eigenvalues.

### **Step-by-Step: A 2×2 Example**

Let’s take a simple 2×2 matrix:  

$$
\mathbf{A} = \begin{pmatrix}
2 & 1 \\
0 & 3
\end{pmatrix}
$$

1. **Write $(\mathbf{A} - \lambda \mathbf{I})$**  

$$
\mathbf{A} - \lambda \mathbf{I} = \begin{pmatrix}
2 - \lambda & 1 \\
0 & 3 - \lambda
\end{pmatrix}
$$

2. **Take the Determinant**  

$$
\det(\mathbf{A} - \lambda \mathbf{I}) = (2 - \lambda)(3 - \lambda) - (1 \cdot 0) = (2 - \lambda)(3 - \lambda)
$$

3. **Set It Equal to Zero**  

$$
(2 - \lambda)(3 - \lambda) = 0 \quad \Rightarrow \quad \lambda = 2 \quad \text{or} \quad \lambda = 3
$$

These are the **eigenvalues**. Next, we find the corresponding **eigenvectors**:

1. **For $\lambda = 2$**: Solve $(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \mathbf{0}$.  

$$
\begin{pmatrix}
2 - 2 & 1 \\
0 & 3 - 2
\end{pmatrix} = \begin{pmatrix}
0 & 1 \\
0 & 1
\end{pmatrix}
$$
- This matrix sends $(x,y)$ to $(y, y)$. For it to be $(0,0)$, we need $y = 0$. So any vector of the form $(x, 0)$ is a solution. A convenient choice is $(1,0)$.

2. **For $\lambda = 3$**: Solve $(\mathbf{A} - 3\mathbf{I})\mathbf{v} = \mathbf{0}$.  

$$
\begin{pmatrix}
2 - 3 & 1 \\
0 & 3 - 3
\end{pmatrix} = \begin{pmatrix}
-1 & 1 \\
0 & 0
\end{pmatrix}
$$

- This requires $-x + y = 0 \Rightarrow y = x$. So one valid vector is $(1,1)$.

So the eigenvalues are $2$ and $3$, with corresponding eigenvectors $(1,0)$ and $(1,1)$ (or any nonzero scalar multiples).

### **A 3×3 Example in Brief**

For a 3×3 matrix, say  

$$
\mathbf{B} = \begin{pmatrix}
2 & 1 & -1 \\
1 & 0 & -3 \\
-1 & -3 & 0
\end{pmatrix}
$$

we form  

$$
\mathbf{B} - \lambda \mathbf{I} = \begin{pmatrix}
2-\lambda & 1 & -1 \\
1 & -\lambda & -3 \\
-1 & -3 & -\lambda
\end{pmatrix}
$$

Then,  

$$
\det(\mathbf{B} - \lambda \mathbf{I}) = 0
$$

gives a *cubic* characteristic polynomial. Solve for $\lambda$ (there might be up to 3 real or complex roots), then find the eigenvectors by substituting each root back into $(\mathbf{B} - \lambda \mathbf{I})\mathbf{v} = \mathbf{0}$.

### **Infinite Solutions and Eigenvectors**

Why do we get infinitely many solutions for each eigenvalue? Because eigenvectors are defined **up to a scalar multiple**—if $(1,0)$ is an eigenvector, so is $(5,0)$ or $(-3,0)$. They all lie on the same line, which is the direction that remains unchanged (up to scaling).

### **What If the Matrix Isn’t Square?**

- **Determinants** are only defined for square matrices.  
- Therefore, **eigenvalues** and **eigenvectors** only make sense for square matrices.  
- For rectangular matrices, we have related concepts (like **singular values** and **singular vectors** in the singular value decomposition), but they’re not called eigenvalues/eigenvectors.

### **Avoiding Common Pitfalls**

1. **Multiplicity**: An eigenvalue can appear more than once (repeated root). Sometimes, one eigenvalue has more than one independent eigenvector (or *less* than expected, which complicates diagonalization).  

2. **Complex Eigenvalues**: For real-valued matrices, you might still get complex solutions for $\lambda$. In that case, your eigenvectors live in a complex vector space.  
3. **Zero Eigenvalue**: This indicates $\mathbf{A}$ is singular. The presence of $\lambda = 0$ implies a non-invertible matrix.

### **Why This Matters (Again)**

- **Diagonalization**: If a matrix has a full set of $n$ independent eigenvectors (for an $n \times n$ matrix), it can be turned into a simpler diagonal form. This makes many operations (like exponentiating the matrix) much easier.  
- **PCA & Machine Learning**: In analyzing data, finding eigenvalues of the *covariance matrix* pinpoints directions of greatest variance.  
- **Physical Interpretations**: In vibrations or quantum mechanics, eigenvalues can represent fundamental frequencies or energy levels.

### **Final Roadmap for Calculation**

To recap the general process:

1. **Write** $(\mathbf{A} - \lambda \mathbf{I})$.  
2. **Compute** the determinant to get the characteristic polynomial.  
3. **Factor/Solve** for eigenvalues $\lambda_i$.  
4. **For each $\lambda_i$**, solve $(\mathbf{A} - \lambda_i \mathbf{I})\mathbf{v} = \mathbf{0}$ to get the eigenvector(s).  
5. **Normalize (Optional)**: Often we pick a convenient scalar multiple (like making the vector have length 1 or some integer ratio).

### **Key Takeaways**

- **Eigenvalues**: The “stretch factors” by which each eigenvector is scaled.  
- **Eigenvectors**: Special **directions** that remain aligned with themselves under the matrix.  
- **Characteristic Polynomial**: The bridge to finding these special values.  
- **Infinite Solutions**: Each eigenvalue yields a line (or plane, etc.) of possible eigenvectors.  

- **Only Square Matrices**: You need a determinant to define eigenvalues, so the matrix must be square.

By following these steps, you can quickly break down seemingly complicated linear transformations and uncover the elegantly simple stretching directions hidden within—exactly what’s used in advanced AI methods like principal component analysis.

---

## **A Deeper Dive into Eigenvalues and Eigenvectors**

Many people find eigenvalues and eigenvectors confusing and have trouble understanding them intuitively. To help clarify these concepts, I’ve created an additional section.

### **Setting the Stage: Matrices as Transformations**

When you see a matrix in a typical math or AI course, it’s easy to get stuck in an “array of numbers” mindset. But the key to truly understanding eigenvalues and eigenvectors is to **interpret** a matrix as a **linear transformation**:

- In **2D**, each 2×2 matrix describes how you move or distort points on a plane.  
- In **3D**, each 3×3 matrix describes how you transform all points in 3D space.

Viewing matrices this way transforms abstract symbols into **actions** on space—stretching, compressing, reflecting, rotating, and more.

### **Where Eigenvectors Come In**

Most vectors get “moved” or “rotated” off of their original lines when you apply a matrix. However, certain **special vectors** stay on their own span (i.e., the line from the origin through that vector) even after the transformation. These are **eigenvectors**.

#### **Staying on the Same Line**

- If you apply the matrix $A$ to an eigenvector $\mathbf{v}$, the result $A\mathbf{v}$ will lie **somewhere** along the same line as $\mathbf{v}$.  
- Mathematically, this “stay-on-the-line” condition is expressed by  

$$
A\mathbf{v} = \lambda \mathbf{v}
$$

where $\lambda$ is a scalar. We call $\lambda$ the **eigenvalue** associated with the eigenvector $\mathbf{v}$.

#### **Interpreting the Eigenvalue**

- If $\lambda = 3$, $\mathbf{v}$ is **stretched** to triple its length.  
- If $\lambda = 0.5$, $\mathbf{v}$ is **squished** to half its length.  
- If $\lambda$ is **negative**, that indicates a **flip** (the direction is reversed) **and** a scale by the absolute value of $\lambda$.  

In 2D examples, you might find that one line is stretched by $\lambda_1$ and another is stretched by $\lambda_2$. In 3D, you could have multiple eigenvalues, each associated with a different direction in space.

### **Concrete Examples**

#### **A Simple 2D Matrix**
Let’s say $A$ sends:
- the vector $(1,0)$(i-hat) to $(3,0)$.  
- the vector $(0,1)$(j-hat) to $(1,2)$.

That means  

$$
A = \begin{pmatrix}
3 & 1 \\
0 & 2
\end{pmatrix}
$$

where the first column is the image of $\mathbf{i}$-hat and the second column is the image of $\mathbf{j}$-hat.

1. **Observing a potential eigenvector:**  
- $\mathbf{i}$-hat $(1,0)$ is actually mapped to $(3,0)$, which stays on the $x$-axis. So $\mathbf{i}$-hat is an eigenvector with eigenvalue 3.  
2. **Finding another eigenvector:**  
- By solving $\det(A - \lambda I) = 0$, you might find $\lambda = 2$ is another eigenvalue. A corresponding eigenvector could be  
   
$$\begin{pmatrix} -1 \\
1 \end{pmatrix}
$$

- which remains on its own diagonal line, scaled by 2.

#### **Rotation in 3D**
- A rotation matrix around some axis doesn’t change any point **exactly** on that axis (the axis is “fixed”), meaning vectors along that axis are eigenvectors with $\lambda = 1$.  
- Every other vector is rotated off of its line, so no other real eigenvectors exist. This underscores the idea that finding **one** axis of rotation is simpler than juggling a full 3×3 matrix.

#### **Rotation in 2D (90°)**
- A pure 90° rotation in the plane has **no real** eigenvectors. Every vector is turned off its original line, meaning no direction remains purely “scaled.”  
- Algebraically, you see this when the polynomial $\det(A - \lambda I)$ yields **complex roots** ($\pm i$), signaling no real eigenvalues.

#### **Shear**
- In a shear, say  

$$
\begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}
$$

- every vector except those on the $x$-axis gets “pushed” diagonally.  
- The $x$-axis vectors remain where they are (so they’re eigenvectors with $\lambda=1$). However, there may **not** be a second distinct eigenvector to span the plane.  

#### **Uniform Scaling**

- If you have a matrix like  

$$
\begin{pmatrix}
2 & 0 \\
0 & 2
\end{pmatrix}
$$

- it **scales everything** by 2, so literally **any** vector is an eigenvector with eigenvalue 2.  

### **The Algebra Behind Finding Them**

1. **Rewrite the condition $A\mathbf{v} = \lambda \mathbf{v}$ as**  

$$
(A - \lambda I)\mathbf{v} = \mathbf{0}
$$

2. **Non-zero $\mathbf{v}$ requires $\det(A - \lambda I) = 0$.**  
   This means the transformation $A - \lambda I$ is **singular**—it “collapses” the space to a lower dimension, leaving at least one non-zero vector in its null space.  
3. **Solve for $\lambda$** by making $\det(A - \lambda I) = 0$. These are your **eigenvalues**.  
4. **For each eigenvalue $\lambda$,** solve $(A - \lambda I)\mathbf{v} = \mathbf{0}$ to find the corresponding **eigenvector(s)**.

### **Eigenbasis and Diagonalization**

If a 2D or 3D matrix has enough eigenvectors to form a **basis** (i.e., a full set of independent eigenvectors spanning the entire space), then in that new coordinate system, the matrix becomes **diagonal**:  

$$
\begin{pmatrix}
\lambda_1 & 0 & 0 \\
0 & \lambda_2 & 0 \\
0 & 0 & \lambda_3
\end{pmatrix}
\quad\text{(or just 2×2 for 2D)}
$$

**Why is that awesome?**
- **Matrix powers**: Computing $A^{100}$ is a breeze when $A$ is diagonal; you just raise each diagonal entry to the 100th power.  
- **Interpretation**: A diagonal matrix is simply a transformation that independently scales each basis vector.  
- **Simplicity**: Many tasks in machine learning (like PCA) or other numeric algorithms rely on diagonalization or related techniques (like the Singular Value Decomposition) to simplify computations.

**Note**: Not every matrix can be fully diagonalized (e.g., a shear matrix may have only one eigenvector). But when you **can** diagonalize a matrix, it’s a powerful tool.

### **Why Does This Matter?**

1. **3D Rotations**: As mentioned, eigenvectors can reveal an **axis of rotation**, which is invaluable for geometric understanding or 3D computations.  
2. **Principal Component Analysis (PCA)**: In machine learning, finding principal axes (eigenvectors of the covariance matrix) helps reduce dimensionality and identify the main “directions” of variation in data.  
3. **Repeated Transformations**: Whether you’re doing iterative methods in computational math or analyzing repeated transformations in a simulation, eigen-decomposition can drastically simplify what might otherwise be an **intractable** process.

### **Putting It All Together**

- **Eigenvectors** are directions in which a transformation acts by simple scaling (including potential flips), rather than any rotation.
- **Eigenvalues** are the factors of that scaling.
- **Diagonalization** (or more advanced forms like Jordan normal form) leverages eigenvectors for deep simplifications of matrix operations.

In essence, **eigenvalues** and **eigenvectors** clarify a transformation’s most **natural** or **fundamental** properties: how it *really* behaves, beyond the specifics of any single coordinate system. This is why they show up everywhere in advanced mathematics, physics, and AI: they isolate the transformation’s essential actions, making complex problems more approachable and more elegant to solve.

### **Further Exploration**

- **Complex Eigenvalues**: Rotations in 2D yield complex eigenvalues with no real eigenvectors, but they still matter for analyzing transformations in the complex plane.  
- **Defective Matrices**: Some matrices have fewer eigenvectors than the dimension of the space (like certain shears), requiring a broader framework (Jordan forms) to handle them.  
- **Applications**: Eigen-decomposition is a bedrock for everything from solving differential equations to Google’s PageRank algorithm and modern neural network initialization strategies.

Ultimately, eigenvalues and eigenvectors are a **bridge** between the numeric representation of a transformation (the matrix) and its **geometric meaning** (stretch, flip, rotate, or reduce). Grasping this bridge unlocks a remarkable amount of power in both theoretical and applied problems.

---

## **On the Number of Eigenvalues**

> **If we pick a 3×3 matrix, do we always get three different eigenvalues—and does that automatically guarantee three separate eigenvectors?** 

Let’s investigate how *repeated eigenvalues* might complicate things.

### **A Quick Refresher**

- An **eigenvalue** $\lambda$ of a square matrix $\mathbf{A}$ is a solution to  

$$
\det(\mathbf{A} - \lambda \mathbf{I}) = 0
$$

- The corresponding **eigenvectors** are nonzero vectors $\mathbf{v}$ that satisfy  

$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}
$$

- The **number of times** a particular eigenvalue appears as a root of the characteristic polynomial is called its **algebraic multiplicity**.  
- The number of **independent** eigenvectors you actually get for that eigenvalue is called its **geometric multiplicity**, and it can be *less* than the algebraic multiplicity.

### **Repeated Eigenvalues in 3×3 Matrices**

#### **Example: Two Distinct Directions for the Same Eigenvalue**

Suppose you have a 3×3 matrix $\mathbf{A}$ whose characteristic polynomial yields eigenvalues $\lambda_1 = 4$, $\lambda_2 = 2$, and again $\lambda_3 = 2$ (so “2” is repeated). One might worry that having the eigenvalue 2 twice means we won’t get enough eigenvectors. But in **some** cases, you still can:

1. **Eigenvalue 4**: You solve $(\mathbf{A} - 4\mathbf{I})\mathbf{v} = \mathbf{0}$ and get a vector $\mathbf{v}_1$.  
2. **Eigenvalue 2 (First Direction)**: You plug $\lambda = 2$ into $(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \mathbf{0}$ and find one eigenvector $\mathbf{v}_2$.  
3. **Eigenvalue 2 (Second Direction)**: Interestingly, because the space of solutions might be 2D (or bigger) for that repeated root, you can find **another** eigenvector $\mathbf{v}_3$ that isn’t just a scalar multiple of $\mathbf{v}_2$.

Hence, you end up with **three** linearly independent eigenvectors, even though one eigenvalue is repeated.

#### **Example: Only One Direction for a Repeated Eigenvalue**

In other situations, you might again get $\lambda_1 = 4$, $\lambda_2 = 2$, $\lambda_3 = 2$, *but* when you solve for $\mathbf{v}$ corresponding to $\lambda=2$, you discover that **only one** independent eigenvector emerges. All the solutions for that eigenvalue lie along a single line in 3D space—meaning the geometric multiplicity of “2” is just 1.

This time, you’ll find:
- **Eigenvalue 4** gives you one independent direction.  
- **Eigenvalue 2** also appears twice in the polynomial, but yields **only one** unique direction.  

So in total, you have just **two** independent eigenvectors for a 3D matrix. You can’t construct a basis of three eigenvectors—thus, the matrix is *not* diagonalizable, and you don’t have a full *eigenbasis*.

### **General Patterns**

1. **2×2 Matrix**:
   - If the two eigenvalues are **different**, you’ll have two distinct eigenvectors—enough for a full eigenbasis in 2D.  

   - If there’s a **repeated** eigenvalue (e.g., both eigenvalues are 3), you might find:
     - **Two** independent eigenvectors, or  
     - **One** eigenvector (then you can’t form a full basis).

2. **3×3 Matrix**:
   - **Three Distinct Eigenvalues**: Always at least one eigenvector per eigenvalue, so you get three independent eigenvectors.  
   - **One Eigenvalue Repeated Twice**: You might have **two** or **three** total independent eigenvectors.  
   - **One Eigenvalue Repeated Three Times**: You could end up with 1, 2, or 3 independent eigenvectors for that single eigenvalue.

The crucial point is the distinction between **algebraic multiplicity** (how many times the eigenvalue shows up in the characteristic polynomial) and **geometric multiplicity** (how many independent vectors correspond to it). A matrix is **diagonalizable** if and only if, for every eigenvalue, **geometric multiplicity = algebraic multiplicity**.

### **Why Care About Repeated Eigenvalues?**

- **Basis Formation**: Repeated eigenvalues might or might not give enough independent directions to span your entire space.  
- **Diagonalization**: If you can’t find a full set of independent eigenvectors, you can’t write $\mathbf{A} = \mathbf{PDP}^{-1}$ in diagonal form.  
- **Complexity in Systems**: In some physical or dynamical systems, repeated eigenvalues can indicate resonance or degenerate modes of behavior.  
- **Eigenvalue Decomposition in AI**: PCA or other methods rely on large covariance matrices where repeated eigenvalues might cluster or hamper direct interpretations of principal components.

### **Key Takeaways**

1. **Eigenvalues can repeat**, and each repeated eigenvalue may provide fewer (or equal) independent eigenvectors than its multiplicity.  
2. **Different outcomes**: Sometimes a repeated eigenvalue yields multiple distinct directions; other times it yields just one.  
3. **Full Eigenbasis**: Only possible when the *total* number of independent eigenvectors adds up to the dimension of your space (e.g., 2 for 2D, 3 for 3D).  
4. **Practical Check**: If you suspect a repeated eigenvalue, solve $(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = \mathbf{0}$ carefully. You might find multiple independent solutions—or just one line of solutions.

Remember: an eigenvalue’s algebraic multiplicity (how many times it appears as a root) doesn’t necessarily match its geometric multiplicity (how many independent vectors it provides). That difference lies at the heart of whether a matrix is easily diagonalizable or not!

---

## **Dimensionality Reduction and Projection**

---

## **Motivating PCA**

---

## **Variance and Covariance**

---

## **Covariance Matrices**

---

## **PCA-Overview**

---

## **PCA-Why It Works**

---

## **PCA-Mathematical Formulation**

---

## **Discrete Dynamical Systems**