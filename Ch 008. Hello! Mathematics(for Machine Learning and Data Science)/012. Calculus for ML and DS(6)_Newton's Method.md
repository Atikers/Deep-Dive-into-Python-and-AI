# Calculus for ML and DS (6)_Newton's Method

## Newton's Method

> **Have you ever had to guess a secret number and fine-tune your guess step by step until you were incredibly close?** 

That’s essentially the idea behind **Newton’s Method**. It’s a powerful technique that, given a good starting guess, can zoom in on the solution of an equation or the minimum of a function very quickly.

### Newton’s Method for Finding Zeros of a Function

Suppose we have a function $f(x)$ and we want to find a value $x^\ast$ such that $f(x^\ast) = 0$. Here’s the Newton’s Method recipe in one variable:

1. **Pick a starting point** $x_0$. (Think of this as your initial guess.)  
2. **Draw the tangent line** to $f(x)$ at $x = x_k$ (your current guess).  
3. **Find where that tangent crosses the x-axis**. That point of intersection becomes your new guess, $x_{k+1}$.  
4. **Repeat** until you (hopefully) converge to a value where $f(x^\ast) \approx 0$.

Mathematically, the tangent line at $x_k$ has slope $f'(x_k)$, and the Newton update formula is:  

$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
$$

- Because, the tangent line at $x_k$ has slope $f'(x_k)$, and $f'(x_k) = \frac{f(x_k)}{x_k - x_{k+1}}$

Why does this formula make sense? Imagine the slope (rise over run). The “rise” is $f(x_k)$, and the “run” we need to adjust by is $\frac{f(x_k)}{f'(x_k)}$. Subtracting this run from the old guess $x_k$ yields the next guess $x_{k+1}$—usually closer to the actual zero of $f$.

### Using Newton’s Method for Optimization

**But what if we want to find a minimum of some function $g(x)$ instead of a zero of $f(x)$?** Recall a basic fact from calculus: a minimum occurs where the derivative $g'(x)$ is zero (or at least, that’s a critical point). So finding the minimum of $g$ is equivalent to finding the zero of $g'(x)$.

1. Let $f(x) = g'(x)$  
2. We look for a point $x^\ast$ such that $g'(x^\ast) = 0$  
3. Apply Newton’s Method to $f(x)$:  

$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

The twist: we now need **both** the first and second derivatives of $g(x)$—that is, $g'(x)$ **and** $g''(x)$—to carry out each Newton update. Then we iterate until $g'(x)$ is close to zero, indicating we’ve found (at least locally) a critical point. Ideally, if $g''(x)$ is positive there, that critical point is a **minimum**.

#### Why do we set f(x) = g'(x)?

This mathematical substitution might seem confusing at first, but it's actually a clever trick to adapt Newton's Method for optimization. Here's why:

1. **Our Real Goal**:
   - We want to find the minimum of function $g(x)$
   - From calculus, we know that minimums occur where $g'(x) = 0$
   - So we need to find $x$ where $g'(x) = 0$

2. **Original Purpose of Newton's Method**:
   - Newton's Method was designed to find roots of equations, i.e., where $f(x) = 0$
   - By setting $f(x) = g'(x)$, we're transforming our "find the minimum" problem
   - Into a "find the root" problem that Newton's Method can solve

3. **Simple Example**:
   - If $g(x) = x^2$
   - Then $g'(x) = 2x$
   - By setting $f(x) = g'(x) = 2x$
   - Finding $f(x) = 0$ gives us $x = 0$, which is indeed the minimum of $x^2$

This substitution is therefore a mathematical bridge that allows us to use a root-finding method (Newton's Method) to solve an optimization problem.

### Step-by-Step Summary

1. **Choose a function** $g(x)$ you want to minimize.  
2. **Compute** $g'(x)$ and $g''(x)$.  
3. **Initialize** with some guess $x_0$.  
4. **Iterate** using Newton’s formula for optimization:  

$$
x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

5. **Stop** when $g'(x)$ is sufficiently small (or when you notice $x_{k+1}$ isn’t changing much).

### A Note on Pros and Cons

- **Pros**: Newton’s Method often converges much faster than standard gradient descent when you’re near the solution. It can “home in” on the minimum in fewer steps.
- **Cons**: You need the second derivative, $g''(x)$. Calculating or storing this for high-dimensional problems can be expensive. Also, Newton’s Method can fail (or diverge) if your starting guess is poor or if $g''(x)$ is close to zero or negative when you expect a minimum.

#### Key Takeaways

- **Newton’s Method**: a technique to find a zero of $f(x)$ by iterating  
  $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$  
- **Optimization**: To minimize $g(x)$, apply Newton’s Method to $g'(x)=0$, yielding  
  $x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}$  
- **Second Derivative**: Critical for determining how big or small a step you take each iteration—and whether the point is actually a minimum.

### Example

#### The Function: $g(x) = e^x - \log(x)$

Let’s revisit a function from an earlier lesson:  

$$
g(x) = e^x - \log(x)
$$

We’re interested in finding **where this function has a minimum**. From basic calculus, a minimum happens when the **first derivative** is zero:  

$$
g'(x) = e^x - \frac{1}{x} = 0
$$

Hence, we’re really hunting for the solution to  

$$
e^x = \frac{1}{x}
$$

Newton’s Method will help us **zoom in** on this solution.

#### Step 1: Set Up $g'(x)$ and $g''(x)$

To apply Newton’s Method for **optimization**, we plug into the formula:  

$$
x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

So we need:
- **First derivative** (already known):  

$$
g'(x) = e^x - \frac{1}{x}
$$

- **Second derivative**:  

$$
(g'(x))' = e^x + \frac{1}{x^2}
$$

#### Step 2: Pick a Starting Guess

Imagine you’re turning that camera lens from a slightly blurry position. We’ll start at a not-so-great guess: $x_0 = 0.05$. That’s pretty close to zero. And note that our function $g(x)$ is valid only for $x>0$, so we can’t pick negative or zero values here.

#### Step 3: Iterate Using Newton’s Formula

For each iteration:  

$$
x_{k+1} = x_k - \frac{e^{x_k} - \frac{1}{x_k}} {e^{x_k} + \frac{1}{x_k^2}}
$$

1. **From $x_0$ to $x_1$**  
- We evaluate $g'(0.05)$ and $g''(0.05)$  
- Plug them in:  
   
$$
x_1 = 0.05 - \frac{\bigl(e^{0.05} - \tfrac{1}{0.05}\bigr)} {\bigl(e^{0.05} + \tfrac{1}{0.05^2}\bigr)} \approx 0.97
$$

- It’s like giving the lens a big twist and suddenly the image is much clearer—**we jump** from $0.05$ to about $0.97$ in one step!

2. **From $x_1$ to $x_2$**  
- Repeat:  

$$
x_2 \approx 0.97 - \frac{e^{0.97} - \tfrac{1}{0.97}} {e^{0.97} + \tfrac{1}{0.97^2}} \approx 0.183
$$

- Notice we changed directions drastically—sometimes you slightly overshoot or undershoot, like toggling the camera lens in the other direction.

3. **From $x_2$ to $x_3$**  
- Another iteration yields  

$$
x_3 \approx 0.320
$$

4. **From $x_3$ to $x_4$**  
- Now  

$$
x_4 \approx 0.477
$$

5. **From $x_4$ to $x_5$**  

$$
x_5 \approx 0.558
$$

6. **From $x_5$ to $x_6$**  
- Finally,  

$$
x_6 \approx 0.567
$$

At this point, $g'(x_6)$ is very close to zero, meaning $x_6$ is extremely close to the actual **minimum**. In fact, $0.567$ is near the famous “Omega constant,” which is about $0.567143\ldots$ for this particular function.

#### Why It Works So Quickly

In many situations, **Newton’s Method** acts like a precise **autofocus system**. Each iteration “sees” not just the slope, **first derivative**, but also how that slope is changing, **second derivative**, allowing it to jump closer to the target solution in fewer steps than a simple gradient descent approach.

#### Key Takeaways

- **Newton’s Method** can home in on a function’s **minimum** with surprisingly few iterations when you have a decent starting point.  
- You need **both** $g'(x)$ and $g''(x)$ to perform each update—a double-edged sword because calculating (or storing) second derivatives can be costly for large-scale problems.  
- Much like **fine-tuning a camera lens** to achieve a perfectly sharp image, each iteration of Newton’s Method adjusts your guess so that the “picture” of your function becomes clearer and the minimum more precisely located.

So there you have it: from a rough guess - $x_0=0.05$ - to a near-exact solution - $x_6 \approx 0.567$ - in just six “clicks” of the lens dial. That’s the power of Newton’s Method in action!

---

## The Second Derivative

> **Ever watched a roller coaster slowly creep up a hill, only to plummet down the other side at breakneck speed?** 

The steepness of its slope at any point can be thought of as the **first derivative**—how fast the track is rising or falling. But the **second derivative** is like the roller coaster’s **“thrill factor”**: it tells us how sharply the track is bending (concave up or concave down), and whether we’re speeding up or slowing down in our climb or descent.

In more formal terms, the second derivative is just “the derivative of the derivative.” If you denote your function by $f(x)$, then:

- The **first derivative** is $f'(x) = \frac{d}{dx}\bigl(f(x)\bigr)$  
- The **second derivative** is $f''(x) = \frac{d}{dx}\bigl(f'(x)\bigr) = \frac{d^2 f}{dx^2}$

### Real-Life Analogy: Driving a Car

Let’s say $x(t)$ measures how far your car has traveled at time $t$. Then:

1. **First derivative** $v(t) = \frac{dx}{dt}$ is your **velocity**. It tells you how fast your position is changing each hour.
2. **Second derivative** $a(t) = \frac{d^2 x}{dt^2}$ is your **acceleration**. It tells you **how quickly your velocity itself is changing**—are you pressing the gas to speed up or the brake to slow down?

- If $a(t) > 0$ (like stepping on the gas), your velocity is **increasing**: the car speeds up.  
- If $a(t) < 0$ (like pressing the brake), your velocity is **decreasing**: the car slows down.  
- If $a(t) = 0$, your velocity is **constant**—you’re cruising at a steady pace.

### Concavity

Now let’s go back to a generic function $f(x)$. The sign of the second derivative, $f''(x)$, determines the function’s **concavity**:

- **Concave Up (Convex)**:  
  - $f''(x) > 0$  
  - The graph of $f(x)$ looks like U-shaped
  - The slope keeps getting “steeper” as you move to the right.  
- **Concave Down**:  
  - $f''(x) < 0$  
  - The graph of $f(x)$ looks like n-shaped
  - The slope becomes “less steep” (or negative faster) as you move to the right.  
- **No Curvature**:  
  - $f''(x) = 0$  
  - The function is changing at a **steady** slope—like a perfectly straight road.

Think of it like a **bowl** versus an **upside-down bowl**. If you pour water into a concave-up bowl, it settles at the bottom (a minimum). If you pour water onto a concave-down surface, it rolls off the peak (a maximum).

### Second Derivative and Optimization

When you want to find maxima or minima:

1. **First Derivative = 0**: This locates a potential **peak** or **valley** in your function.  
2. **Second Derivative**:
   - If $f''(x) > 0$ at that point, you have a **local minimum** (the curve is U-shaped).  
   - If $f''(x) < 0$ at that point, you have a **local maximum** (the curve is n-shaped).  
   - If $f''(x) = 0$ at that point, the test is **inconclusive**; you may need further analysis (it could be a point of inflection).

For example:
- **Local Minimum**: $f'(x^\ast) = 0$ and $f''(x^\ast) > 0$  
- **Local Maximum**: $f'(x^\ast) = 0$ and $f''(x^\ast) < 0$  

This helps us decide whether the point we’ve found by setting $f'(x)=0$ is actually a **valley** (where function values are lower than nearby points) or a **hill** (where function values are higher).

### Why It Matters for Newton’s Method

When using **Newton’s Method** to find the minimum of a function $g(x)$, the update step uses not only $g'(x)$ but also $g''(x)$. This second derivative ensures you’re adjusting your guess in the right direction and by the right amount. Essentially, you’re looking at:  

$$
x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

The second derivative here is like having extra “road information” when driving. Not only do you know if you need to speed up or slow down, but you also know how sharply the road curves. This knowledge helps you get to the “lowest” point (the minimum) faster and with fewer detours.

### Key Takeaways

- **Second Derivative** = “Derivative of the derivative.” It captures how **quickly** your slope is changing.  
- **Physical Analogy**: In motion, second derivative is **acceleration**—the rate of change of velocity.  
- **Concavity**: $f''(x) > 0$ means "concave up", while $f''(x) < 0$ means "concave down".  
- **Optimization**:  
  - A point where $f'(x)=0$ can be a max or a min.  
  - The second derivative’s sign tells you which one it is (max if negative, min if positive).  

So, next time you’re riding a roller coaster or accelerating on the highway, remember: that “push” forward or backward feeling is your body’s real-life encounter with the second derivative!

---

## The Hessian

> **Have you ever tried to figure out the shape of a trampoline’s surface after a few people stand on it at different spots?** 

In one dimension, you only worry about “how steep the slope is” and “how quickly it’s curving up or down.” But in two dimensions—like a stretched-out trampoline surface—you suddenly have to keep track of curvature in the **x**-direction **and** the **y**-direction, **plus** how those directions affect each other. That’s where the **Hessian** comes in.

### From One Variable to Two (or More)

- **One variable**: A function $f(x)$ has a **first derivative** $f'(x)$ and a **second derivative** $f''(x)$.  
  - $f'(x)$ tells you how fast $f$ is changing.  
  - $f''(x)$ tells you how fast that rate of change is itself changing—essentially the curvature.

- **Two variables**: A function $f(x,y)$ now depends on both $x$ and $y$.  
  - We have two **first partial derivatives**:  
    - $f_x(x,y)$: how $f$ changes if we nudge $x$ a bit, keeping $y$ fixed.  
    - $f_y(x,y)$: how $f$ changes if we nudge $y$ a bit, keeping $x$ fixed.  
  - For the **second derivatives**, we look at all possible ways to take the derivative of $f_x$ and $f_y$ again—w.r.t. $x$ or $y$.

This leads to **four** basic second partial derivatives:

1. $f_{xx}(x,y) = \dfrac{\partial}{\partial x}(f_x(x,y))$  
2. $f_{yy}(x,y) = \dfrac{\partial}{\partial y}(f_y(x,y))$  
3. $f_{xy}(x,y) = \dfrac{\partial}{\partial y}(f_x(x,y))$  
4. $f_{yx}(x,y) = \dfrac{\partial}{\partial x}(f_y(x,y))$

Under most smoothness conditions, $f_{xy}(x,y) = f_{yx}(x,y)$, meaning the order in which you take derivatives doesn’t matter.

This property - that mixed partial derivatives are equal, $f_{xy} = f_{yx}$, when certain smoothness conditions are met - is known as Clairaut's theorem or Schwarz's theorem. Let me explain why this is true intuitively and mathematically:

#### Intuitive Explanation

Imagine you're on a hill and want to measure how the slope changes:
- $f_{xy}$ means: First move in x-direction and measure the slope change, then move in y-direction and measure how that slope change itself changes
- $f_{yx}$ means: First move in y-direction and measure the slope change, then move in x-direction and measure how that slope change itself changes

If the surface is "smooth enough" (continuous and differentiable), the order shouldn't matter - you'll end up at the same point measuring the same local curvature.

#### Mathematical Requirements

For this theorem to hold, the function must satisfy these conditions:
1. Both partial derivatives must exist in a neighborhood of the point
2. The mixed partial derivatives must be continuous at the point

#### Simple Example

Let's look at $f(x,y) = xy^2$:

1. Finding $f_{xy}$:
   - First take $\frac{\partial}{\partial x}$: $f_x = y^2$
   - Then take $\frac{\partial}{\partial y}$ of that: $f_{xy} = 2y$

2. Finding $f_{yx}$:
   - First take $\frac{\partial}{\partial y}$: $f_y = 2xy$
   - Then take $\frac{\partial}{\partial x}$ of that: $f_{yx} = 2y$

As expected, $f_{xy} = f_{yx} = 2y$

### Counter-Example

When the smoothness conditions aren't met, the equality might fail. A classic example is:  

$$
f(x,y) = \begin{cases} 
\frac{xy(x^2-y^2)}{x^2+y^2} & \text{if } (x,y) \neq (0,0) \\
0 & \text{if } (x,y) = (0,0)
\end{cases}
$$

At (0,0), $f_{xy} \neq f_{yx}$ because the function isn't "smooth enough" at that point.

This property is crucial in multivariable calculus and has important applications in optimization and machine learning, as it allows us to compute partial derivatives in whichever order is most convenient.

### The Hessian Matrix

All these second partial derivatives combine into a square matrix called the **Hessian**:

$$
H(f)(x,y) = \begin{pmatrix}
f_{xx}(x,y) & f_{xy}(x,y) \\
f_{yx}(x,y) & f_{yy}(x,y)
\end{pmatrix}
$$

Think of the Hessian like a **2D “curvature report”** for $f(x,y)$:

- The diagonal entries, $f_{xx}$ and $f_{yy}$, tell you how $f$ bends along the **x** and **y** axes individually.  
- The off-diagonal entries, $f_{xy}$ and $f_{yx}$, capture **coupling**: how a tiny change in $y$ affects the slope in the $x$-direction and vice versa.

**Analogy**:  
- If you’re in a one-dimensional world, “second derivative” is just one number that tells you if the road is curving up or down.  
- In a two-dimensional world, you’re on a **mountain** with **x** and **y** directions. The Hessian is like a **map** that not only shows how steep the climb is in the east-west direction ($x$) and north-south direction ($y$), but also how the slope in one direction might tilt if you move in the other direction.

### Example: $f(x,y) = 2x^2 + 3y^2 - xy$

1. **First partial derivatives**:  
   - $f_x(x,y) = \frac{\partial}{\partial x}(2x^2 + 3y^2 - xy) = 4x - y$  
   - $f_y(x,y) = \frac{\partial}{\partial y}(2x^2 + 3y^2 - xy) = 6y - x$

2. **Second partial derivatives**:  
   - $f_{xx} = \frac{\partial}{\partial x}(4x - y) = 4$  
   - $f_{xy} = \frac{\partial}{\partial y}(4x - y) = -1$  
   - $f_{yx} = \frac{\partial}{\partial x}(6y - x) = -1$  
   - $f_{yy} = \frac{\partial}{\partial y}(6y - x) = 6$

3. **Hessian matrix**:  

$$
H(f)(x,y) = \begin{pmatrix} 4 & -1 \\
-1 & 6 \end{pmatrix}
$$

This $2\times 2$ matrix bundles all the second derivative information into one neat package. Notice that $f_{xy} = f_{yx} = -1$, which is typical for smooth functions.

### Why Is the Hessian Important?

- **Curvature in Multiple Directions**: The Hessian indicates if you’re at a local minimum, local maximum, or a saddle point when the gradient (the vector of first partials) is zero.  
- **Multivariate Newton’s Method**: Remember how in one dimension we used $x_{k+1} = x_k - \dfrac{f'(x_k)}{f''(x_k)}$? In two dimensions, we do something similar but replace $f''(x_k)$ (a single number) with the **inverse** of the Hessian matrix. Essentially,  

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H(f)^{-1}(\mathbf{x}_k)\nabla f(\mathbf{x}_k).
$$

- The Hessian helps us “curve” our step in the right direction for faster convergence to a local minimum.

- **Shape Analysis**: In machine learning or data science, we often want to know how “bumpy” or “curvy” our loss surface is. The Hessian reveals whether you’re in a gentle valley, a steep gorge, or a saddle.

### Intuition: “Multi-Directional Acceleration”

In one dimension, a second derivative is like an **acceleration**—how your velocity (slope) changes. In two (or more) dimensions, the Hessian is like having a record of all possible accelerations in **every** direction around your point:

- **Diagonal entries**: How the slope in $x$ changes if you move in $x$, and how the slope in $y$ changes if you move in $y$.  
- **Off-diagonal entries**: How the slope in $x$ changes if you move in $y$, and vice versa.

It’s as if you’re checking not just how bumpy the terrain is along one direction but also how that bumpiness shifts when you move sideways in another direction.

## Key Takeaways

1. **The Hessian is the “second derivative” in multiple dimensions**—a matrix summarizing curvature in all directions.  
2. **Diagonal entries** measure curvature along each coordinate axis; **off-diagonal entries** measure how those axes influence each other.  
3. **Multivariate Newton’s Method** uses the Hessian to more accurately jump toward minima, just like single-variable Newton’s Method but now with matrix operations.  
4. Understanding the Hessian is crucial for analyzing local maxima, minima, and saddle points when dealing with functions of more than one variable.

So if someone’s “standing on a trampoline,” the Hessian is the blueprint telling you exactly how the fabric (the function) bends in all directions—whether it’s pulling you down into a valley, pushing you up on a ridge, or sitting you precariously on a saddle in between!

---

## Hessians and Concavity

## Newton's Method for Two Variables


