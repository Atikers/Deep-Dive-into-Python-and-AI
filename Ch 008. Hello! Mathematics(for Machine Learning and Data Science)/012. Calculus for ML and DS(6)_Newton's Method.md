# Calculus for ML and DS (6)_Newton's Method

## Newton's Method

> **Have you ever had to guess a secret number and fine-tune your guess step by step until you were incredibly close?** 

That’s essentially the idea behind **Newton’s Method**. It’s a powerful technique that, given a good starting guess, can zoom in on the solution of an equation or the minimum of a function very quickly.

### Newton’s Method for Finding Zeros of a Function

Suppose we have a function $f(x)$ and we want to find a value $x^\ast$ such that $f(x^\ast) = 0$. Here’s the Newton’s Method recipe in one variable:

1. **Pick a starting point** $x_0$. (Think of this as your initial guess.)  
2. **Draw the tangent line** to $f(x)$ at $x = x_k$ (your current guess).  
3. **Find where that tangent crosses the x-axis**. That point of intersection becomes your new guess, $x_{k+1}$.  
4. **Repeat** until you (hopefully) converge to a value where $f(x^\ast) \approx 0$.

Mathematically, the tangent line at $x_k$ has slope $f'(x_k)$, and the Newton update formula is:  

$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
$$

- Because, the tangent line at $x_k$ has slope $f'(x_k)$, and $f'(x_k) = \frac{f(x_k)}{x_k - x_{k+1}}$

Why does this formula make sense? Imagine the slope (rise over run). The “rise” is $f(x_k)$, and the “run” we need to adjust by is $\frac{f(x_k)}{f'(x_k)}$. Subtracting this run from the old guess $x_k$ yields the next guess $x_{k+1}$—usually closer to the actual zero of $f$.

### Using Newton’s Method for Optimization

**But what if we want to find a minimum of some function $g(x)$ instead of a zero of $f(x)$?** Recall a basic fact from calculus: a minimum occurs where the derivative $g'(x)$ is zero (or at least, that’s a critical point). So finding the minimum of $g$ is equivalent to finding the zero of $g'(x)$.

1. Let $f(x) = g'(x)$  
2. We look for a point $x^\ast$ such that $g'(x^\ast) = 0$  
3. Apply Newton’s Method to $f(x)$:  

$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

The twist: we now need **both** the first and second derivatives of $g(x)$—that is, $g'(x)$ **and** $g''(x)$—to carry out each Newton update. Then we iterate until $g'(x)$ is close to zero, indicating we’ve found (at least locally) a critical point. Ideally, if $g''(x)$ is positive there, that critical point is a **minimum**.

#### Why do we set f(x) = g'(x)?

This mathematical substitution might seem confusing at first, but it's actually a clever trick to adapt Newton's Method for optimization. Here's why:

1. **Our Real Goal**:
   - We want to find the minimum of function $g(x)$
   - From calculus, we know that minimums occur where $g'(x) = 0$
   - So we need to find $x$ where $g'(x) = 0$

2. **Original Purpose of Newton's Method**:
   - Newton's Method was designed to find roots of equations, i.e., where $f(x) = 0$
   - By setting $f(x) = g'(x)$, we're transforming our "find the minimum" problem Into a "find the root" problem that Newton's Method can solve

3. **Simple Example**:
   - If $g(x) = x^2$
   - Then $g'(x) = 2x$
   - By setting $f(x) = g'(x) = 2x$
   - Finding $f(x) = 0$ gives us $x = 0$, which is indeed the minimum of $x^2$

This substitution is therefore a mathematical bridge that allows us to use a root-finding method (Newton's Method) to solve an optimization problem.

### Step-by-Step Summary

1. **Choose a function** $g(x)$ you want to minimize.  
2. **Compute** $g'(x)$ and $g''(x)$.  
3. **Initialize** with some guess $x_0$.  
4. **Iterate** using Newton’s formula for optimization:  

$$
x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

5. **Stop** when $g'(x)$ is sufficiently small (or when you notice $x_{k+1}$ isn’t changing much).

### A Note on Pros and Cons

- **Pros**: Newton’s Method often converges much faster than standard gradient descent when you’re near the solution. It can “home in” on the minimum in fewer steps.
- **Cons**: You need the second derivative, $g''(x)$. Calculating or storing this for high-dimensional problems can be expensive. Also, Newton’s Method can fail (or diverge) if your starting guess is poor or if $g''(x)$ is close to zero or negative when you expect a minimum.

#### Key Takeaways

- **Newton’s Method**: a technique to find a zero of $f(x)$ by iterating  
  $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$  
- **Optimization**: To look for a point $x^\ast$ such that $g'(x^\ast) = 0$, apply Newton’s Method to $g'(x)=0$, yielding $x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}$  
- **Second Derivative**: Critical for determining how big or small a step you take each iteration—and whether the point is actually a minimum.

### Example

#### The Function: $g(x) = e^x - \log(x)$

Let’s revisit a function from an earlier section:  

$$
g(x) = e^x - \log(x)
$$

We’re interested in finding **where this function has a minimum**. From basic calculus, a minimum happens when the **first derivative** is zero:  

$$
g'(x) = e^x - \frac{1}{x} = 0
$$

Hence, we’re really hunting for the solution to  

$$
e^x = \frac{1}{x}
$$

Newton’s Method will help us **zoom in** on this solution.

#### Step 1: Set Up $g'(x)$ and $g''(x)$

To apply Newton’s Method for **optimization**, we plug into the formula:  

$$
x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

So we need:
- **First derivative** (already known):  

$$
g'(x) = e^x - \frac{1}{x}
$$

- **Second derivative**:  

$$
(g'(x))' = e^x + \frac{1}{x^2}
$$

#### Step 2: Pick a Starting Guess

Imagine you’re turning that camera lens from a slightly blurry position. We’ll start at a not-so-great guess: $x_0 = 0.05$. That’s pretty close to zero. And note that our function $g(x)$ is valid only for $x>0$, so we can’t pick negative or zero values here.

#### Step 3: Iterate Using Newton’s Formula

For each iteration:  

$$
x_{k+1} = x_k - \frac{e^{x_k} - \frac{1}{x_k}} {e^{x_k} + \frac{1}{x_k^2}}
$$

1. **From $x_0$ to $x_1$**  
- We evaluate $g'(0.05)$ and $g''(0.05)$  
- Plug them in:  
   
$$
x_1 = 0.05 - \frac{\bigl(e^{0.05} - \tfrac{1}{0.05}\bigr)} {\bigl(e^{0.05} + \tfrac{1}{0.05^2}\bigr)} \approx 0.97
$$

- It’s like giving the lens a big twist and suddenly the image is much clearer—**we jump** from $0.05$ to about $0.97$ in one step!

2. **From $x_1$ to $x_2$**  
- Repeat:  

$$
x_2 \approx 0.97 - \frac{e^{0.97} - \tfrac{1}{0.97}} {e^{0.97} + \tfrac{1}{0.97^2}} \approx 0.183
$$

- Notice we changed directions drastically—sometimes you slightly overshoot or undershoot, like toggling the camera lens in the other direction.

3. **From $x_2$ to $x_3$**  
- Another iteration yields  

$$
x_3 \approx 0.320
$$

4. **From $x_3$ to $x_4$**  
- Now  

$$
x_4 \approx 0.477
$$

5. **From $x_4$ to $x_5$**  

$$
x_5 \approx 0.558
$$

6. **From $x_5$ to $x_6$**  
- Finally,  

$$
x_6 \approx 0.567
$$

At this point, $g'(x_6)$ is very close to zero, meaning $x_6$ is extremely close to the actual **minimum**. In fact, $0.567$ is near the famous “Omega constant,” which is about $0.567143\ldots$ for this particular function.

#### Why It Works So Quickly

In many situations, **Newton’s Method** acts like a precise **autofocus system**. Each iteration “sees” not just the slope, **first derivative**, but also how that slope is changing, **second derivative**, allowing it to jump closer to the target solution in fewer steps than a simple gradient descent approach.

#### Key Takeaways

- **Newton’s Method** can home in on a function’s **minimum** with surprisingly few iterations when you have a decent starting point.  
- You need **both** $g'(x)$ and $g''(x)$ to perform each update—a double-edged sword because calculating (or storing) second derivatives can be costly for large-scale problems.  
- Much like **fine-tuning a camera lens** to achieve a perfectly sharp image, each iteration of Newton’s Method adjusts your guess so that the “picture” of your function becomes clearer and the minimum more precisely located.

So there you have it: from a rough guess - $x_0=0.05$ - to a near-exact solution - $x_6 \approx 0.567$ - in just six “clicks” of the lens dial. That’s the power of Newton’s Method in action!

---

## The Second Derivative

> **Ever watched a roller coaster slowly creep up a hill, only to plummet down the other side at breakneck speed?** 

The steepness of its slope at any point can be thought of as the **first derivative**—how fast the track is rising or falling. But the **second derivative** is like the roller coaster’s **“thrill factor”**: it tells us how sharply the track is bending (concave up or concave down), and whether we’re speeding up or slowing down in our climb or descent.

In more formal terms, the second derivative is just “the derivative of the derivative.” If you denote your function by $f(x)$, then:

- The **first derivative** is $f'(x) = \frac{d}{dx}(f(x))$  
- The **second derivative** is $f''(x) = \frac{d}{dx}(f'(x)) = \frac{d^2 f}{dx^2}$

### Real-Life Analogy: Driving a Car

Let’s say $x(t)$ measures how far your car has traveled at time $t$. Then:

1. **First derivative** $v(t) = \frac{dx}{dt}$ is your **velocity**. It tells you how fast your position is changing each hour.
2. **Second derivative** $a(t) = \frac{d^2 x}{dt^2}$ is your **acceleration**. It tells you **how quickly your velocity itself is changing**—are you pressing the gas to speed up or the brake to slow down?

- If $a(t) > 0$ (like stepping on the gas), your velocity is **increasing**: the car speeds up.  
- If $a(t) < 0$ (like pressing the brake), your velocity is **decreasing**: the car slows down.  
- If $a(t) = 0$, your velocity is **constant**—you’re cruising at a steady pace.

### Concavity

Now let’s go back to a generic function $f(x)$. The sign of the second derivative, $f''(x)$, determines the function’s **concavity**:

- **Concave Up (Convex)**:  
  - $f''(x) > 0$  
  - The graph of $f(x)$ looks like U-shaped
  - The slope keeps getting “steeper” as you move to the right.  
- **Concave Down**:  
  - $f''(x) < 0$  
  - The graph of $f(x)$ looks like n-shaped
  - The slope becomes “less steep” (or negative faster) as you move to the right.  
- **No Curvature**:  
  - $f''(x) = 0$  
  - The function is changing at a **steady** slope—like a perfectly straight road.

> If you want to understand this concept better, you can watch this video: [Second Derivative Test](https://www.youtube.com/watch?v=BLkz5LGWihw)

Think of it like a **bowl** versus an **upside-down bowl**. If you pour water into a concave-up bowl, it settles at the bottom (a minimum). If you pour water onto a concave-down surface, it rolls off the peak (a maximum).

### Second Derivative and Optimization

When you want to find maxima or minima:

1. **First Derivative = 0**: This locates a potential **peak** or **valley** in your function.  
2. **Second Derivative**:
   - If $f''(x) > 0$ at that point, you have a **local minimum** (the curve is U-shaped).  
   - If $f''(x) < 0$ at that point, you have a **local maximum** (the curve is n-shaped).  
   - If $f''(x) = 0$ at that point, the test is **inconclusive**; you may need further analysis (it could be a point of inflection).

For example:
- **Local Minimum**: $f'(x^\ast) = 0$ and $f''(x^\ast) > 0$  
- **Local Maximum**: $f'(x^\ast) = 0$ and $f''(x^\ast) < 0$  

This helps us decide whether the point we’ve found by setting $f'(x)=0$ is actually a **valley** (where function values are lower than nearby points) or a **hill** (where function values are higher).

### Why It Matters for Newton’s Method

When using **Newton’s Method** to find the minimum of a function $g(x)$, the update step uses not only $g'(x)$ but also $g''(x)$. This second derivative ensures you’re adjusting your guess in the right direction and by the right amount. Essentially, you’re looking at:  

$$
x_{k+1} = x_k - \frac{g'(x_k)}{g''(x_k)}
$$

The second derivative here is like having extra “road information” when driving. Not only do you know if you need to speed up or slow down, but you also know how sharply the road curves. This knowledge helps you get to the “lowest” point (the minimum) faster and with fewer detours.

### Key Takeaways

- **Second Derivative** = “Derivative of the derivative.” It captures how **quickly** your slope is changing.  
- **Physical Analogy**: In motion, second derivative is **acceleration**—the rate of change of velocity.  
- **Concavity**: $f''(x) > 0$ means "concave up", while $f''(x) < 0$ means "concave down".  
- **Optimization**:  
  - A point where $f'(x)=0$ can be a max or a min.  
  - The second derivative’s sign tells you which one it is (max if negative, min if positive).  

So, next time you’re riding a roller coaster or accelerating on the highway, remember: that “push” forward or backward feeling is your body’s real-life encounter with the second derivative!

---

## The Hessian

> **Have you ever tried to figure out the shape of a trampoline’s surface after a few people stand on it at different spots?** 

In one dimension, you only worry about “how steep the slope is” and “how quickly it’s curving up or down.” But in two dimensions—like a stretched-out trampoline surface—you suddenly have to keep track of curvature in the **x**-direction **and** the **y**-direction, **plus** how those directions affect each other. That’s where the **Hessian** comes in.

### From One Variable to Two (or More)

- **One variable**: A function $f(x)$ has a **first derivative** $f'(x)$ and a **second derivative** $f''(x)$.  
  - $f'(x)$ tells you how fast $f$ is changing.  
  - $f''(x)$ tells you how fast that rate of change is itself changing—essentially the curvature.

- **Two variables**: A function $f(x,y)$ now depends on both $x$ and $y$.  
  - We have two **first partial derivatives**:  
    - $f_x(x,y)$: how $f$ changes if we nudge $x$ a bit, keeping $y$ fixed.  
    - $f_y(x,y)$: how $f$ changes if we nudge $y$ a bit, keeping $x$ fixed.  
  - For the **second derivatives**, we look at all possible ways to take the derivative of $f_x$ and $f_y$ again—w.r.t. $x$ or $y$.

This leads to **four** basic second partial derivatives:

1. $f_{xx}(x,y) = \dfrac{\partial}{\partial x}(f_x(x,y))$  
2. $f_{yy}(x,y) = \dfrac{\partial}{\partial y}(f_y(x,y))$  
3. $f_{xy}(x,y) = \dfrac{\partial}{\partial y}(f_x(x,y))$  
4. $f_{yx}(x,y) = \dfrac{\partial}{\partial x}(f_y(x,y))$

Under most smoothness conditions, $f_{xy}(x,y) = f_{yx}(x,y)$, meaning the order in which you take derivatives doesn’t matter.

This property - that mixed partial derivatives are equal, $f_{xy} = f_{yx}$, when certain smoothness conditions are met - is known as Clairaut's theorem or Schwarz's theorem. Let me explain why this is true intuitively and mathematically:

#### Intuitive Explanation

Imagine you're on a hill and want to measure how the slope changes:
- $f_{xy}$ means: First move in x-direction and measure the slope change, then move in y-direction and measure how that slope change itself changes
- $f_{yx}$ means: First move in y-direction and measure the slope change, then move in x-direction and measure how that slope change itself changes

If the surface is "smooth enough" (continuous and differentiable), the order shouldn't matter - you'll end up at the same point measuring the same local curvature.

#### Mathematical Requirements

For this theorem to hold, the function must satisfy these conditions:
1. Both partial derivatives must exist in a neighborhood of the point
2. The mixed partial derivatives must be continuous at the point

#### Simple Example

Let's look at $f(x,y) = xy^2$:

1. Finding $f_{xy}$:
   - First take $\frac{\partial}{\partial x}$: $f_x = y^2$
   - Then take $\frac{\partial}{\partial y}$ of that: $f_{xy} = 2y$

2. Finding $f_{yx}$:
   - First take $\frac{\partial}{\partial y}$: $f_y = 2xy$
   - Then take $\frac{\partial}{\partial x}$ of that: $f_{yx} = 2y$

As expected, $f_{xy} = f_{yx} = 2y$

### Counter-Example

When the smoothness conditions aren't met, the equality might fail. A classic example is:  

$$
f(x,y) = \begin{cases} 
\frac{xy(x^2-y^2)}{x^2+y^2} & \text{if } (x,y) \neq (0,0) \\
0 & \text{if } (x,y) = (0,0)
\end{cases}
$$

At (0,0), $f_{xy} \neq f_{yx}$ because the function isn't "smooth enough" at that point.

This property is crucial in multivariable calculus and has important applications in optimization and machine learning, as it allows us to compute partial derivatives in whichever order is most convenient.

### The Hessian Matrix

All these second partial derivatives combine into a square matrix called the **Hessian**:  

$$
H(f)(x,y) = \begin{pmatrix}
f_{xx}(x,y) & f_{xy}(x,y) \\
f_{yx}(x,y) & f_{yy}(x,y)
\end{pmatrix}
$$

Think of the Hessian like a **2D “curvature report”** for $f(x,y)$:

- The diagonal entries, $f_{xx}$ and $f_{yy}$, tell you how $f$ bends along the **x** and **y** axes individually.  
- The off-diagonal entries, $f_{xy}$ and $f_{yx}$, capture **coupling**: how a tiny change in $y$ affects the slope in the $x$-direction and vice versa.

**Analogy**:  
- If you’re in a one-dimensional world, “second derivative” is just one number that tells you if the road is curving up or down.  
- In a two-dimensional world, you’re on a **mountain** with **x** and **y** directions. The Hessian is like a **map** that not only shows how steep the climb is in the east-west direction ($x$) and north-south direction ($y$), but also how the slope in one direction might tilt if you move in the other direction.

### Example: $f(x,y) = 2x^2 + 3y^2 - xy$

1. **First partial derivatives**:  
   - $f_x(x,y) = \frac{\partial}{\partial x}(2x^2 + 3y^2 - xy) = 4x - y$  
   - $f_y(x,y) = \frac{\partial}{\partial y}(2x^2 + 3y^2 - xy) = 6y - x$

2. **Second partial derivatives**:  
   - $f_{xx} = \frac{\partial}{\partial x}(4x - y) = 4$  
   - $f_{xy} = \frac{\partial}{\partial y}(4x - y) = -1$  
   - $f_{yx} = \frac{\partial}{\partial x}(6y - x) = -1$  
   - $f_{yy} = \frac{\partial}{\partial y}(6y - x) = 6$

3. **Hessian matrix**:  

$$
H(f)(x,y) = \begin{pmatrix} 4 & -1 \\
-1 & 6 \end{pmatrix}
$$

This $2\times 2$ matrix bundles all the second derivative information into one neat package. Notice that $f_{xy} = f_{yx} = -1$, which is typical for smooth functions.

### Why Is the Hessian Important?

- **Curvature in Multiple Directions**: The Hessian indicates if you’re at a local minimum, local maximum, or a saddle point when the gradient (the vector of first partials) is zero.  
- **Multivariate Newton’s Method**: Remember how in one dimension we used $x_{k+1} = x_k - \dfrac{f'(x_k)}{f''(x_k)}$? In two dimensions, we do something similar but replace $f''(x_k)$ (a single number) with the **inverse** of the Hessian matrix. Essentially,  

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H(f)^{-1}(\mathbf{x}_k)\nabla f(\mathbf{x}_k)
$$

- The Hessian helps us “curve” our step in the right direction for faster convergence to a local minimum.

- **Shape Analysis**: In machine learning or data science, we often want to know how “bumpy” or “curvy” our loss surface is. The Hessian reveals whether you’re in a gentle valley, a steep gorge, or a saddle.

### Intuition: “Multi-Directional Acceleration”

In one dimension, a second derivative is like an **acceleration**—how your velocity (slope) changes. In two (or more) dimensions, the Hessian is like having a record of all possible accelerations in **every** direction around your point:

- **Diagonal entries**: How the slope in $x$ changes if you move in $x$, and how the slope in $y$ changes if you move in $y$.  
- **Off-diagonal entries**: How the slope in $x$ changes if you move in $y$, and vice versa.

It’s as if you’re checking not just how bumpy the terrain is along one direction but also how that bumpiness shifts when you move sideways in another direction.

## Key Takeaways

1. **The Hessian is the “second derivative” in multiple dimensions**—a matrix summarizing curvature in all directions.  
2. **Diagonal entries** measure curvature along each coordinate axis; **off-diagonal entries** measure how those axes influence each other.  
3. **Multivariate Newton’s Method** uses the Hessian to more accurately jump toward minima, just like single-variable Newton’s Method but now with matrix operations.  
4. Understanding the Hessian is crucial for analyzing local maxima, minima, and saddle points when dealing with functions of more than one variable.

So if someone’s “standing on a trampoline,” the Hessian is the blueprint telling you exactly how the fabric (the function) bends in all directions—whether it’s pulling you down into a valley, pushing you up on a ridge, or sitting you precariously on a saddle in between!

---

## Hessians and Concavity

> **Imagine standing on different surfaces—a bowl, a dome, or a saddle:** each one curves differently under your feet. In **one dimension**, a positive second derivative means a “U-shape” (a local minimum), and a negative second derivative means an “n-shape” (a local maximum). But in **two dimensions**, the “bowl,” “dome,” or “saddle” shapes are captured by the **Hessian** and its **eigenvalues**. Let’s explore how that works and why it matters for determining minima, maxima, or saddle points in multivariable functions.

### Recap: Single Variable vs. Multiple Variables

In **one variable**, $f(x)$:
- **Local minimum**: $f'(x^\ast)=0$ and $f''(x^\ast)>0$ (U-shape)  
- **Local maximum**: $f'(x^\ast)=0$ and $f''(x^\ast)<0$ (n-shape)  
- **Inconclusive**: $f'(x^\ast)=0$ and $f''(x^\ast)=0$

In **two variables**, $f(x,y)$:
1. We still need the first derivatives (partial derivatives) to vanish at a critical point:  

$$
\frac{\partial f}{\partial x}(x^\ast,y^\ast) = 0, 
\quad
\frac{\partial f}{\partial y}(x^\ast,y^\ast) = 0
$$

2. We then look at the Hessian matrix $H$ at $(x^\ast,y^\ast)$. The Hessian $H$ is a $2\times 2$ matrix of second partial derivatives:  

$$
H(x^\ast,y^\ast) = \begin{pmatrix}
f_{xx}(x^\ast,y^\ast) & f_{xy}(x^\ast,y^\ast) \\
f_{yx}(x^\ast,y^\ast) & f_{yy}(x^\ast,y^\ast)
\end{pmatrix}
$$

3. Instead of a single second derivative value (as in 1D), you check the **eigenvalues** $\lambda_1$ and $\lambda_2$ of this matrix. 

> If you want to refresh your memory on eigenvalues, refer to previous section: [Eigenvalues and Eigenvectors](https://github.com/Atikers/Deep-Dive-into-Python-and-AI/blob/main/Ch%20008.%20Hello!%20Mathematics(for%20Machine%20Learning%20and%20Data%20Science)/006.%20Linear%20Algebra(6)_Eigenvalues%20and%20Eigenvectors.md)

### Eigenvalues and Shape: “Bowl,” “Dome,” or “Saddle”

The **signs** of $\lambda_1$ and $\lambda_2$ determine the surface shape at the critical point: 

1. **Both eigenvalues positive**, $\lambda_1>0, \lambda_2>0$  
   - **Bowl shape** (concave up).  
   - Like a **U-shaped** paraboloid in 2D.  
   - The point is a **local minimum**.

2. **Both eigenvalues negative**, $\lambda_1<0, \lambda_2<0$  
   - **Dome shape** (concave down).  
   - Like an **n-shaped** paraboloid.  
   - The point is a **local maximum**.

3. **One eigenvalue positive and the other negative**, $\lambda_1>0, \lambda_2<0$ or $\lambda_1<0, \lambda_2>0$  
   - **Saddle shape**.  
   - The surface curves up in one direction and down in another (think of a **potato chip** or an actual **horse saddle**).  
   - The point is a **saddle point**—neither a maximum nor a minimum.

4. **Any eigenvalue is zero** or there’s another mix (e.g., some positive, some zero, or in higher dimensions, some negative and some zero)  
   - **Inconclusive** using this test alone. We need more advanced analysis to know the nature of the critical point.

### Concrete Examples

1. **Concave Up (Local Minimum)**  
- Example: $f(x,y) = 2x^2 + 3y^2 - xy$.  
- The Hessian at $(0,0)$ is  
   
$$
H(0,0) = \begin{pmatrix}
4 & -1 \\
-1 & 6
\end{pmatrix}
$$

$$
\text{det}(H(0,0)- \lambda I) = \text{det}\begin{pmatrix}
4-\lambda & -1 \\
-1 & 6-\lambda \end{pmatrix} = (4-\lambda)(6-\lambda) - (-1)(-1) = 23 - 10\lambda + \lambda^2
$$

- Its eigenvalues turn out to be **positive**, $\lambda_1 \approx 6.41$, $\lambda_2 \approx 3.59$.  
- Because they are both positive, $(0,0)$ is a **local minimum**—like the bottom of a **bowl**.

2. **Concave Down (Local Maximum)**  
- Example: $f(x,y) = -2x^2 - 3y^2 - xy + 15$.  
- The Hessian at $(0,0)$ might be  
   
$$
\begin{pmatrix}
-4 & -1 \\
-1 & -6
\end{pmatrix}
$$  

$$
\text{det}(H(0,0)- \lambda I) = \text{det}\begin{pmatrix}
-4-\lambda & -1 \\
-1 & -6-\lambda \end{pmatrix} = (-4-\lambda)(-6-\lambda) - (-1)(-1) = 23 + 10\lambda + \lambda^2
$$

- Eigenvalues are both negative, $\lambda_1 \approx -3.59$, $\lambda_2 \approx -6.41$.  
- That means a **dome** shape, so $(0,0)$ is a **local maximum**.

3. **Saddle Point**  
- Example: $f(x,y) = 2x^2 - 2y^2$.  
- Hessian at $(0,0)$ is  

$$
\begin{pmatrix}
4 & 0 \\
0 & -4
\end{pmatrix}
$$  

$$
\text{det}(H(0,0)- \lambda I) = \text{det}\begin{pmatrix}
4-\lambda & 0 \\
0 & -4-\lambda \end{pmatrix} = (4-\lambda)(-4-\lambda) = 16 - \lambda^2
$$

- The eigenvalues are $\lambda_1=4$ (positive) and $\lambda_2=-4$ (negative).  
- This mismatch of signs indicates a **saddle** shape—up in one direction, down in the other.

### General Case: More Variables

For functions $f(x_1,x_2,\dots,x_n)$, the Hessian is an $n\times n$ matrix of all second partial derivatives. We check its **eigenvalues**:

- **All positive**: local minimum (concave up in every direction).  
- **All negative**: local maximum (concave down in every direction).  
- **Mixed signs**: saddle point.  
- **Zeros or other combos**: inconclusive; you’d need deeper analysis.

### Key Takeaways

1. **Eigenvalues of the Hessian** = The key to multivariable “second-derivative tests.”  
2. **Concave Up**: If All eigenvalues $>0$, “Bowl”, a local minimum.  
3. **Concave Down**: If All eigenvalues $<0$, “Dome”, a local maximum.  
4. **Mixed Signs**: “Saddle” shape.  
5. **Zeros** or complicated mixes: Test is inconclusive—more investigation needed.

**Bottom line**: Just like checking a 1D function’s second derivative to see if it’s a valley(>0), or a hill(<0), in higher dimensions we check **all** directions. The Hessian’s eigenvalues reveal whether your “surface” is more like a **bowl**, a **dome**, or a **saddle**—and that tells you if you’ve found a minimum, a maximum, or neither!

---

Let’s get to the *why* behind the above explanation. It all boils down to how the **Hessian** captures the **curvature** of a function near a critical point, and how **eigenvalues** encode the “principal directions” of that curvature. Here’s a step‐by‐step way to see it:

### The Hessian in a Local Quadratic Approximation

Imagine you have a function $f(\mathbf{x})$ - with $\mathbf{x}\in \mathbb{R}^n$ - and you’re examining it near a critical point $\mathbf{x}^\ast$.  
A common technique is to use the **Taylor expansion** around $\mathbf{x}^\ast$. Up to second order terms, that expansion looks like this:  

$$
f(\mathbf{x}) 
\approx f(\mathbf{x}^\ast) + \underbrace{\nabla f(\mathbf{x}^\ast)}_{=\mathbf{0}}\cdot (\mathbf{x}-\mathbf{x}^\ast) + \tfrac{1}{2}(\mathbf{x}-\mathbf{x}^\ast)^\top H (\mathbf{x}-\mathbf{x}^\ast)
$$

where
- $\nabla f(\mathbf{x}^\ast)$ is the gradient at the critical point, which is $\mathbf{0}$ if $\mathbf{x}^\ast$ is indeed a stationary/critical point.
- $H = H(f)(\mathbf{x}^\ast)$ is the **Hessian matrix** at $\mathbf{x}^\ast$.

Therefore, **near** $\mathbf{x}^\ast$, the function behaves roughly like a **quadratic form**:  

$$
f(\mathbf{x}^\ast) + \tfrac{1}{2}(\mathbf{x}-\mathbf{x}^\ast)^\top H (\mathbf{x}-\mathbf{x}^\ast)
$$

If you can understand **that** quadratic shape, you’ll see whether $\mathbf{x}^\ast$ is a local min, local max, or something else.

### Eigenvalues = Curvature in Principal Directions

A real, symmetric matrix like the Hessian can be **diagonalized** via an orthogonal transformation:  

$$
H = Q \Lambda Q^\top
$$

where
- $Q$ is an orthogonal matrix whose columns are the **eigenvectors** (i.e., directions), and
- $\Lambda$ is a diagonal matrix of **eigenvalues** $\lambda_1,\dots,\lambda_n$

### What does that mean geometrically?

1. **Eigenvectors** give you the principal directions of curvature.  
2. **Eigenvalues** tell you whether the function curves **up** or **down** (and *how strongly*) in each of those directions.

So if you rewrite the quadratic form $(\mathbf{x}-\mathbf{x}^\ast)^\top H (\mathbf{x}-\mathbf{x}^\ast)$ in the eigenbasis, you end up with something like  

$$
\sum_{i=1}^n \lambda_i u_i^2
$$

where $u_i$ are coordinates along each eigenvector direction. This sum is the “shape” near $\mathbf{x}^\ast$

- If **all** $\lambda_i>0$, every term $\lambda_i u_i^2$ is nonnegative, so the sum can’t go below 0—this means $\mathbf{x}^\ast$ is a **minimum**.  
- If **all** $\lambda_i<0$, every term is nonpositive, so the sum can’t go above 0—this means $\mathbf{x}^\ast$ is a **maximum**.  
- If there’s a **mix** of positive and negative $\lambda_i$, the function curves up in some directions but down in others—a **saddle** shape.  
- If any eigenvalue is **zero**, the second‐order approximation in that direction doesn’t give you a clear up/down curve, so the test is **inconclusive**.

In short, **knowing the signs of the eigenvalues** tells you exactly how the quadratic “bowl” (the local shape of $f$) behaves around $\mathbf{x}^\ast$.

### Connecting to the 1D Case

In one dimension, the Hessian is just the second derivative $f''(x^\ast)$. That’s a single number—analogous to having just **one** eigenvalue. So:

- $f''(x^\ast)>0$ (a “positive” eigenvalue) means a local minimum (U‐shape).  
- $f''(x^\ast)<0$ (a “negative” eigenvalue) means a local maximum (n‐shape).  
- $f''(x^\ast)=0$ means we can’t tell from the second derivative alone (could be an inflection point or something more complex).

When you move to higher dimensions, you’re essentially looking at many directions at once. The **definiteness** of the Hessian matrix (whether all eigenvalues are positive, negative, or mixed) generalizes the idea of sign - $f''$ - from 1D to nD.

### Why This Works in Optimization

When we say “positive definite,” we mean for every direction $\mathbf{d}\neq\mathbf{0}$,  

$$
\mathbf{d}^\top H \mathbf{d} > 0
$$

That’s the hallmark of a local minimum: if you try to move in *any* direction away from $\mathbf{x}^\ast$, the function’s second‐order change is positive, so $f$ goes **up**. Similarly, “negative definite” means the second‐order change is always negative in every direction, so $f$ goes **down** away from $\mathbf{x}^\ast$, implying a local maximum.

Hence, the definiteness of the Hessian (all positive eigenvalues vs. all negative vs. mixed) is a direct measure of **how** the function is curving in every direction around the point $\mathbf{x}^\ast$.

### Final Intuition

- **Positive eigenvalues**: “Everywhere you step away from $\mathbf{x}^\ast$, the function is higher,” giving a valley/bowl shape → minimum.  
- **Negative eigenvalues**: “Everywhere you step away from $\mathbf{x}^\ast$, the function is lower,” giving a dome shape → maximum.  
- **Mixed sign eigenvalues**: “Some directions go up, others go down,” giving a saddle shape → neither.  

This is why the Hessian + eigenvalues approach is the natural extension of the single-variable second derivative test. It’s the most direct tool for describing local curvature in multiple dimensions.

---

## Newton's Method for Two Variables

> **Have you ever tried finding the deepest spot in a big yard by wandering around, testing different points, and then moving in a direction that you think leads downhill?** 

In one dimension (like walking on a straight line), Newton’s Method helps you quickly zero in on a minimum or maximum. In **two dimensions**—imagine you can walk in any direction on a 2D surface—the same idea applies, but now we use the **Hessian** (a 2×2 matrix of second partial derivatives) instead of a single second derivative.

Below, we’ll walk through how **Newton’s Method** generalizes from one variable to two, show you the essential formula, and illustrate it with an example.

### From One Variable to Two (or More)

In **one dimension**, the Newton update rule is:  

$$
x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
$$

And you can think of:
- $f'(x_k)$ as the “slope” at your current point
- $f''(x_k)$ as the “curvature” telling you how quickly the slope is changing

When you move to **two variables** $x$ and $y$:
- The “slope” idea becomes the **gradient** $\nabla f(x,y)$, a vector with two components $(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})$.
- The “curvature” idea becomes the **Hessian**, a $2\times 2$ matrix of second partial derivatives.

Newton’s update rule in two variables is:  

$$
\begin{pmatrix}
x_{k+1} \\
y_{k+1}
\end{pmatrix} = \begin{pmatrix}
x_k \\
y_k
\end{pmatrix} - [H(f)(x_k,y_k)]^{-1} \nabla f(x_k,y_k)
$$

where:
- $\nabla f(x_k,y_k)$ is the 2×1 gradient vector at $(x_k,y_k)$  
- $H(f)(x_k,y_k)$ is the 2×2 Hessian matrix at $(x_k,y_k)$  
- We must **invert** this Hessian matrix, just like we divided by $f''(x_k)$ in 1D

### Why the Order Matters

Notice in the formula:  

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H^{-1}(x_k,y_k)\nabla f(x_k,y_k)
$$

The matrix $[H^{-1}]$ is $2\times 2$, and the gradient is a $2\times 1$ column vector, so we multiply them as **matrix × vector**. We cannot reverse that multiplication because the dimensions wouldn’t line up. (In short, matrix multiplication is not commutative.)

### An Example Step-by-Step

Let's see how it works on a sample function:  

$$
f(x,y) = x^4 + 0.8y^4 + 4x^2 + 2y^2 - xy - 0.2x^2y
$$

1. **Compute the Gradient** $\nabla f(x,y)$  
- This gives us two partial derivatives:  
   
$$
f_x(x,y) = \frac{\partial f}{\partial x}, 
\quad
f_y(x,y) = \frac{\partial f}{\partial y}
$$

2. **Compute the Hessian** $H(f)(x,y)$  
- This is the $2\times 2$ matrix of second partials:  

$$
\begin{pmatrix}
f_{xx}(x,y) & f_{xy}(x,y) \\
f_{yx}(x,y) & f_{yy}(x,y)
\end{pmatrix}
$$

3. **Choose a starting point** $(x_0, y_0)$. Suppose we pick $(4,4)$.  
4. **Evaluate** the gradient and Hessian at $(4,4)$, then apply the update rule:  

$$
\begin{pmatrix}
x_1 \\
y_1
\end{pmatrix} = \begin{pmatrix}
4 \\
4
\end{pmatrix} - [H(4,4)]^{-1} \nabla f(4,4)
$$

5. **Iterate** (repeat!)  
- Plug in $(x_1, y_1)$ to get new gradient and Hessian, then compute $(x_2, y_2)$ the same way.  
- Continue until the change is very small, meaning you’ve (hopefully) reached a minimum.

### Convergence in a Few Steps

In the example, starting at $(4,4)$, just a few iterations can bring us near a point where $\nabla f(x,y) \approx \mathbf{0}$. This zero-gradient location is often a **local minimum**, provided the Hessian is positive definite there (both eigenvalues > 0).

**Analogy**:  
Think of each iteration like adjusting a **2D joystick** that controls your position on a curved landscape. The gradient says, “You’re sloping downward in this direction,” and the Hessian helps you figure out **how steep** that slope is and adjusts your step size accordingly. Because you see “both the slope and curvature,” Newton’s Method usually gets you to the bottom a lot faster than if you only knew the slope (as in regular gradient descent).

### Key Points

1. **Generalizing from 1D**: Instead of dividing by $f''(x_k)$, we multiply by the inverse of the Hessian matrix.  
2. **Gradient $\nabla f$**: Tells you the direction of steepest ascent (or descent if you negate it).  
3. **Hessian $H$**: Tells you how the surface curves in every direction—key for choosing the best step size and direction simultaneously.  
4. **Caution**: Newton’s Method can fail if the Hessian is not invertible, or if you’re not near a good local optimum. But when it works, it’s **very** fast.

**Bottom line**: Newton’s Method in two variables is like **learning to navigate hills and valleys with both eyes open**—the gradient shows you where to go, and the Hessian helps you gauge the terrain’s curvature. By repeatedly applying  

$$
(x_{k+1},\,y_{k+1}) = (x_k,\,y_k) - [H(f)(x_k,y_k)]^{-1} \nabla f(x_k,y_k)
$$

you can quickly **zoom in** on a local minimum of a 2D function.