# Probability and Statistics for ML and DS(6)_Point Estimation

## Maximum Likelihood Estimation: Finding the Most Probable Explanation

> *Have you ever walked into a room and immediately tried to figure out what happened there? Maybe you noticed popcorn scattered on the floor, or a half-empty glass of water, and your brain automatically started creating possible scenarios. **What's fascinating is that your brain is naturally performing what statisticians call "Maximum Likelihood Estimation"** - one of the most powerful and intuitive concepts in statistics and machine learning!*

### The Detective's Mindset: What Is Maximum Likelihood Estimation?

Imagine yourself as a detective who observes *evidence* and needs to determine the most likely *scenario* that produced it. This detective-like thinking is the essence of Maximum Likelihood Estimation (MLE).

In our object-oriented framework, we can think of MLE as follows:

```
class MaximumLikelihoodEstimation {
    properties:
        - evidence (observed data)
        - possibleScenarios (candidate models/parameters)
    
    method findMostLikelyScenario() {
        return the scenario that maximizes P(evidence | scenario)
    }
}
```

MLE works by answering one simple question: "Which possible scenario would most likely produce the evidence we've observed?"

### A Real-World Example: The Mystery of the Popcorn

Let's walk through a simple example to understand this concept better:

You enter a living room and see popcorn scattered on the floor near the couch. What happened here? Let's consider three possible scenarios:

1. People were watching a movie
2. People were playing board games
3. Someone was taking a nap

As a clever detective, you want to find which scenario most likely produced the evidence (popcorn on the floor). In other words, you want to maximize:  

$$
P(\text{popcorn on floor} | \text{scenario})
$$

For each scenario, you estimate this probability:
- $P(\text{popcorn} | \text{movie watching})$ = High (Movies and popcorn go together!)
- $P(\text{popcorn} | \text{board games})$ = Medium (Snacks during games are common, but popcorn is messy for handling game pieces)
- $P(\text{popcorn} | \text{nap})$ = Low (People rarely eat popcorn while sleeping)

The "movie watching" scenario gives the highest probability of producing the observed popcorn evidence, so according to MLE, it's your best guess for what happened!

### MLE in Machine Learning: Models as Explanation Objects

In machine learning, we apply this same detective logic but with data as our evidence and models as our possible explanations.

Here's how it works:
1. We have a dataset (our evidence)
2. We have multiple possible models that might explain the data (our scenarios)
3. We want to find which model most likely produced our data

Mathematically, we're finding:  

$$
\text{Best Model} = \arg\max_{\text{model}} P(\text{data} | \text{model})
$$

In our object-oriented framework:

```
class MachineLearningModel {
    properties:
        - parameters (the specific configuration of this model)
    
    method calculateLikelihood(data) {
        return P(data | this.parameters)
    }
}

class MLETrainer {
    method findBestModel(possibleModels, trainingData) {
        bestLikelihood = -âˆž
        bestModel = null
        
        for each model in possibleModels:
            likelihood = model.calculateLikelihood(trainingData)
            if likelihood > bestLikelihood:
                bestLikelihood = likelihood
                bestModel = model
                
        return bestModel
    }
}
```

### The Linear Regression Connection

When we perform linear regression, we're actually doing MLE behind the scenes! Each possible line is a different "scenario" that might explain our data points.

Imagine we have three candidate regression lines:
- Model 1: $y = 2x + 1$
- Model 2: $y = 3x - 2$
- Model 3: $y = 0.5x + 3$

For each model (line), we calculate how likely it is that our observed data points would be generated by that line. The model that makes our data most likely to occur is the one we choose.

In regression, we typically assume that data points are generated from the "true line" plus some random noise. The line that makes our observed data most probable is our **maximum likelihood estimate**!

### Beyond Basic MLE: A Peek at MAP Estimation

MLE gives us the scenario that makes our evidence most likely. But sometimes we have prior beliefs about which scenarios are more probable before seeing any evidence.

Maximum A Posteriori (MAP) estimation integrates these prior beliefs with the evidence. It's like an experienced detective who knows certain scenarios are inherently more common than others before examining the evidence.

Mathematically, while MLE maximizes $P(\text{data} | \text{model})$, MAP maximizes $P(\text{model} | \text{data})$.

Using Bayes' theorem:  

$$
P(\text{model} | \text{data}) \propto P(\text{data} | \text{model}) \times P(\text{model})
$$

The fascinating insight is that MAP estimation can be viewed as MLE with regularization - a technique used in machine learning to prevent overfitting. This connection shows how incorporating prior beliefs can lead to more robust models!

### Summary

Maximum Likelihood Estimation is like being a detective who finds the most likely explanation for observed evidence. When given multiple possible explanations (models), choose the one that would most likely produce your evidence (data). In simple terms: "Pick the cause that would most naturally create the effect you've observed."

---

## Maximum Likelihood Estimation: Bernoulli Example

> *Have you ever wondered how we determine the true probability of a coin landing heads? Is it really 50%, or could it be biased? Let's discover how Maximum Likelihood Estimation helps us answer this question using real data!*

### The Mystery of the Three Coins

Imagine you've tossed a coin 10 times and observed 8 heads (H) and 2 tails (T):

$$
H, H, H, H, H, H, H, H, T, T
$$

You have three possible coins that could have produced this result:
- Coin 1: Probability of heads = 0.7
- Coin 2: Fair coin with probability of heads = 0.5
- Coin 3: Probability of heads = 0.3

Which coin most likely produced our observed data? This is where our MLE detective skills come into play!

In our object-oriented framework, we can model these coins as instances of a `BernoulliGenerator` class:

```
class BernoulliGenerator {
    properties:
        - p (probability of success/heads)
    
    method calculateLikelihood(outcomes) {
        // Calculate P(observed data | this coin)
        return p^(number of heads) * (1-p)^(number of tails)
    }
}
```

### Calculating the Likelihood for Each Coin

Let's calculate the probability (likelihood) of observing 8 heads and 2 tails for each coin:

For Coin 1 (p = 0.7):  

$$
L(0.7; 8H, 2T) = 0.7^8 \times 0.3^2 = 0.0051
$$

For Coin 2 (p = 0.5):  

$$
L(0.5; 8H, 2T) = 0.5^8 \times 0.5^2 = 0.5^{10} = 0.0010
$$

For Coin 3 (p = 0.3):  

$$
L(0.3; 8H, 2T) = 0.3^8 \times 0.7^2 = 0.00003
$$

Coin 1 gives the highest likelihood (0.0051), so following our MLE principle, we would select Coin 1 as the most likely one used for the tosses.

### Finding the Optimal Probability

But what if we could design our own custom coin with any probability $p$ of heads? What value of $p$ would maximize the likelihood of seeing exactly 8 heads and 2 tails?

We need to express this as a function of $p$ that we can maximize:  

$$
L(p; 8H, 2T) = p^8 \times (1-p)^2
$$

This likelihood function represents the probability of our observed data given a coin with probability p of heads. Now our goal is to find the value of p that maximizes this function.

### The Log-Likelihood Approach

Since we're dealing with products of small numbers (which can cause numerical underflow), we use a common technique in statistics: working with the logarithm of the likelihood function. Because logarithm is a strictly increasing function, the value of p that maximizes the original likelihood will also maximize the log-likelihood:  

$$
\ell(p; 8H, 2T) = \log(p^8(1-p)^2) = 8\log(p) + 2\log(1-p)
$$

To find the maximum, we take the derivative with respect to $p$ and set it equal to zero:  

$$
\frac{d}{dp}\ell(p; 8H, 2T) = \frac{8}{p} + \frac{2}{1-p}(-1) = \frac{8}{p} - \frac{2}{1-p} = 0
$$

because by the chain rule,  

$$
\frac{d}{dp}\log(1-p) = \frac{1}{1-p} \cdot \frac{d}{dp}[1-p]
$$  

$$
\frac{d}{dp}[1-p] = -1
$$

So,  

$$
2 \cdot (-\frac{1}{1-p}) = -\frac{2}{1-p}
$$

Solving for $p$:  

$$
\frac{8}{p} = \frac{2}{1-p}
$$  

$$
8(1-p) = 2p
$$  

$$
8 - 8p = 2p
$$  

$$
\hat{p} = \frac{8}{10} = 0.8
$$

This means a **coin with exactly 80% probability of heads**(=Scenario) would be most likely to produce our observed outcome(=Evidence). Notice how this matches perfectly with the proportion of heads in our observed data (8 out of 10)!

### The General Case: Bernoulli MLE

Let's extend this to the general case. Suppose we have $n$ independent coin tosses (Bernoulli trials), where each toss results in either a success (1) or failure (0):  

$$
X = (X_1, X_2, X_3, ..., X_n)
$$

Each $X_i$ is an **independent and identically distributed (i.i.d.)** Bernoulli random variable with parameter p:  

$$
X_i \stackrel{i.i.d.}{\sim} \text{Bernoulli}(p)
$$

The likelihood function is:  

$$
L(p; \mathbf{x}) = P_p(\mathbf{X} = \mathbf{x}) = \prod_{i=1}^n P_{X_i}(x_i) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}
$$

This elegant formula handles both heads and tails automatically:
- If $x_i = 1$ (heads), the contribution to the product is $p$
- If $x_i = 0$ (tails), the contribution is $(1-p)$

Let $\sum_{i=1}^n x_i$ represent the total number of heads, and $n - \sum_{i=1}^n x_i$ represent the total number of tails. Then our likelihood function can be rewritten as:  

$$
L(p; \mathbf{x}) = p^{\sum_{i=1}^n x_i} (1-p)^{n-\sum_{i=1}^n x_i}
$$

Taking the logarithm:  

$$
\ell(p; \mathbf{x}) = \log(p^{\sum_{i=1}^n x_i} (1-p)^{n-\sum_{i=1}^n x_i}) = \left(\sum_{i=1}^n x_i\right)\log(p) + \left(n-\sum_{i=1}^n x_i\right)\log(1-p)
$$

To find the maximum, we take the derivative with respect to p and set it equal to zero:  

$$
\frac{d}{dp}\ell(p; \mathbf{x}) = \frac{\sum_{i=1}^n x_i}{p} - \frac{n-\sum_{i=1}^n x_i}{1-p} = 0
$$

Solving for p:  

$$
\frac{\sum_{i=1}^n x_i}{p} = \frac{n-\sum_{i=1}^n x_i}{1-p}
$$  

$$
\sum_{i=1}^n x_i (1-p) = (n-\sum_{i=1}^n x_i)p
$$  

$$
\sum_{i=1}^n x_i - p\sum_{i=1}^n x_i = np - p\sum_{i=1}^n x_i
$$  

$$
\sum_{i=1}^n x_i = np
$$  

$$
\hat{p} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}
$$

This beautiful result shows that the maximum likelihood estimate for the Bernoulli parameter $p$ is simply the sample mean $\bar{x}$ - the proportion of successes in your observed data!

In our object-oriented framework, we can implement this as:

```
class BernoulliMLE {
    method estimateParameter(outcomes) {
        // The MLE is simply the proportion of successes
        return sum(outcomes) / length(outcomes)
    }
}
```

### Summary

For a Bernoulli process like coin flipping, the maximum likelihood estimate of the probability parameter is the proportion of successes in your observed data. This insight connects the theoretical math with our intuition - if you observe 80% heads, your best estimate of the coin's true probability is 0.8. The elegance of MLE is that it formalizes this intuitive result with precise mathematical reasoning.