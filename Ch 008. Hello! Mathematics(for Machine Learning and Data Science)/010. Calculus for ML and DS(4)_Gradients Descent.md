# ***Calculus for ML and DS (4) – Gradient Descent***

## Optimization with Gradient Descent in One Variable

> **Have you ever tried to find the lowest spot on a rolling hill simply by walking around and seeing if you’re going up or down?** 

That process of searching step-by-step until you (hopefully) reach the bottom is very similar to a powerful technique in Machine Learning called **gradient descent**, which you've learned in the previous chapter - **Ch006. Hello! Machine Learning_006. Supervised Learning(6)_Gradient Descent**. 

In this section, we will open the hood of the "Gradient Descent" technique and see how it works in the simpler setting of one variable before moving on to higher dimensions. Knowing what happens under the hood is a great way to understand the technique and its limitations.

### Why Gradient Descent?

In many optimization problems, you might already know that setting the derivative of a function to zero can help find a minimum (or maximum). For instance, to minimize a function $f(x)$, you often solve $f'(x) = 0$. However, as functions get more complicated—especially in higher dimensions—solving $f'(x) = 0$ can be very difficult or even impossible to do exactly.

That’s where **gradient descent** comes in. It’s an **iterative** procedure that helps find a minimum without requiring an exact algebraic solution. Instead, you move step-by-step, guided by the slope (the derivative) of the function.

### Stepping Through an Example

Consider the function again:  

$$
f(x) = e^x - \ln(x)
$$

1. Its derivative is:  

$$
f'(x) = e^x - \frac{1}{x}
$$

2. Solving $e^x - \frac{1}{x} = 0$ exactly is quite tricky. There is a special number (often related to the so-called Omega constant) that solves this, but the point is: **it’s not straightforward to solve by hand**.

Rather than getting stuck, we can **iterate** toward the minimum with gradient descent:

1. Choose a **starting point** $x_0$ - some value of $x$.
2. At each step, **update** your current position using:  

$$
x_{\text{new}} = x_{\text{old}} - \alpha f'(x_{\text{old}})
$$

where $\alpha$ is a small number called the **learning rate**.

#### Why Subtract the Derivative?

- If $f'(x_{\text{old}})$ is **positive**, it means the slope is uphill in the positive direction, so you should move **left** (subtract a positive quantity).
- If $f'(x_{\text{old}})$ is **negative**, it means the slope is downhill in the positive direction, so you should move **right** (subtract a negative quantity, which is effectively adding).

### Learning Rate: The "Goldilocks" Factor

> Goldilocks is a character in a children's story who finds three bowls of porridge. She first tastes the first bowl and finds it too hot. Then she tastes the second bowl and finds it too cold. Finally, she finds the third bowl just right. The term "Goldilocks" is used to describe the "just right" value in finance, engineering, data science, and more.

- $\alpha$ (the learning rate) controls **how big** your steps are. Do you remember the analogy of the hiking?
- If $\alpha$ is **too large**, you might jump right over the minimum and keep missing it—like too big steps on a hiking trail.
- If $\alpha$ is **too small**, it might take forever to get anywhere—like too small steps on a hiking trail.

Finding the **just right** value for $\alpha$ is often part of the art and science of using gradient descent in real-world problems.

### A Mini Walkthrough

1. **Pick a starting point**: say $x_0 = 0.05$.
2. **Calculate the derivative** at $x_0$: $f'(0.05)$.
3. **Update** the point:  

$$
x_1 = 0.05 - \alpha f'(0.05)
$$

4. Repeat the process, generating $x_2, x_3, \dots$ until your values stop changing much, indicating you’re close to a minimum.

At each iteration, you never have to solve $f'(x) = 0$ exactly. You only need to **plug in** $x_k$ into $f'(x)$ and do the update. This simplicity makes gradient descent incredibly powerful for large and complex problems.

### An Analogy: Hiking Down a Slope

Imagine you’re on a foggy mountain with a blindfold. You can sense whether the ground slopes up or down around your feet. Each step:
- If you feel the ground sloping *downhill* ahead, you move in that direction (subtract a negative slope).
- If you feel it sloping *uphill*, you step the other way (subtract a positive slope).

By taking careful steps (small learning rate), you won’t trip over the ridge. By sensing the slope each time (the derivative), you iteratively head toward the lowest valley—your function’s minimum.

### Potential Pitfalls: Local Minima

Not every valley is the **lowest** valley overall. Some functions have multiple dips (local minima). Gradient descent can get “stuck” in one of these smaller dips if you start there. A common strategy:
- **Try different starting points** and see if you can find a lower dip elsewhere.

### Key Takeaways

- **Gradient Descent** is an iterative method to find minima:  

$$
x_{\text{new}} = x_{\text{old}} - \alpha f'(x_{\text{old}})
$$

- The **learning rate** $\alpha$ determines step size.
- You don’t need to solve $f'(x) = 0$ analytically; you only need to compute $f'(x)$.
- Multiple runs with different starting points can help avoid getting stuck in local minima.

As you know, this method is a bedrock technique in Machine Learning and Data Science. Once you understand it for a single variable, you’ll be ready to tackle the multi-variable case. 

In essence, gradient descent is like methodically walking down a slope, guided by the steepness of the ground beneath your feet, until you reach a comfortable resting place at (or near) the bottom.

---

## Optimization with Gradient Descent in Two Variables

> **Imagine you’re standing in a large field where the ground is warmer in some places and cooler in others** 

Your mission is to walk around until you find the **coolest spot**. You can go **forward, backward, left, or right**, and after each move, you check the temperature to see if you’re getting closer to that perfect coolness. **Gradient descent** in two variables works exactly like that—except it uses the **mathematical “slope”** in both directions to guide you **downhill** to the minimum of a function.

### From Single-Variable to Multi-Variable

Previously, you learned how gradient descent works for a function of **one variable**, $f(x)$. You had a single derivative $f'(x)$ to tell you how the function was changing around your point $x$. If $f'(x)$ was **positive**, you shifted left; if it was **negative**, you shifted right—always aiming to move “downhill.”

But what if your function depends on **two variables**, say $x$ and $y$? Think about a **3D landscape** where $x$ and $y$ are your horizontal coordinates, and the function value $f(x,y)$ represents the “height” (or temperature, or any quantity you want to minimize).

1. Instead of one slope $f'(x)$, you get **two slopes**:  
   - $\frac{\partial f}{\partial x}(x,y)$ tells you how fast $f$ changes in the $x$-direction.  
   - $\frac{\partial f}{\partial y}(x,y)$ tells you how fast $f$ changes in the $y$-direction.

2. These partial derivatives form a **gradient vector**, $\nabla f(x, y)$, which is basically an arrow in the direction of **steepest ascent**.

### The Gradient in Two Variables

#### What Is a Gradient?

When dealing with $f(x,y)$, the **gradient** $\nabla f(x, y)$ is:  

$$
\nabla f(x, y) =
\begin{bmatrix}
\frac{\partial f}{\partial x}(x, y) \\
\frac{\partial f}{\partial y}(x, y)
\end{bmatrix}
$$

- Each component of this vector tells you how $f$ changes if you make a tiny move in just one direction $x$ or $y$.
- The gradient points **where the function goes up the fastest**. If you want to go **down** (minimize), you do the opposite: head in the **negative** gradient direction.

#### Visualizing the Gradient

Picture a **3D hill**: the $x$-axis, $y$-axis, and $z = f(x,y)$ going upwards.  
- If you stand at some point $(x,y)$ on this hill, the gradient vector is like a little arrow on the ground telling you how to climb “up” the slope.
- If you want to **descend**, you walk in the **opposite** direction of that arrow.

### Gradient Descent: The Recipe for Two Variables

#### Start Somewhere

Pick an initial point $(x_0, y_0)$. Just like in the 1D case, your first guess might be random or based on some prior knowledge.

#### Find the Gradient

Compute:

$$
\nabla f(x_k, y_k) =
\begin{bmatrix}
\frac{\partial f}{\partial x}(x_k, y_k) \\
\frac{\partial f}{\partial y}(x_k, y_k)
\end{bmatrix}.
$$

These partial derivatives tell you the local slope in each direction.

#### Update Step

To move “downhill,” you **subtract** a fraction of the gradient from your coordinates:  

$$
\begin{bmatrix}
x_{k+1} \\
y_{k+1}
\end{bmatrix} = \begin{bmatrix}
x_k \\
y_k
\end{bmatrix} - \alpha \begin{bmatrix}
\frac{\partial f}{\partial x}(x_k, y_k) \\
\frac{\partial f}{\partial y}(x_k, y_k)
\end{bmatrix}
$$

where $\alpha$ is the **learning rate**.

#### Repeat Until “Close Enough”

Keep recalculating the gradient at each new point and stepping downhill until your position barely changes—meaning you’re (hopefully) near a **minimum**.

### Why the Negative Gradient?

In one variable, a **positive** derivative means the function is sloping upward, so you move left. A **negative** derivative means the function slopes downward, so you move right.

In two variables, the gradient is an **arrow** pointing where $f$ increases the most. If you want to **decrease** $f$, you step in the **opposite** direction of that arrow. Mathematically, that’s why we do:  

$$
(x, y) - \alpha \nabla f(x, y)
$$

### Real-World Analogy: Temperature in a Room

Let’s say $f(x,y)$ represents the **temperature** at each point $(x,y)$ in a room. You want to find the **coolest spot**:

1. **Check the Slopes**: If you move slightly to the right - increase $x$, does the temperature go up or down? If you move slightly forward - increase $y$, does the temperature go up or down?
2. **Gradient Vector**: Tells you the direction in which the temperature increases the fastest (the hottest direction).
3. **Head Opposite**: Move toward the cooler area by going the exact **other way** from the gradient.
4. **Tiny Steps**: The learning rate $\alpha$ ensures you don’t overshoot or wander randomly. A moderate $\alpha$ helps you steadily zero in on the coldest region.

### Practical Example

#### A Two-Variable Function

A function might look like:  

$$
f(x, y) = 85 - \frac{1}{90} x^2 (x - 6) y^2 (y - 6)
$$

If this function represents temperature, a **lower** $f(x,y)$ means a **cooler** temperature.

#### Gradient Calculation

You’d derive the partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ (which might look complicated, but the idea is straightforward: each partial derivative tracks how $f$ changes with respect to one variable).

#### Iteration

1. Choose an initial point, say $(0.5, 0.6)$.  
2. Compute the gradient at that point.  
3. Update $(x, y)$ by subtracting $\alpha$ times the gradient.  
4. Repeat. Each iteration should bring you closer to a spot where $f(x,y)$ is minimized (coolest temperature).

### Learning Rate: The “Size” of Your Steps

Just like in 1D gradient descent, the **learning rate** $\alpha$:

- **Too Large**: You might overshoot and end up bouncing around without settling into a minimum.  
- **Too Small**: You inch along very slowly, taking forever to converge.  
- **Balanced**: You gradually close in on the minimum in a reasonable number of steps.

### Local vs. Global Minima: Multiple “Dips”

In some functions, there might be several “cold spots” (local minima), but only one **global** cold spot. Gradient descent can sometimes get stuck in one of these smaller dips. A common trick is to:

- **Try multiple starting points**. 
- Pick the lowest final value among all runs.  

This doesn’t guarantee you’ll find the absolute global minimum, but it helps a lot in practice.

### Summary of the Process

1. **Initialize**: Choose an initial guess $(x_0, y_0)$.
2. **Gradient**: At each iteration $k$, compute $\nabla f(x_k, y_k)$.
3. **Update Rule**:  

$$
x_{k+1} = x_k - \alpha \frac{\partial f}{\partial x}(x_k, y_k), \quad
y_{k+1} = y_k - \alpha \frac{\partial f}{\partial y}(x_k, y_k)
$$

4. **Repeat** until changes are minimal or you reach a set number of steps.
5. **Result**: A point x*, y* where $f$ is hopefully at or near its minimum.

### Key Takeaways

- **Gradient in 2D**: A vector combining partial derivatives that points where the function rises fastest.
- **Moving “Downhill”**: You subtract a scaled version (the learning rate times the gradient vector) from your current coordinates.
- **Iterative Nature**: Small, repeated steps guide you to a low point of $f(x,y)$.
- **Local Minima Issue**: The function might have multiple dips; starting in different places can help find a better solution.
- **Foundation of ML**: Gradient descent in many variables is the workhorse of training machine learning models (like neural networks), where you adjust **lots** of parameters at once.

With two variables learned, you have the concept needed for even **higher-dimensional** gradient descent, which is crucial in modern machine learning. The logic is the same—just more coordinates to track!

---

## Optimization with Gradient Descent - Least Squares

> **Imagine you’re trying to draw the **“best line”** through a set of points on a graph. Maybe these points represent houses (with their sizes on the x-axis and their prices on the y-axis)**

You suspect a straight line could describe their relationship. But how do you figure out exactly **which** line is best? 

In smaller cases, you might try solving equations or doing some clever algebra. But for **bigger** problems—or when you just want a more direct, iterative method—there’s a powerful technique, **gradient descent**. Let’s see how it works for **least squares**, one of the most common approaches to **linear regression**.

### Least Squares Recap: What Are We Minimizing?

#### The Linear Model

We have a line:  

$$
y = mx + b
$$

where 
- **$m$** is the slope (how steep the line is),
- **$b$** is the intercept (where the line crosses the y-axis).

#### The Cost Function

To measure how good or bad our line is, we often use the **sum of squared errors** (SSE). Suppose we have $n$ data points $(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)$. The **error** for each point is the vertical distance between the data’s $y_i$ and the line’s prediction $m x_i + b$. We then **square** this error and **sum** (or average) over all points. A common form is:  

$$
L(m, b) 
= \frac{1}{2n} \sum_{i=1}^{n} (m\,x_i + b \;-\; y_i)^2
$$

We include $\tfrac{1}{2n}$ mainly for convenience—when we take derivatives, it cancels out nicely. Either way, **minimizing** $L(m,b)$ finds the best-fit line.

### Why Gradient Descent?

#### Analytical vs. Iterative Solutions

For a small dataset or a simple case, you might solve $\frac{\partial L}{\partial m} = 0$ and $\frac{\partial L}{\partial b} = 0$ **directly** (the so-called “normal equations”). However, that can get messy, or simply **impossible** in higher dimensions or with more complex cost functions.

**Gradient descent** offers a neat, iterative approach:
1. Start with a random guess $(m_0, b_0)$.  
2. Calculate how to adjust $m$ and $b$ to **reduce** the cost.  
3. Update your guesses in small steps, guided by the partial derivatives (the “gradient”).  
4. Repeat until you settle on a pair $(m, b)$ that **minimizes** the cost.

### Setting Up Gradient Descent for Linear Regression

#### The Gradient of $L(m, b)$

To use gradient descent, we need the **partial derivatives** of $L(m, b)$ with respect to $m$ and $b$. From the cost function above:  

$$
\frac{\partial L}{\partial m}(m,b) = \frac{1}{n} \sum_{i=1}^{n} (m x_i + b - y_i) x_i
$$  

$$
\frac{\partial L}{\partial b}(m,b) = \frac{1}{n} \sum_{i=1}^{n} (m x_i + b - y_i)
$$

These two expressions form the **gradient**:  

$$
\nabla L(m,b) = \begin{bmatrix} \frac{\partial L}{\partial m}(m,b) \\
\frac{\partial L}{\partial b}(m,b) \end{bmatrix}
$$

#### Update Rule

At each iteration $k$, we adjust $m$ and $b$:  

$$
\begin{bmatrix}
m_{k+1} \\
b_{k+1}
\end{bmatrix} = \begin{bmatrix}
m_k \\
b_k
\end{bmatrix} - \alpha \begin{bmatrix}
\frac{\partial L}{\partial m}(m_k,b_k) \\
\frac{\partial L}{\partial b}(m_k,b_k)
\end{bmatrix}
$$

where **$\alpha$** is the **learning rate**—the size of the step we take each time.

### An Illustrative Example

#### Power Lines or House Prices

Whether you’re trying to minimize the **Wi-Fi Router cost** from some earlier example or you have data like:
- (1,2), (2,5), (3,3)  
for simple regression, your goal is the same: find $(m,b)$ to minimize the sum of squared errors. 

**Analytical** solutions might be feasible for small examples, but gradient descent generalizes well, especially if you have a larger dataset or if the cost function is more complicated than a standard parabola.

#### Visualization
- Picture the left graph with data points and various lines: $y = mx + b$.  
- On the right, imagine a 3D “bowl”-shaped surface, where the horizontal axes are $m$ and $b$, and the vertical axis is the **cost** $L(m,b)$.  
- Gradient descent is like **rolling a ball** down that bowl. Each step “down” is found by looking at the partial derivatives and updating $(m,b)$ in the negative gradient direction.

### Iteration Step by Step

1. **Initialize**: Choose some $(m_0, b_0)$. This might be $(0,0)$ or small random values.  
2. **Compute Gradient**: At $(m_k,b_k)$, find $\frac{\partial L}{\partial m}$ and $\frac{\partial L}{\partial b}$.  
3. **Update**:  

$$
m_{k+1} = m_k - \alpha \frac{\partial L}{\partial m}(m_k,b_k) \\
b_{k+1} = b_k - \alpha \frac{\partial L}{\partial b}(m_k,b_k)
$$

4. **Repeat**: Keep iterating until the changes become very small or you hit a maximum iteration count.  
5. **Solution**: You end up at m*, b* that **minimizes** $L(m,b)$—your best-fit line.

### Learning Rate: Taking the Right Size Steps

- **Too Large** $\alpha$ big: You might “leap” over the minimum and bounce around.  
- **Too Small** $\alpha$ tiny: You inch toward the bottom, taking forever to converge.  
- **Just Right**: A moderate $\alpha$ lets you converge steadily without overshooting.

In linear regression with a squared-error cost (which is **convex**), you **can’t** get stuck in weird local minima. Eventually, if your steps are sensibly sized, you’ll converge to the **global minimum**.

### Multiple Observations and Summation

When you have **many** data points $(x_i, y_i)$:
1. The cost $L(m,b)$ sums (or averages) the squared errors across **all** points.  
2. Its gradient is simply the sum of partial derivatives for each point.  
3. Gradient descent then updates $(m,b)$ based on the average of those errors.  

That’s why it works for **large datasets**: you just keep iterating the same formula, and you’ll find a line that best fits **all** the points in a least-squares sense.

### Why It Matters
- **Scalability**: Gradient descent works even if you have **millions** of data points or many parameters like $m$, $b$, plus more features.
- **Simplicity**: Each step is straightforward: just compute the gradient and update.  
- **Foundation of ML**: Most modern machine learning models—such as neural networks—rely on gradient descent (or variations) under the hood.

### Summary & Key Takeaways
1. **Least Squares Cost**: Measures how “off” your line is, by squaring and summing the vertical errors.  
2. **Gradient Descent**: An iterative method to **minimize** this cost:
   - Start with a guess $(m_0, b_0)$.  
   - Compute partial derivatives (the gradient).  
   - Update $(m, b)$ by subtracting a fraction $\alpha$ of the gradient.  
   - Repeat until convergence.  
3. **Learning Rate**: Controls step size—balancing speed and stability.  
4. **No Local Minima** in Basic Linear Regression: The “bowl” shape means a single global minimum.  
5. **Scales to Larger Problems**: This same approach generalizes to more data points and more parameters.

With gradient descent, you’ve got a practical, scalable way to solve linear regression (and beyond) without cumbersome algebra. By understanding how to “walk downhill” in the cost function, you can confidently handle a wide range of optimization tasks in machine learning.
