# ***Calculus for ML and DS(3)_Gradients***

## Introduction to Optimization in Multiple Variables

> **Have you ever tried to find the steepest path uphill when hiking on a mountain trail?** 

In one direction, the slope might feel gentle, while in another, it’s suddenly very steep. This real-world experience hints at the concept we call the “tangent plane” in two-variable calculus—a direct extension of the “tangent line” idea from one-variable calculus.

### From One Dimension to Two

In **Optimization chapter**, we focused on functions of one variable, like  

$$
f(x) = x^2
$$  

If you sketch $y = x^2$, you get a parabola. At any point $x$, the *derivative* tells you the **slope** of the tangent line at that spot.

But many real-world problems involve **two inputs** - say, $x$ and $y$ - and produce **one output** (like a height or cost). A simple example is  

$$
f(x,y) = x^2 + y^2
$$  

Now, $f$ must be graphed in **three-dimensional space**: 
- The horizontal plane for $x$ and $y$.  
- The vertical axis (often labeled $z$) for $f(x,y)$.  

### The Tangent Plane

When you plot $z = x^2 + y^2$, it forms a curved, bowl-shaped surface. Instead of a single tangent *line*, we get a tangent *plane* at each point on the surface. 

- **Analogy**: Picture setting a flat sheet of paper so that it just “kisses” the surface at one specific point. That sheet is the **tangent plane**.  
- In one dimension, you needed just a *slope* to describe how steep the line is.  
- In two dimensions, you need *partial derivatives* in the $x$-direction and in the $y$-direction to describe the orientation of that plane.

### Slicing the Surface

To understand the tangent plane, we can slice the 3D surface in two perpendicular ways:

1. **Fix** $y=4$: This gives a 2D “slice” $f(x,4) = x^2 + 4^2$. That slice is just a parabola in terms of $x$.  
   - The derivative of $f(x,4)$ with respect to $x$ is $\frac{d}{dx}(x^2 + 16) = 2x$.  
   - At a specific $x$-value, say $x=2$, the slope is $2\times 2 = 4$. That slope is one direction of the tangent plane.

2. **Fix** $x=2$: Now we get $f(2,y) = 2^2 + y^2$, again a parabola—this time in $y$.  
   - The derivative with respect to $y$ is $\frac{d}{dy}(4 + y^2) = 2y$.  
   - At, say, $y=4$, the slope is $2\times 4 = 8$. That’s the slope in the *other* direction of the plane.

By combining these two slopes (one in the $x$-direction and one in the $y$-direction), we fully define how the tangent plane sits at the point $(x,y) = (2,4)$.

### Why Is This Important for Optimization?

- In machine learning or data science, we often want to **optimize** (minimize or maximize) a function $f(x,y)$ that depends on multiple variables.  
- Just like a slope of zero in one dimension points to a potential minimum or maximum, a **flat** tangent plane in multiple dimensions signals a similar “optimal” point.  
- We’ll introduce **gradient descent** soon: a method that systematically moves “downhill” in multiple variables by following the direction of steepest descent—something we can find using *both* partial derivatives.

### Key Takeaways

1. **Moving from Lines to Planes**: A “tangent line” in one variable extends to a “tangent plane” (or “tangent hyperplane” in even higher dimensions).  
2. **Slices Simplify the Math**: Fix one variable at a time to see “parabolas” that reveal partial derivatives.  
3. **Partial Derivatives**: They measure how the function changes if you move *just* in the $x$ direction or *just* in the $y$ direction. Together, they help pin down the tangent plane.  
4. **Foundation for Gradient Descent**: Understanding how these partial derivatives come together is crucial for optimization techniques widely used in machine learning.

**In short:** If you can grasp how tangent lines generalize to tangent planes, you’re well on your way to handling complex optimization problems involving multiple inputs—an essential tool for modern AI and data science!

---

## Partial Derivatives

> **Have you ever watched a TV cooking show where each judge samples one part of a dish at a time—maybe first the sauce, then the crust—before giving their overall verdict?** 

That’s a bit like how *partial derivatives* work when a function depends on multiple variables. Instead of tasting everything at once, partial derivatives let you “fix” some ingredients (variables) as constants, and see how the function changes when you vary just one.

### The Idea Behind Slicing

When you have a function of two variables, say  

$$
f(x,y) = x^2 + y^2
$$  

you can imagine it as a *3D surface* (like a “bowl”) in $(x,y,z)$-space, where $z = f(x,y)$.  

1. **Fix y as a constant**: You slice the surface with a vertical plane.  
   - That slice is effectively a *parabola* in $x$ alone.  
   - You can find the **slope** of its tangent line just as in single-variable calculus.  
2. **Fix x as a constant**: You slice the surface with a different vertical plane.  
   - You get another parabola—this time in terms of $y$.  
   - Its tangent line’s slope corresponds to the derivative with respect to $y$.

These slopes in each direction are precisely the **partial derivatives**.

### How to Compute a Partial Derivative

Let’s say we want the partial derivative of $f(x,y)$ with respect to $x$. We denote this as  

$$
\frac{\partial f}{\partial x} \quad \text{or} \quad f_x
$$

* $\partial$ is called **di**

1. **Treat all other variables as constants**. Here, you pretend $y$ is just a fixed number.  
2. **Differentiate normally** with respect to $x$.

#### Example 1: $f(x,y) = x^2 + y^2$

- **Partial w.r.t.** $x$: $y^2$ is treated like a constant;  

$$
\frac{\partial}{\partial x}(x^2 + y^2) = 2x + 0 = 2x
$$  

- **Partial w.r.t.** $y$: Now $x^2$ is the constant;  

$$
\frac{\partial}{\partial y}(x^2 + y^2) = 0 + 2y = 2y
$$

Notice how each partial derivative is exactly like a single-variable derivative for that slice.

### Another Example: $f(x,y) = 3x^2y^3$

Consider  

$$
f(x,y) = 3x^2 y^3
$$

1. $\partial f/\partial x$:  
   - Treat $y^3$ as a constant “box.”  
   - The 3 is also a constant factor.  
   - So, this is like a function $f(x)=a \times x^2 \times b$ - (a, b are constants)
   - Derivative of $x^2$ w.r.t. $x$ is $2x$.  
   - So you get $3 \times 2x \times y^3 = 6x y^3$  

2. $\partial f/\partial y$:  
   - Treat $x^2$ as a constant “box.”  
   - Again, the 3 is a constant factor.  
   - So, this is like a function $f(y)=a \times b \times y^3$ - (a, b are constants)
   - Derivative of $y^3$ w.r.t. $y$ is $3y^2$.  
   - So you get $3 \times x^2 \times 3y^2 = 9x^2y^2$

### Why Partial Derivatives Matter

- **Multidimensional Slopes**: In a function with multiple inputs, the *gradient* is just a list (or vector) of all partial derivatives. It tells you how sensitive the function is to each variable.  
- **Tangent Planes**: For 2-variable functions, partial derivatives help define a *plane* tangent to the surface, not just a line.  
- **Machine Learning**: Most optimization algorithms (like **gradient descent**) rely on partial derivatives to figure out how to update parameters to reduce loss.

### Going Beyond Two Variables

For a function of $n$ variables,  

$$
f(x_1, x_2, \dots, x_n)
$$

you can compute $n$ partial derivatives—one for each variable. This leads us into the idea of **gradients**, which we’ll explore next. 

**In short:** Partial derivatives let you “peek” at how a function changes when you vary **one** variable at a time, holding others fixed. It’s the cornerstone of higher-dimensional calculus and the basis for advanced optimization methods used throughout data science and machine learning.

---

## Gradient

> **Have you ever tried to find the steepest path to descend when standing on a mountain ridge?** 

You look around and realize there’s not just *one* slope but multiple directions to explore. That’s where the idea of the *gradient* comes in—a single mathematical tool that wraps up all the “slopes” in different directions into one tidy package.

### From Partial Derivatives to a Gradient Vector

When a function depends on two variables, like  

$$
f(x, y)
$$

we can measure how $f$ changes if we nudge $x$ a bit (while holding $y$ fixed) or if we nudge $y$ a bit (while holding $x$ fixed). These **partial derivatives** are:  

$$
\tfrac{\partial f}{\partial x}, \quad \tfrac{\partial f}{\partial y}
$$

Instead of treating them separately, we combine them into a **vector** called the **gradient**, denoted by  

$$
\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)
$$

* $\nabla$ is called **nabla**

- **Analogy**: Think of each component as a clue about how quickly the function height changes if you step in the $x$-direction or the $y$-direction. Put them together, and you get a **direction** of the fastest increase in $f$.

#### Example

For  

$$
f(x,y) = x^2 + y^2
$$

its partial derivatives are:  

$$
\tfrac{\partial f}{\partial x} = 2x, \quad \tfrac{\partial f}{\partial y} = 2y
$$

So the gradient is  

$$
\nabla f = (2x,\,2y)
$$

If you pick a specific point, say $(x,y) = (2,3)$, then  

$$
\nabla f(2,3) = (4, 6)
$$

### Geometric Meaning

1. **Slope in Every Direction**: In single-variable calculus, you have one slope at each point. In two variables, you have *infinitely many* slopes—one for every direction you move. 
2. **Gradient Points in Direction of Steepest Change**: The gradient vector at a point shows the direction of steepest change in $f$, and its size (magnitude) tells you how steep that change is.
3. **Link to Tangent Planes**: The gradient collects the partial derivatives that define the plane tangent to the surface $z = f(x,y)$. If you know how steep the surface is in $x$ and $y$ directions, you can figure out the orientation of the tangent plane.

### Using Gradients to Find Minima and Maxima

In single-variable calculus, you find a function’s minimum (or maximum) by **setting the derivative to 0**. For a two-variable function $f(x,y)$, you look for the point(s) where **both** partial derivatives are 0 simultaneously:  

$$
\frac{\partial f}{\partial x} = 0
\quad\text{and}\quad
\frac{\partial f}{\partial y} = 0
$$

- **Example**: For $f(x,y) = x^2 + y^2$,  

$$
\frac{\partial f}{\partial x} = 2x = 0 \quad \Longrightarrow \quad x=0
$$  

$$
\frac{\partial f}{\partial y} = 2y = 0 \quad \Longrightarrow \quad y=0
$$

  So the only candidate for a minimum is $(0,0)$—and indeed it’s the bottom of the bowl.

- **General Case**: If $f$ depends on many variables $(x_1,\dots,x_n)$, you set **all** partial derivatives to zero:  

$$
\nabla f = \mathbf{0}
$$

Solving this system locates potential minima or maxima.

### Why Gradients Matter in AI and Data Science

- **Gradient Descent**: A cornerstone of machine learning, gradient descent uses the gradient to adjust parameters in the direction that *reduces* the error or loss function.  
- **Scalability**: No matter how many variables (model parameters) you have—2, 17, or even millions—the gradient concept stays the same: gather all partial derivatives into **one vector**, then follow them to optimize.  
- **Efficient Computation**: Modern frameworks can compute these partial derivatives automatically (autograd), so you don’t need to do the algebra by hand.

### Key Takeaways

1. **Gradient = Vector of Partial Derivatives**: It’s your all-in-one tool to see how a function changes in any direction.  
2. **Finding Min/Max**: Setting the gradient to zero locates potential extrema, just like setting a single derivative to zero in 1D.  
3. **Essential in ML**: Training a neural network? You’ll rely heavily on gradients for backpropagation and parameter updates.

**In short:** The gradient is like a compass telling you which way is “steepest downhill” on a surface. In machine learning, that information is exactly what you need to travel “downhill” (i.e., reduce error) and hone in on the best parameters for your model.

---

## Optimization with Gradients

> **Have you ever tried to find the coolest spot in a room on a scorching summer day? Or perhaps you’ve wondered how to draw the “best-fitting” line through a collection of points on a graph?**

These questions boil down to a common idea in machine learning and data science: **optimization**. In this section, we’ll explore how **gradients** (partial derivatives) help us navigate mathematical “landscapes” to find the points or lines that minimize (or maximize) some quantity, be it temperature or cost.

### Optimization in Two Dimensions

#### A Room Example

Imagine a large room where each spot on the floor has its own temperature. You can walk around in two directions: the $x$-direction (left to right) and the $y$-direction (front to back). Let’s call the temperature function:  

$$
T(x, y) = 85 - \frac{1}{90} x^2 (x - 6) y^2 (y - 6)
$$

Think of $T(x, y)$ as a “landscape” of heights, where taller peaks represent hotter spots and deeper valleys represent cooler spots.

#### Why Gradients Help

If you’re standing somewhere in that 2D room, you want to move toward a cooler spot. One way is to guess a few directions and see which step makes things colder. But in math and machine learning, we do something more elegant:

- We compute **partial derivatives** $\frac{\partial T}{\partial x}$ and $\frac{\partial T}{\partial y}$, which act like a **compass** telling us how steeply temperature changes as we move in each direction.
- Where both partial derivatives are zero simultaneously, the “slope” in the $x$ and $y$ directions is zero—suggesting a peak, valley, or plateau.

In more concrete terms:  

$$
\frac{\partial T}{\partial x} = -\frac{1}{90} x(3x - 12) y^2 (y - 6)
$$  

$$
\frac{\partial T}{\partial y} = -\frac{1}{90} x^2 (x - 6) y (3y - 12)
$$

We set both to zero to find any **critical points**. One of those points will turn out to be the coolest spot in the room!

#### Hunting for the Coldest Spot

By examining the factors that multiply out in $\frac{\partial T}{\partial x}$ and $\frac{\partial T}{\partial y}$, we can see potential solutions like $x=0$, $x=4$, $y=0$, $y=4$, etc. Once we compare the temperature at each candidate point:

- We discover that the **minimum** occurs at around $(x, y) = (4, 4)$, yielding a temperature of roughly $73.6^\circ\!C$ (which is the coolest area in this scenario).
- Other points might be “hotter” or even outside the room. By evaluating $T(x,y)$ at each candidate, we confirm which is truly the minimum.

This simple process—**set derivatives to zero, solve for variables, then compare**—applies to many machine learning problems where we need to find an optimal location or parameter set.

### Linear Regression and Cost Functions

#### A Story About Placing a Wi-Fi Router

Now let's jump to a second example: imagine you have a home with multiple rooms at different coordinates on a floor plan. You can run a “main signal line”—think of it like a thin corridor of wireless coverage described by an equation—but each room still needs a small vertical cable to tap into that line. 

Why a vertical cable? You might picture that the main router’s coverage extends along some straight path - like $y = mx + b$ in the plane - and each room connects via a straight cable dropped from that line. The “cost” of running these cables depends on how long each one is. In this metaphor, **longer cables mean more signal loss or expense**, so you want to minimize the total cable length. 

To keep the math neat, let’s say each “cable length” is measured by its vertical distance to the router’s line. And to discourage any one cable from becoming too large, we’ll work with **squared** cable lengths—so the total cost is the sum of the squares of those vertical distances. This setup parallels a core idea in machine learning called **linear regression**.

#### The Cost Function

Suppose you have three rooms, each with coordinates $(x_i, y_i)$. For instance:

- Room A is at $(1,2)$ (maybe a living room).
- Room B is at $(2,5)$ (could be a bedroom).
- Room C is at $(3,3)$ (an office, perhaps).

Our “router line” is given by the equation:  

$$
y = mx + b
$$

where $m$ is the slope (how steeply the coverage corridor tilts) and $b$ is the intercept (where it crosses the $y$-axis). The vertical distance from any room $(x_i, y_i)$ to this line is simply $(m x_i + b) - y_i$. 

If we square that distance to measure cost, the total cost $E(m,b)$ for all three rooms is:  

$$
E(m,b) = \sum_{i=1}^{3} \bigl(m x_i + b - y_i\bigr)^2
$$

Plugging in the specific room coordinates:

- Room A’s squared distance is $(m \cdot 1 + b - 2)^2$
- Room B’s squared distance is $(m \cdot 2 + b - 5)^2$
- Room C’s squared distance is $(m \cdot 3 + b - 3)^2$

So,  

$$
E(m,b) = (m + b - 2)^2 + (2m + b - 5)^2 + (3m + b - 3)^2
$$

When you expand and combine like terms, you get a polynomial:  

$$
E(m,b) = 14m^2 + 3b^2 + 38 + 12mb - 42m - 20b
$$

We want to find the values of $m$ and $b$ that **minimize** this total squared distance.

#### Using Partial Derivatives

To minimize $E(m,b)$, we look at **where its slope is zero** in both directions $m$ and $b$. Concretely, that means we set the partial derivatives equal to zero:  

$$
\frac{\partial E}{\partial m} = 0 
\quad\text{and}\quad 
\frac{\partial E}{\partial b} = 0
$$

Let’s calculate each one:  

$$
\frac{\partial E}{\partial m} = 28m + 12b - 42
$$  

$$
\frac{\partial E}{\partial b} = 12m + 6b - 20
$$

Now we solve this system of equations:  

$$
28m + 12b - 42 = 0
$$  

$$
12m + 6b - 20 = 0
$$

It’s a straightforward set of linear equations. One way to solve is:

- Multiply the second equation by 2:  
  $24m + 12b - 40 = 0$
- Subtract that from the first equation to eliminate $b$:  

$$
(28m - 24m) + (12b - 12b) - (42 - 40) = 0
$$  

$$
4m - 2 = 0
$$

So $4m = 2$, giving $m = \tfrac{1}{2}$. Plugging $m = \tfrac{1}{2}$ back into either equation (say the second one):  

$$
12 \cdot \tfrac{1}{2} + 6b - 20 = 0 
$$  

$$
6 + 6b - 20 = 0 
$$  

$$
6b = 14 
$$  

$$
b = \tfrac{7}{3}
$$

Thus,  

$$
m = 0.5, \quad b = \tfrac{7}{3}
$$

These values **minimize** the squared distances between the router’s line and each of the three rooms.

#### Interpreting the Result

- $m = 0.5$ means the “coverage corridor” rises gently as you go right.
- $b = 7/3$ is where it crosses the $y$-axis.

If you plug these into $E(m,b)$, you get a minimum total squared distance of about $4.167$. In practical terms, this line is **positioned so that all three rooms have shorter vertical cables** to tap into the Wi-Fi signal.

This procedure—**finding a line that best fits points in a plane**—is at the heart of **linear regression**. In machine learning, the points might represent data samples, and the best-fitting line (or hyperplane in higher dimensions) helps us make predictions and analyze trends.

**Key Insight**: Whether you’re figuring out how to position a Wi-Fi router for minimal wiring or running a regression for data, the mathematics behind “optimal placement” boils down to taking derivatives, setting them to zero, and solving.