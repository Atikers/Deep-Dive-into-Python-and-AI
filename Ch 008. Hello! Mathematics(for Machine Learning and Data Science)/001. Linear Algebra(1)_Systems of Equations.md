# ***Linear Algebra(1) - Systems of Equations***

## **Linear Algebra Applied**

> **Have you ever wondered how your phone can guess the next word you’re about to type, or how an AI system can predict the power output of a wind turbine given the wind speed and temperature?**  

Linear algebra lies at the heart of many of these remarkable capabilities. From the simplest predictive models—like linear regression—to the grand, multi-layered neural networks that power computer vision or language models, linear algebra acts as the fundamental language that makes it all possible.

### **Why Linear Algebra Matters for AI**

In modern AI, data often comes in the form of large tables of numbers—think of rows as different observations or examples (e.g., different wind turbine measurements) and columns as features (e.g., wind speed, temperature, humidity, and so on). We can stack all these numbers into an object called a **matrix**, and many questions we ask about our data boil down to working with these matrices.  

For example, in a **linear regression** model, you might start with a simple equation like:  

$$
y = w \times x + b
$$

where  
- $x$ could be wind speed,  
- $w$ is a weight that tells us how strongly wind speed influences power output, and  
- $b$ is a constant (often called a *bias* or *intercept*).

As you add more features—like temperature or air pressure—the equation might look like:  

$$
y = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$

Rather than seeing this as a single equation, you typically have many rows of data, each row giving you its own equation (one for each measurement in your dataset). You then try to “solve” these equations—often in an approximate sense—to find the best values of the weights $w_i$ and the bias $b$.

### **Systems of Linear Equations in Action**

When you collect multiple observations, it’s as if you have multiple linear equations all at once. For instance, if you have $m$ measurements, you end up with $m$ equations:  

$$
\begin{aligned}
w_1 x_1^{(1)} + w_2 x_2^{(1)} + \cdots + w_n x_n^{(1)} + b &= y^{(1)} \\
w_1 x_1^{(2)} + w_2 x_2^{(2)} + \cdots + w_n x_n^{(2)} + b &= y^{(2)} \\
& \vdots \\
w_1 x_1^{(m)} + w_2 x_2^{(m)} + \cdots + w_n x_n^{(m)} + b &= y^{(m)} 
\end{aligned}
$$

Here, the superscript $(1)$ or $(2)$, etc., simply means the values come from the first, second, or $m$th measurement.  

All these linear equations taken together form what we call a **system of linear equations**. Linear algebra gives us systematic methods for analyzing and solving these systems—often by arranging numbers into matrices and performing operations on them.

#### **Matrix Notation for Systems of Equations**

The system of equations we just saw can be elegantly represented using matrix notation:  

$$
W \cdot X + b = \hat{y}
$$

where:
- $W = [w_1 \; w_2 \; w_3 \; w_4 \; ... \; w_n]$ is a weight vector containing all our coefficients
- $X$ is a matrix where each row represents one observation:  

$$
X = \begin{bmatrix} 
x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & x_4^{(1)} & ... & x_n^{(1)} \\
x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & x_4^{(2)} & ... & x_n^{(2)} \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & x_4^{(m)} & ... & x_n^{(m)}
\end{bmatrix}
$$

- $b$ is the bias term
- $\hat{y} = [y^{(1)} \; y^{(2)} \; ... \; y^{(m)}]$ is the vector of predicted outputs

This matrix notation is not just more compact—it's also how we implement these calculations in practice using libraries like NumPy or PyTorch. The dot product ($\cdot$) between $W$ and $X$ automatically handles all the multiplication and addition operations needed to compute our predictions.

### **Connecting the Dots**

1. **Geometric Insight**  
   - In two dimensions, each equation like $y = mx + b$ represents a **line**.  
   - In three dimensions, each equation represents a **plane**.  
   - In higher dimensions (where you have many features), we still talk about lines and planes, but they are generalized into concepts called **hyperplanes**.

2. **Engineering & Physics Analogy**  
   - Think of each equation as a rule describing how a physical system behaves—like relating the voltage, current, and resistance in an electrical circuit. If you have multiple circuit elements and constraints, you end up with multiple equations that must all be satisfied at once. That’s exactly what happens with systems of linear equations in AI: each data point adds one more constraint.

3. **Art & Perspective**  
   - When an artist draws in perspective, they use lines converging at a vanishing point to capture 3D objects on a 2D canvas. In linear algebra, we “project” higher-dimensional data down to simpler representations. These projections obey rules that can be described by matrix multiplication. It’s a lot like how perspective transforms a 3D scene into a 2D image.

4. **Medicine & Modeling**  
   - In medical research, you might model how different drug dosages, patient ages, and other health metrics combine to produce certain outcomes. Once again, we’re effectively building a system of linear equations—each patient’s data creates one linear equation linking the treatment variables to the outcome.  

By exploring **systems of equations**, **matrix representations**, **singular and non-singular matrices**, and **determinants**, you’ll gain powerful tools to handle high-dimensional data and solve practical problems.  

### **Looking Ahead**

In the sections that follow, we’ll delve deeper into how these systems of equations are represented and solved:

- **System of Sentences** – Translating word problems or real-world observations into algebraic statements.  
- **System of Equations as Lines and Planes** – Seeing the geometry behind the math.  
- **A Geometric Notion of Singularity** – Understanding when systems do or don’t have solutions.  
- **Singular vs Non-Singular Matrices** – Why some matrices can’t be inverted and why that matters.  
- **Linear Dependence and Independence** – The foundation for vector spaces and more advanced AI concepts.  
- **The Determinant** – A special number that captures vital information about a matrix.

All these ideas become especially useful once you realize how directly they apply to machine learning tasks like regression or more advanced methods. By the end of this journey, you’ll see that **linear algebra isn’t just theoretical math—it’s a central pillar of modern AI.**

---

## **System of Sentences**

> **Have you ever tried solving a riddle where each clue reveals a piece of hidden information—like figuring out the color of a dog, a cat, and a bird?**  

This idea of combining clues to reach a logical conclusion is very similar to how we combine equations in linear algebra. But before we jump into formal equations, it helps to see how “systems of sentences” can work as analogies for “systems of equations.”

### **Sentences as Pieces of Information**

Imagine each sentence as one piece of information describing the color of an animal. For example, consider this simple scenario:

- **Sentence 1**: The dog is black.  
- **Sentence 2**: The cat is orange.

If you have one dog and one cat, these sentences don’t contradict each other, and each provides new information. A collection of such sentences is what we call a **system** of sentences.

1. **Complete System**: If the number of distinct pieces of information matches the number of sentences, we call the system **complete**. In everyday language, this means every sentence adds something new; there are no repeats or contradictions.
2. **Redundant System**: If a system repeats the same fact multiple times (e.g., “The dog is black” and again “The dog is black”), then it provides fewer unique pieces of information than the total number of sentences. We say the system is **redundant**.
3. **Contradictory System**: If one sentence directly conflicts with another (e.g., “The dog is black” vs. “The dog is white”), the system is **contradictory** because it’s impossible for both statements to be true at the same time.

### **Singular vs. Non-Singular**

In linear algebra, we often talk about whether a system is **singular** or **non-singular**:

- A **non-singular system** is **complete**: it has exactly as many pieces of distinct information as it has sentences. No sentence is wasted or contradictory.
- A **singular system** is either **redundant** (repeats info) or **contradictory** (impossible to satisfy all at once).

As a quick example:

1. **System A**  
   - Sentence 1: The dog is black.  
   - Sentence 2: The cat is orange.  
     
   Here, each sentence adds unique information—no contradiction, no repetition. The system is complete and therefore **non-singular**.

2. **System B**  
   - Sentence 1: The dog is black.  
   - Sentence 2: The dog is black.  
     
   Both sentences say the same thing, so there’s only one piece of information total. This makes the system **redundant** and therefore **singular**.

3. **System C**  
   - Sentence 1: The dog is black.  
   - Sentence 2: The dog is white.  
     
   Clearly, the dog can’t be both black and white if we assume only one dog. This system is **contradictory** and thus **singular**.

> **Note:**  
> A simple way to think of “singular” vs. “non-singular” is that a non-singular system typically has a precise, one-and-only-one solution (or interpretation), while a singular system either has no valid solution (contradictory) or an infinite number of solutions (redundant). In matrix terms, this often ties back to the idea that a non-singular matrix has a non-zero determinant (meaning it’s invertible), whereas a singular matrix has a determinant of zero (meaning it’s not invertible). This distinction is crucial in linear algebra because it determines whether we can “solve” a system uniquely or not. We’ll explore this connection more in later chapters.

### **Scaling Up: More Sentences, More Animals**

We can extend this idea to three or more sentences. For instance:

- **System D**  
  - Sentence 1: The dog is black.  
  - Sentence 2: The cat is orange.  
  - Sentence 3: The bird is red.  

  Each statement introduces a new color for a different animal. Hence, we have three unique pieces of information and three sentences: a **non-singular** system.

- **System E**  
  - Sentence 1: The dog is black.  
  - Sentence 2: The dog is black.  
  - Sentence 3: The bird is red.  

  This system is **redundant** (the first two sentences repeat the same fact about the dog), so it’s **singular**.

- **System F**  
  - Sentence 1: The dog is black.  
  - Sentence 2: The dog is white.  
  - Sentence 3: The bird is red.  

  This system is **contradictory** (the dog can’t be two colors simultaneously), so it’s also **singular**.

When we talk about **how redundant** a system is, we’re hinting at a concept known as the **rank** in linear algebra. For now, just remember that rank essentially measures how much unique information a system contains.

### **The Connection to Equations**

You might wonder: *What does any of this have to do with solving equations?* The key point is that **sentences** in everyday language can be viewed as constraints or facts about a situation—just like **equations** in math impose constraints on unknowns.  

- A **complete** system of sentences parallels a system of equations with a unique solution.  
- A **redundant** system parallels one that has repeated constraints, which might not change the solution set.  
- A **contradictory** system parallels a system of equations that has no solution (e.g., lines that never intersect).

Understanding this analogy makes it easier to grasp how and why we solve systems of equations in linear algebra.

### **Key Takeaways**

- **Pieces of Information vs. Number of Sentences**: A system is most informative (non-singular) when each sentence contributes a unique fact.  
- **Redundancy**: Repeated facts make the system singular because it adds no new information.  
- **Contradiction**: Conflicting statements also make the system singular because it’s impossible to satisfy all conditions.  
- **Rank (Informal)**: This measures how many unique pieces of information a system really contains. In linear algebra, it’s connected to how we solve systems of equations.  
- **Analogy to Linear Equations**: Systems of sentences behave like systems of equations—each sentence or equation restricts possibilities, guiding us toward a solution or revealing inconsistencies.

---

## **System of Equations**

> **Have you ever strolled through a grocery store and wondered if you could deduce the exact price of each fruit just by observing the totals on your receipts?**  

Equations are a lot like sentences: each one provides a piece of numerical information about unknown values. In the same way that a sentence might say “the dog is black,” an equation might say “an apple plus a banana costs $10.” By collecting multiple equations, we create a **system of equations**, which tells us how to solve for each unknown value (like the cost of each fruit).

### **From Sentences to Equations**

Let’s say you have two unknowns:

- $a$: the cost of an apple  
- $b$: the cost of a banana  

If you know that buying one apple and one banana totals $10, you can translate this sentence into a mathematical statement:  

$$
a + b = 10
$$

This is your first linear equation. If, on a different day, you buy the same apple plus two bananas and pay \$12, you get a second equation:  

$$
a + 2b = 12
$$

Just like in our “systems of sentences” analogy, each equation can provide a unique piece of information, no information at all (repeats), or contradictory information.

### **Unique Solution: A Complete, Non-Singular System**

Suppose your system looks like this:

1. $a + b = 10$  
2. $a + 2b = 12$  

From the first day to the second day, you notice that the **only** difference in cost is one extra banana—\$2 more. That means $b = 2$. Substituting back into the first equation, $a + 2 = 10$, so $a = 8$. Here, we get exactly one possible answer:  

- $a = 8$  
- $b = 2$  

In linear algebra terms, this system is **complete** (each equation gave us fresh information) and **non-singular** (it has exactly one unique solution).

### **Infinite Solutions: A Redundant, Singular System**

Now consider:

1. $a + b = 10$  
2. $2a + 2b = 20$  

That second equation is just the first equation multiplied by 2. Mathematically, we haven’t learned anything new. As a result, there isn’t enough information to pin down a unique $a$ and $b$. **Any** pair of numbers that sum to 10 will satisfy both equations. You could have:

- $a = 8, b = 2$  
- $a = 5, b = 5$  
- $a = 1.7, b = 8.3$  
- … and so on!

Because of this redundancy, the system has **infinitely many solutions**. It’s called **singular** in linear algebra, since you can’t isolate a single unique solution.

### **No Solutions: A Contradictory, Singular System**

Finally, suppose you have:

1. $a + b = 10$  
2. $2a + 2b = 24$  

From the first equation, you’d expect $2a + 2b$ to be $2\times 10 = 20$, not 24. These two equations **contradict** each other. No single pair of values can make both equations true at once, so there are **no solutions**. Once again, this is **singular**, but for a different reason: the system is **contradictory**.

### **Extending to More Variables**

What if you’re also curious about cherries? Then you might have three unknowns: 

- $a$: the cost of an apple  
- $b$: the cost of a banana  
- $c$: the cost of a cherry  

You can collect three equations—one for each shopping trip—to solve for $a$, $b$, and $c$. For instance:

1. $a + b + c = 10$  
2. $a + 2b + c = 15$  
3. $a + b + 2c = 12$  

By looking at the difference between these equations, you can isolate the costs of the extra items (like the additional banana in the second equation or the additional cherry in the third), eventually finding a unique set of values for $a$, $b$, and $c$. 

### **Linear vs. Non-Linear Equations**

So far, our equations have been **linear**. A linear equation allows you to:

- Multiply each variable by a constant (like $a, 2b, 3c$).  
- Add or subtract these terms.  
- Include a constant term (like $10$ or $15$).  

In more formal terms, a linear equation in variables $x_1, x_2, ...$ takes the form:

$$p_1 x_1 + p_2 x_2 + \cdots + p_n x_n = q,$$

where $p_1, p_2, \dots$ and $q$ are constants. 

**Non-linear** equations can involve squares (like $a^2$), products of variables (like $ab$), exponentials, or trigonometric functions. These are **not** covered by typical linear algebra techniques, because they bend or curve in ways that prevent the nice, straightforward solutions we can find in linear systems.

### **Applications to Other Fields**

- **Physics**: Balancing forces in a static system often leads to linear equations for each direction (e.g., horizontal and vertical force sums).  
- **Chemistry**: Balancing chemical equations—like ensuring the same number of each type of atom on both sides—can be translated into linear systems.  
- **Engineering**: Circuits with resistors and voltages (using Ohm’s law $V = IR$) can also lead to systems of linear equations describing current flow.

In each case, whether we have a **unique solution** (one outcome), **infinite solutions** (lots of possible outcomes), or **no solution** (impossible setup) mirrors what we see in our simple fruit-price examples.

### **Key Takeaways**

1. **Equations = Information**: Each linear equation is like a statement that adds constraints on the unknowns.  
2. **Complete vs. Redundant vs. Contradictory**: 
   - **Complete** (non-singular) $\rightarrow$ one unique solution.  
   - **Redundant** (singular) $\rightarrow$ infinitely many solutions.  
   - **Contradictory** (singular) $\rightarrow$ no solutions at all.  
3. **Higher Dimensions**: Adding more items or variables just extends the concept. You can collect more equations to figure out more unknowns.  
4. **Linear vs. Non-Linear**: Linear systems have variables combined only by addition, subtraction, and multiplication by constants. Non-linear systems can involve powers, products of variables, or more complex functions.  

Understanding these fundamentals of systems of linear equations prepares you for the next steps in linear algebra—where you’ll see how matrices give us a powerful toolkit for solving (and interpreting) large sets of equations in real-world applications.

---

## **System of Equations as Lines and Planes**

Below is the **complete** draft, continuing from where it was cut off. Every math expression is wrapped in the format \($...$\). Enjoy!

---

## **System of Equations as Lines and Planes**

> **Have you ever tried mapping out where two roads might intersect on a city grid, or how multiple flat surfaces (like large sheets of paper) might meet in space?**

It turns out that **linear equations** can be viewed in precisely this geometric way. In two dimensions, each linear equation is a **line**. In three dimensions, each linear equation becomes a **plane**. And if you had more variables, you’d be dealing with higher-dimensional objects (often called **hyperplanes**). This geometric viewpoint helps us see why some systems of equations have a single solution, many solutions, or no solutions at all.

### **Lines in Two Dimensions**

Let’s start with a simple equation involving two unknowns, $a$ and $b$. Suppose:  

$$
a + b = 10
$$

We can interpret this on a 2D coordinate grid where the horizontal axis is $a$ and the vertical axis is $b$. Each point $(a, b)$ on the grid that satisfies $a + b = 10$ lies on a **straight line**. For instance:

- (10, 0): because 10 + 0 = 10  
- (0, 10): because 0 + 10 = 10  
- (4, 6): because 4 + 6 = 10  

All such points form a continuous line. Every point on that line is a valid solution to the equation $a + b = 10$.

#### **Slope and Intercepts**  
We might rewrite $a + b = 10$ as $b = -a + 10$ Then:  
- The **slope** (how steep the line is) is -1.  
- The **vertical intercept** (where the line crosses the $b$-axis) is 10.

### **Visualizing a System of Two Equations**

Now, if each equation is a line, then a **system** of two equations is simply **two lines drawn on the same plane**:

1. **Example 1: Unique Solution**  

$$
\begin{cases}
a + b = 10,\\
a + 2b = 12
\end{cases}
$$

- Plotting both lines reveals they intersect exactly once, at $(8, 2)$. This is a **non-singular** (complete) system with a **unique** solution.

2. **Example 2: Infinite Solutions**  

$$
\begin{cases}
a + b = 10,\\
2a + 2b = 20
\end{cases}
$$  

- Here, the second line is just a multiple of the first—so geometrically, it’s the **same line**. Every point on that line satisfies **both** equations, yielding **infinitely many solutions**. This system is **redundant** and therefore **singular**.

3. **Example 3: No Solutions**  

$$
\begin{cases}
a + b = 10,\\
2a + 2b = 24
\end{cases}
$$  

- If $a + b$ is 10, you’d expect $2a + 2b$ to be $2 \times 10 = 20$, not 24. On a graph, these lines are **parallel** and never meet. So there is **no solution**. This system is **contradictory** and hence **singular**.

### **Planes in Three Dimensions**

When you add one more variable, say $c$, each equation can be visualized as a **plane** in three-dimensional space. For example:  

$$
a + b + c = 1
$$

You can think of the 3D axes as:
- The $a$-axis (horizontal),
- The $b$-axis (vertical),
- The $c$-axis (coming out of or going into the page/screen).

A few points on this plane:
- (1, 0, 0) because 1 + 0 + 0 = 1  
- (0, 1, 0) because 0 + 1 + 0 = 1  
- (0, 0, 1) because 0 + 0 + 1 = 1  

All such points—and every point in between on that flat surface—satisfy $a + b + c = 1$

#### **Intersections of Planes**  
- **One Unique Intersection (a Point)**: Three distinct planes can intersect at a single point (like how you might place three sheets of paper so they meet at exactly one corner). This typically indicates a **non-singular** system with one unique solution.  
- **A Line of Intersection**: Sometimes two planes intersect along a line, and if the third plane doesn’t add any new constraint, you end up with an **infinite** set of solutions (every point on that line works).  
- **Identical Planes**: If you have two or three planes describing the same flat region, they are redundant—leading to infinitely many solutions.  
- **Parallel or Contradictory Planes**: If at least one plane can’t intersect with the others at a common point (for instance, it’s parallel and shifted), you get **no solutions**—a contradictory system.

### **Singularity vs. Non-Singularity(Revisited)**

Just like with **systems of sentences**, systems of linear equations can be:

- **Non-Singular (Complete)**: Enough independent information to give a **unique** solution.  
- **Singular**:  
  - **Redundant**: Information is repeated, leading to **infinite** solutions.  
  - **Contradictory**: Information conflicts, leading to **no** solutions.

In two dimensions (2D), this translates to:
- **Unique solution**: Two lines intersect at a point.  
- **Infinite solutions**: Lines overlap entirely.  
- **No solutions**: Lines are parallel and never intersect.

In three dimensions (3D), it translates to:
- **Unique solution**: Three planes meet at a single point.  
- **Infinite solutions**: Planes coincide or intersect along a line or an entire plane.  
- **No solutions**: At least one plane is parallel or shifted, so there’s no common intersection point.

### **Key Takeaways**

1. **Equations as Geometry**: Each linear equation in two variables corresponds to a line; in three variables, to a plane.  
2. **Systems as Intersections**: A system of equations means we look for points (or lines, planes) that satisfy *all* equations simultaneously—geometrically, this is where all lines/planes intersect.  
3. **Singularity**:  
   - **Redundant** systems have multiple equations that describe the same geometric object, resulting in infinitely many solutions.  
   - **Contradictory** systems have geometric objects that never intersect, so there are no solutions.  
4. **Non-Singularity**: Distinct lines or planes that intersect at exactly one point provide a single, unique solution.  
5. **Higher Dimensions**: While it’s harder to visualize, the same principles apply for 4D or more variables—each equation describes a hyperplane, and solutions are their intersections.

With these visual insights, you’ll find that solving systems of equations can often feel more intuitive: just think about whether you have **one intersection**, **infinite intersections**, or **no intersection at all**.

---

## **A Geometric Notion of Singularity**

> **Have you ever wondered why some lines in the plane meet neatly at a single point, while others either overlap completely or never cross at all?**

In linear algebra, **singularity** is closely tied to these geometric behaviors. When we talk about **non-singular** systems, we mean the lines (or planes, in higher dimensions) intersect in exactly one point. **Singular** systems, on the other hand, exhibit behaviors such as lines overlapping or failing to intersect at all.

### **From Three Example Systems to Two Buckets**

Previously, you may have seen three common outcomes for a pair of linear equations in two variables (like $a$ and $b$):

1. **A Unique Solution**: The two lines intersect in exactly one point.  
2. **Infinite Solutions**: The two lines lie on top of each other (completely overlapping).  
3. **No Solutions**: The two lines are parallel but distinct (never meet).

From these outcomes, we called the systems **non-singular** (the first case) or **singular** (the latter two). However, we can simplify this story by focusing on singularity alone and grouping “infinite solutions” and “no solutions” under one umbrella: they are both **singular** systems.

### **What If We Force All Constants to Zero?**

Consider a general linear equation in two variables, such as:  

$$
a + b = c
$$

If you set $c = 0$, you get:  

$$
a + b = 0
$$

which describes a line that **must** pass through the origin (0,0). Why? Because if $a = 0$ and $b = 0$, the equation becomes $0 + 0 = 0$, which is true.  

Now imagine doing this for **both** equations in your system—making all constant terms zero. Geometrically, this means:

1. **Non-Singular System** (originally one intersection):
   - The lines still intersect in exactly one point—namely at (0,0)—because they are not multiples of each other.
2. **Singular System** with infinite solutions (originally overlapping lines):
   - Those same lines still overlap when shifted to the origin, so you again get infinitely many solutions along that single line.
3. **Singular System** with no solutions (originally parallel lines):
   - Once you collapse both constant terms to zero, those lines become *the same line through the origin*.  
   - Now they overlap and yield infinitely many solutions—but crucially, they remain **singular**.

The key insight is that **changing the constant term does not affect whether the system is singular or non-singular**. It affects whether the lines are shifted up, down, or not at all—but either they’re parallel (singular), overlapping (singular), or they meet at a single point (non-singular).

### **Why Focus on Singularity?**

In more advanced linear algebra, **singularity** or **non-singularity** of a system translates directly into whether its associated **matrix** is **invertible** or not. This has far-reaching consequences:

- **Non-Singular** (Invertible Matrix)  
  - A unique solution exists, and we can solve for it cleanly (e.g., by matrix methods such as the inverse).
- **Singular** (Non-Invertible Matrix)  
  - Either there are infinitely many solutions (redundant) or no solutions at all (contradictory).

By allowing ourselves to assume the constant terms are zero, we simplify visualizations and notation without losing the core idea of singularity. In such cases, every line (or plane, in 3D) necessarily passes through the origin (0,0) (or (0,0,0)), making it clearer whether those geometric objects coincide or differ.

### **Key Takeaways**

1. **Singularity Is About Independence**  
   - If two lines in 2D (or planes in 3D) are genuinely different (not multiples of each other) and intersect in a single point, the system is **non-singular**. Otherwise, it’s **singular**.

2. **Constants Don’t Change Singularity**  
   - Translating lines (by shifting constant terms) moves them around the plane but does not affect whether they are parallel, overlapping, or intersecting at a point.

3. **Simplifying the System**  
   - Setting constants to zero forces lines through the origin. A pair of lines will either intersect at (0,0) (non-singular) or coincide (singular).

4. **Matrix Perspective**  
   - These geometric ideas connect directly to matrix invertibility. A matrix is non-singular if its rows (or columns) are not multiples of one another, corresponding to lines (or planes) that aren’t overlapping in the geometric sense.

By focusing on **singularity** vs. **non-singularity**, you’ll have a powerful lens for understanding more sophisticated topics in linear algebra—like **matrix rank**, **determinants**, and **vector spaces**—all of which hinge on whether certain sets of vectors or equations are fundamentally “independent”.

---

## **Singular vs Non-Singular Matrices**

> **Have you ever wondered why some systems of equations can be solved by inverting a matrix, while others seem impossible to solve or have infinitely many solutions?**  

This question leads us directly into one of the most important concepts in linear algebra: the difference between **singular** and **non-singular** matrices.

### **From Equations to Matrices**

When you write down a system of linear equations, each equation can be viewed as a row of **coefficients**. For instance, consider two equations in variables $a$ and $b$:  

$$
\begin{cases}
a + b = 0 \\
a + 2b = 0
\end{cases}
$$

We can capture the coefficients of $a$ and $b$ in a $2 \times 2$ grid:

|    | $a$ | $b$ |
|:--:|:-----:|:-----:|
|**Eqn 1**| 1 | 1 |
|**Eqn 2**| 1 | 2 |

This grid is called a **matrix**. Specifically, we often denote it by  

$$
M = \begin{pmatrix}
1 & 1\\
1 & 2
\end{pmatrix}
$$

- Each **row** corresponds to one linear equation.  
- Each **column** corresponds to one variable’s coefficient (here, $a$ or $b$).

### **Non-Singular (Invertible) Matrices**

If the above system has a **unique solution**, then the corresponding matrix is said to be **non-singular**. In practical terms, a non-singular matrix:

1. **Has an inverse** (like how the number 5 has a reciprocal, but 0 does not).  
2. Corresponds to lines (in 2D) or planes (in 3D) that intersect at exactly one point.  
3. Guarantees that you can solve the system of equations uniquely.

For the $2 \times 2$ case, there’s a simple test using the **determinant**, denoted by $\det(M)$. If  

$$
\det(M) = 
\begin{vmatrix}
1 & 1\\
1 & 2
\end{vmatrix}
= (1 \cdot 2) - (1 \cdot 1) = 2 - 1 = 1,
$$

then $\det(M) \neq 0$. This means $M$ is **non-singular**, implying the system has a unique solution (in this case, $a = 0, b = 0$).

### **Singular (Non-Invertible) Matrices**

Now consider a slightly different system:  

$$
\begin{cases}
a + b = 0 \\
2a + 2b = 0
\end{cases}
$$

The corresponding matrix becomes  

$$
M' = \begin{pmatrix}
1 & 1 \\
2 & 2
\end{pmatrix}
$$

Notice how the second row is just **twice** the first row. The determinant test quickly shows  

$$
\det(M') = 
\begin{vmatrix}
1 & 1 \\
2 & 2
\end{vmatrix}
= (1 \cdot 2) - (1 \cdot 2) = 2 - 2 = 0
$$

which means $M'$ is **singular**—it cannot be inverted. Geometrically, these two equations represent the **same line**, so there are **infinitely many solutions** $(a, b)$ that satisfy both equations (for example, $a = -b$). Singular matrices can also correspond to **no solutions** if the lines (or planes) are parallel and distinct; the unifying theme is that $\det(M) = 0$ for a singular matrix.

### **Extending to More Variables**

For three variables $a, b, c$, you can form a $3 \times 3$ matrix of coefficients:  

$$
A = \begin{pmatrix}
\text{coeff of }a & \text{coeff of }b & \text{coeff of }c \\
\text{coeff of }a & \text{coeff of }b & \text{coeff of }c \\
\text{coeff of }a & \text{coeff of }b & \text{coeff of }c
\end{pmatrix}
$$

- **Non-Singular (Invertible)**: The system has a **unique** solution. In geometric terms, the three planes intersect at exactly one point.  
- **Singular (Non-Invertible)**: You either have **infinitely many** solutions (the planes might overlap in a line or a single plane), or **no** solutions at all (the planes might be parallel in some way).

Once again, you can use the determinant (or more advanced methods) to figure out if the matrix is singular or non-singular.

### **Constants Don’t Change Singularity**

A crucial insight is that **changing the constant terms in your equations (e.g., the right-hand side of $a + b = \dots$) does not affect whether the matrix of coefficients is singular or non-singular.** Why? Because those constants do not appear in the coefficient matrix—they only shift the lines or planes up or down. If the columns (variables) are still linearly dependent, the matrix remains singular; if they’re independent, the matrix remains non-singular.

### **Key Takeaways**

1. **Matrices from Systems**  
   - Each row represents an equation; each column represents one variable’s coefficient.

2. **Non-Singular = One Unique Solution**  
   - The coefficient matrix has an inverse (in 2D, determinant is not 0).  
   - Geometrically, lines or planes intersect at exactly one point.

3. **Singular = Infinitely Many or No Solutions**  
   - The coefficient matrix is not invertible (in 2D, determinant is 0).  
   - Geometrically, lines might overlap (infinite solutions) or be parallel (no solutions).

4. **Constants vs. Coefficients**  
   - Shifting the constants moves the lines or planes but does not affect the matrix’s singularity.  
   - Singularity is fundamentally about the relationships among the **variables** (i.e., the columns of the matrix).

By understanding the difference between **singular** and **non-singular** matrices, you’ve taken a major step in linear algebra. Next, we’ll see how concepts like the **inverse** of a matrix and the **determinant** help us solve systems more systematically—and how they underpin many foundational ideas in AI and beyond.

---

## **Linear Dependence and Independence**

> **Have you ever wondered why, in some problems, certain equations feel “redundant” while others contribute entirely new information?**  

This notion of whether an equation (or a row of a matrix) is “new” or just a repeat of what you already know is what **linear dependence** and **independence** are all about. When rows (or columns) in a matrix are “copies” (or scaled/combined versions) of each other, the matrix—and the system it represents—turn out to be **singular**. But when no row can be derived from the others, you get **independent** rows and a **non-singular** system with a unique solution.

### **Recap: Singular vs. Non-Singular Systems**

- A **singular** system of linear equations either has **infinitely many** solutions or **no** solutions.  
- A **non-singular** system has **exactly one** unique solution.

We’ve seen before that this hinges on whether equations are essentially duplicates or if they bring distinct information. Linear dependence captures this idea in a more formal way.

### **From Sentences to Equations to Rows**

1. **Systems of Sentences**  
   - A sentence is “redundant” if it can be inferred from others.  
   - If multiple sentences say the same thing (or conflict in a way that adds nothing new), the system is **singular**.

2. **Systems of Equations**  
   - An equation is “redundant” if it can be generated by the others—meaning it doesn’t provide new constraints.  
   - If each equation is genuinely different, the system is **non-singular** and yields a single unique solution.

3. **Matrix Rows**  
   - When we translate each equation into a row of **coefficients**, redundancy shows up as one row being a linear combination of the others.  
   - If no row is a linear combination of the others, the rows (and hence the matrix) are **linearly independent**, and the system is **non-singular**.

### **What Is Linear Dependence?**

A set of rows (or columns) is **linearly dependent** if you can find constants (not all zero) that multiply each row in such a way that they sum to the **zero row**. For example:

- Suppose you have three equations in three variables $a, b, c$:  
  1. $a = 1 \rightarrow a + 0b + 0c = 1$  
  2. $b = 2 \rightarrow 0a + b + 0c = 2$  
  3. $a + b = 3 \rightarrow a + b + 0c = 3$  

  Notice that **Equation 3** is simply **Equation 1 plus Equation 2**. It adds no fresh information; it’s **dependent**.  

- In matrix form, these become rows of coefficients:  

$$
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 1 & 0
\end{pmatrix}
$$

The **third row** is the sum of the **first** and **second** rows, so the rows are linearly dependent.

#### **Geometric Analogy (2D or 3D)**  
- In 2D, if one line is a scalar multiple of another, they lie directly on top of each other—making the system singular.  
- In 3D, if one plane is a combination of two others, the three planes don’t properly “span” a full 3D space, and again we get singularity (either infinite solutions or none).

### **Examples of Dependence**

1. **Exact Scalar Multiples**  
   - For two equations $a + b = 0$ and $2a + 2b = 0$, the second is just twice the first. One row is a multiple of the other, so **dependent** rows → **singular** matrix.

2. **Sum or Average of Rows**  
   - For three rows, it can be more subtle. Sometimes row 2 might be the average (or another linear combination) of row 1 and row 3. If that’s true, row 2 adds nothing new, so the system is singular.

3. **Multiple Relations**  
   - Highly dependent systems can have many different ways to combine rows. For instance, if each row is just a scaled version of the first row, the matrix is extremely singular (think 1,1,1, then 2,2,2, then 3,3,3, etc.).

### **Linear Independence**

In contrast, a set of rows is **linearly independent** if no row can be expressed as a combination of the others. Concretely:

- **None** of the equations is redundant.  
- **No** row is a simple scalar multiple or sum/average of the others.  
- The matrix has full “originality” in its rows, yielding a non-singular system with exactly one solution.

If you’ve tried all possible ways to combine the rows and can’t recreate one row from the others, then you’ve demonstrated independence. In larger matrices, it can be cumbersome to do this by mere inspection, which is why we develop systematic methods (like row-reduction) to check for dependence or independence.

### **Why Does This Matter?**

1. **Solving Systems Efficiently**  
   - A non-singular matrix (rows are independent) means you can solve for your variables cleanly.  

2. **Geometry and Beyond**  
   - In physics, you might interpret each row as a separate “equilibrium” condition. If some conditions are redundant, you’re not adding new constraints to the system.  
   - In engineering, linearly independent equations can represent different stress or strain conditions in a structure—if one condition is just a blend of others, it doesn’t provide extra safety checks.  
   - In data science or AI, columns of a data matrix might represent different features. If some features are purely linear combinations of others, they’re redundant, and you can remove them without losing predictive power.

3. **Matrix Operations and Inverses**  
   - If the coefficient matrix is singular (rows or columns are dependent), you can’t invert it—meaning certain linear algebra algorithms fail or become more complicated (like needing pseudo-inverses).

### **Key Takeaways**

- **Linear Dependence** means at least one row (or column) can be formed by combining other rows (or columns).  
  - The system or matrix is **singular**.  
  - Either we have infinitely many solutions (redundancy) or none (contradiction).
- **Linear Independence** means no row (or column) is a linear combination of the others.  
  - The system or matrix is **non-singular**.  
  - There is exactly **one** unique solution for the system of equations.
- **Checking Dependence** can be done by:  
  - Trying to express one row as a combination of the others.  
  - Row-reduction methods (which you’ll learn soon).  
  - Determinants (for square matrices).

Once you see how these ideas simplify or complicate real-world problems—like combining multiple constraints in engineering or selecting features in machine learning—you’ll recognize **linear independence** as a bedrock concept of linear algebra and AI.

---

## **The determinant**

> **Have you ever wanted a lightning-fast way to check if a system of equations is solvable without actually doing all the algebra? :)**  

That’s exactly what the **determinant** does for us in linear algebra. It’s a single number associated with a square matrix, and it tells us if a matrix is **singular** (zero determinant) or **non-singular** (non-zero determinant).  

### **Determinant for a 2×2 Matrix**

Consider a 2×2 matrix:  

$$
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}
$$

Its **determinant** is given by:  

$$
\det \begin{pmatrix}
a & b\\
c & d
\end{pmatrix}
= ad - bc.
$$

1. **Non-Singular (Invertible)**:  
   If $ad - bc \neq 0$, the matrix is **non-singular**, and the corresponding system of equations has a unique solution.  
2. **Singular (Not Invertible)**:  
   If $ad - bc = 0$, the matrix is **singular**, and its system of equations either has infinitely many solutions or no solutions.

Think of $ad$ as the product of the **main diagonal**, while $bc$ is the product of the **other diagonal**. Subtracting these two gives a quick test for singularity.

### **2×2 Example**

- **Matrix A**: Since the determinant is non-zero, $A$ is **non-singular**  

$$
A = \begin{pmatrix}
1 & 1\\
1 & 2
\end{pmatrix},
\quad
\det(A) = (1 \times 2) - (1 \times 1) = 2 - 1 = 1 \neq 0
$$

- **Matrix B**: Here, the determinant is zero, so $B$ is **singular**.  

$$
B = \begin{pmatrix}
1 & 1\\
2 & 2
\end{pmatrix},
\quad
\det(B) = (1 \times 2) - (2 \times 1) = 2 - 2 = 0.
$$

### **Determinant for a 3×3 Matrix**

For a 3×3 matrix:  

$$
\begin{pmatrix}
x & y & z \\
p & q & r \\
u & v & w
\end{pmatrix}
$$

the determinant can be computed by extending the 2×2 idea but accounting for more diagonals. One common method is sometimes called the “diagonal rule” or “Sarrus’ rule”:

1. **Sum of “left-to-right” diagonals**  
   - $(x \times q \times w) + (y \times r \times u) + (z \times p \times v)$

2. **Subtract the “right-to-left” diagonals**  
   - $- (z \times q \times u) - (y \times p \times w) - (x \times r \times v)$

Symbolically,  

$$
\det = xqw + yr u + zpv - (zqu + ypw + xrv).
$$

- If this sum is **zero**, the matrix is **singular**.  
- If it’s **non-zero**, the matrix is **non-singular**.

#### **Shortcut for Triangular Matrices**  
If your matrix is **upper** (or lower) triangular—meaning all entries below (or above) the main diagonal are zero—its determinant is simply the **product of the diagonal entries**. For instance:  

$$
\begin{pmatrix}
1 & 1 & 1 \\
0 & 2 & 2 \\
0 & 0 & 3
\end{pmatrix},
\quad
\det = 1 \times 2 \times 3 = 6 \neq 0 \Rightarrow \text{non-singular.}
$$

If even one diagonal entry was zero, you’d get a determinant of zero, implying singularity.

### **Why the Determinant Matters**

1. **Fast Singularity Test**  
   - No need to fully solve the system. If $\det \neq 0$, you have exactly one solution; if $\det = 0$, you either have infinitely many or none.

2. **Matrix Inverse**  
   - A matrix is invertible **only** if its determinant is non-zero. This fact underpins many solution techniques in linear algebra, especially in higher-dimensional problems.

3. **Geometric Interpretation**  
   - In 2D, $\lvert \det(A)\rvert$ can represent the area scaling factor of a transformation; in 3D, it’s a volume scaling factor. A determinant of zero means the transformation “flattens” space onto a lower dimension.

4. **Broad Applications**  
   - Whether you’re dealing with transformations in graphics, changing variables in integrals, or solving large systems in data science, the determinant is a linchpin concept.

### **Key Takeaways**

- **Determinant = 0** $\implies$ **Singular Matrix** (no inverse, system is either redundant or contradictory).  
- **Determinant $\neq 0$** $\implies$ **Non-Singular Matrix** (invertible, system has a unique solution).  
- **2×2 Determinant**: $ad - bc$.  
- **3×3 Determinant**: Sum of products of “forward diagonals” minus sum of products of “backward diagonals.”  
- **Triangular Matrices**: Determinant is the product of diagonal elements.

By learning the determinant, you add a powerful tool to your linear algebra toolkit—enabling you to quickly gauge the solvability of systems and the invertibility of matrices, all essential skills for further studies in AI and beyond.