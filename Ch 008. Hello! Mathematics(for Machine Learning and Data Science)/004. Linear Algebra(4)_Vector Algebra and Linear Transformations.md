# ***Vector Algebra***

## **Machine Learning Motivation**

**Have you ever wondered how a simple line—like the ones you see in high school algebra—could help predict the future?** For instance, imagine you want to anticipate how many hours a plant might grow given certain amounts of water and sunlight. It turns out that we can often represent problems like this with straightforward equations and then apply a branch of mathematics called *linear algebra* to make sense of them. 

### A Quick Overview of Vectors and Matrices

To warm up, recall how you handle everyday numbers: you can add, subtract, multiply, and even invert them (for example, the inverse of 2 is 1/2). With vectors and matrices—just like with single numbers—you can perform similar operations. You can add two vectors, multiply a vector by a matrix, or even find the inverse of a matrix under specific conditions.

But why are these operations so important for machine learning? One big reason is that any dataset can be expressed in terms of rows and columns—think of rows as individual examples and columns as different features of the data. Matrices and vectors are the natural building blocks for this representation.

### Linear Regression as a Motivating Example

Early in your journey with machine learning, you might encounter **linear regression**. In linear regression, you assume your data roughly follows a relationship that looks like:  

$$
y = W \cdot X + b
$$

- $X$ is the matrix of *features*. Each row in $X$ could be one example (like one plant in your garden), and each column is a particular characteristic (like water input, sunlight hours, etc.).
- $W$ is a vector (or sometimes a matrix) of *weights*, which are the values the model adjusts to fit the data. You can think of them like knobs that the model tweaks to better match reality.
- $b$ (the *bias*) is a constant (or sometimes a small vector) that shifts the output up or down.

If you have multiple examples—say, $m$ plants—then $y$ might be another vector of length $m$, capturing the predicted growth for each plant. Notice how the equation $W \cdot X + b$ is just a more sophisticated version of the line equation you studied in algebra class ($y = mx + c$). 

In practice, real-world data are often *not* perfectly linear, so we rarely solve these equations as a simple one-shot. Instead, algorithms iteratively adjust $W$ and $b$ to best fit the data. Even so, the linear model remains a powerful starting point and is surprisingly effective for many tasks.

### The Leap to Neural Networks

What if your data is *highly nonlinear*—say you’re tracking not just water and sunlight but also soil chemistry, temperature variations, and genetic factors? In that case, a single linear equation might not capture the complexity of your plant growth data. **Neural networks** step in to handle these richer relationships.

Despite their fancy diagrams, neural networks are essentially a **large collection of linear equations** layered together. You can imagine each layer of a neural network as a transformation:  

$$
\text{Layer Output} = \sigma\bigl(W^{[l]} \cdot (\text{Previous Layer}) + b^{[l]}\bigr)
$$

where:
- $W^{[l]}$ is the set of weights for layer $l$,
- $b^{[l]}$ is the bias term for layer $l$,
- $\sigma(\cdot)$ is the *activation function* that injects nonlinearity into the model,
- and “Previous Layer” is just another vector or matrix of data outputs from the prior step.

If you’ve taken a physics or engineering class, you might see an analogy in how multiple filters or transformations act on a signal in sequence. Each layer in the network filters or alters its inputs slightly differently, and together they reveal more complex patterns than any one filter could on its own.

### Why Care About Linear Algebra Here?

All these steps—multiplying inputs by weights, adding biases, stacking layers—boil down to operations on vectors and matrices. Instead of writing dozens (or thousands!) of equations separately, you can represent them compactly with linear algebra. This is exactly why understanding vectors, matrices, and their properties is so crucial to machine learning. 

1. **Efficiency**: Matrix operations can be computed quickly, especially on modern hardware like GPUs.
2. **Clarity**: One matrix equation can replace a huge block of separate equations, making ideas easier to see and manipulate.
3. **Generality**: From linear regression to neural networks to advanced AI models, matrices and vectors show up everywhere.

### Looking Ahead

*"004. Linear Algebra(4)_Vector Algebra and Linear Transformations"* section focuses on:
- Understanding the geometry of vectors (magnitude and direction).
- Learning about matrix operations (such as multiplication and finding inverses).
- Exploring the concept of **linear transformations**, which is a powerful way to visualize what matrices do.

Ultimately, whether you’re constructing a simple linear regression model or designing a cutting-edge neural network, you’ll keep returning to the same essential *linear building blocks*. Mastering them now will make the rest of your AI journey much smoother.

We’ll explore vectors, matrices, and linear transformations step by step. Don’t worry if some parts feel abstract. Remember, each concept you learn here will have a place in real-world applications—be it in predicting plant growth, diagnosing medical images, or creating art with machine learning.

Let’s start exploring these powerful ideas!

---

## **Vectors and their Properties**

**How can a simple list of numbers help us measure distance—whether we’re walking through city blocks or flying in a helicopter?** That’s exactly what vectors do for us. A vector can be thought of as an arrow on a plane (2D) or in a higher-dimensional space (3D, 4D, and so on). In any dimension, a vector has two key attributes:

1. **Magnitude (Size):** How “long” the arrow is.  
2. **Direction:** Where the arrow is pointing.

Below, we’ll explore how these attributes are defined and why they’re so useful.

### **What Is a Vector?**

A vector is simply an ordered list (or *tuple*) of numbers. If you have a vector with two coordinates, like (4, 3), you can picture it in the 2D plane as an arrow pointing to the point (4, 3) starting from the origin (0, 0). If you have a 3D vector like (4, 3, 1), you can visualize it in three-dimensional space.

The number of coordinates in a vector is called its **dimension**.  
- 2D vectors have the form $(x_1, x_2)$.  
- 3D vectors have the form $(x_1, x_2, x_3)$.  
- In general, $n$-dimensional vectors have the form $(x_1, x_2, \dots, x_n)$.

### **Magnitude: Taxicab vs. Helicopter Distance**

Consider a 2D vector (a, b). One way to measure its size (or *magnitude*) is to imagine walking through a city grid:

- **Taxicab Distance (L1-norm)**  
  Think of city blocks: if you can only travel along streets going north-south or east-west, the distance between (0, 0) and (a, b) is simply  
  
$$
|a| + |b|
$$

  This is called the **L1-norm** of the vector. We use absolute values because distance must be positive whether $a$ or $b$ is negative or positive.

- **Helicopter Distance (L2-norm)**  
  If you have a helicopter, you can take a direct flight from $(0, 0)$ to $(a, b)$. According to the Pythagorean theorem, the distance is:  
  
$$
\sqrt{a^2 + b^2}
$$  

  This is called the **L2-norm** of the vector, and it matches our usual notion of “length.” Unless otherwise specified, when we say “the norm” of a vector, we usually mean the L2-norm.

> **Taxicab distance** is how far you must drive on a grid of streets.  
> **Helicopter distance** is a straight line that cuts across everything.

#### **Generalizing to Higher Dimensions**

For an $n$-dimensional vector  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n),
$$

the two norms extend naturally:

- **L1-norm:**  
  
$$
\|\mathbf{x}\|_1 = |x_1| + |x_2| + \dots + |x_n|
$$

- **L2-norm:**  

$$
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}
$$

### **Direction of a Vector**

Along with how large a vector is, its **direction** is equally important. For a 2D vector (4, 3), you can find the angle $\theta$ it makes with the horizontal $x$-axis by noting that  

$$
\tan(\theta) = \frac{3}{4}
$$

Hence,  

$$
\theta = \arctan\!\bigl(\tfrac{3}{4}\bigr)
$$

which works out to roughly 0.64 radians (or about $36.87^\circ$).

Two vectors may have different magnitudes but still point in the same direction. For instance, (2, 1.5) points the same way as (4, 3)—just half the “length.”

### **Vector Notation**

There are many valid ways to write a vector:

1. **Row vector:** $(x_1, x_2, \dots, x_n)$  
2. **Column vector:**  

$$
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
$$

3. **Brackets:** $[x_1, x_2, \dots, x_n]$

Also, different resources may write a vector name in bold (e.g. **x**) or with a little arrow above it (e.g. $\vec{x}$). This chapter uses a simple italic $x$ with subscripts for its components, such as $x_1$, $x_2$, and so on.

The notation might look different, but the underlying idea is the same: a vector is just a collection of numbers.

### **Key Takeaways**

- A vector’s **magnitude** (or *norm*) can be measured in more than one way; the most common is the L2-norm, which corresponds to the direct “as-the-crow-flies” distance.  
- The **direction** of a vector in 2D can be found via trigonometry, and similar principles extend to higher dimensions with more advanced geometry.  
- Vectors can be written in multiple formats, but the concept remains the same: they are building blocks of much of the math in AI and machine learning.

Next, we’ll explore how to perform arithmetic with vectors and see how these operations give us insights into complex datasets and transformations. Let’s get started!

---

## **Vector Operations**

**What happens when you combine two arrows—do they just magically give you another arrow?** In fact, they do! This is one of the central ideas of *vector operations*: you can add and subtract vectors, and even stretch or flip them by multiplying by scalars. Let’s see how this works step by step.

### **Adding and Subtracting Vectors**

Suppose you have two 2D vectors:  

$$
\mathbf{u} = (4, 1), \quad \mathbf{v} = (1, 3)
$$

The **sum** of these vectors is found by adding their components:  

$$
\mathbf{u} + \mathbf{v} = (4 + 1, 1 + 3) = (5, 4)
$$

Geometrically, if you draw both $\mathbf{u}$ and $\mathbf{v}$ starting from the same point, their sum $\mathbf{u} + \mathbf{v}$ is the diagonal of the parallelogram whose sides are $\mathbf{u}$ and $\mathbf{v}$. If you imagine walking along one arrow and then turning to follow the second arrow, you end up at the tip of the summed vector.

Similarly, the **difference** between two vectors—say, $(4, 1)$ minus $(1, 3)$—is calculated by subtracting each component:  

$$
(4, 1) - (1, 3) = (4 - 1, 1 - 3) = (3, -2)
$$

Here, the result $(3, -2)$ corresponds to the other diagonal of the parallelogram. You can think of it as the arrow you’d travel if you start at $(1, 3)$ and move to negative direction of $(4, 1)$.

#### **General Form**

For two $n$-dimensional vectors  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n) 
\quad\text{and}\quad 
\mathbf{y} = (y_1, y_2, \dots, y_n),
$$

the operations are:

- **Addition:**  

$$
\mathbf{x} + \mathbf{y} 
= (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n) 
$$

- **Subtraction:**  

$$
\mathbf{x} - \mathbf{y} 
= (x_1 - y_1, x_2 - y_2, \dots, x_n - y_n)
$$

Both vectors must have the same dimension for these operations to make sense.

### **Distances Between Vectors**

In many machine learning tasks(think about clustering, recommendation systems, etc.), we care about **how far apart** two vectors are. For example, consider the vectors $(1, 5)$ and $(6, 2)$:

1. **L1 Distance:**  
   Also called the taxicab distance, it’s the sum of the absolute differences of each component:  
   
$$
|(1, 5) - (6, 2)|
= |1 - 6| + |5 - 2|
= 5 + 3
= 8
$$

2. **L2 Distance:**  
   This is the straight-line (Euclidean) distance:  
   
$$
|(1, 5) - (6, 2)|
= \sqrt{(1-6)^2 + (5-2)^2}
= \sqrt{(-5)^2 + 3^2}
= \sqrt{25 + 9}
= \sqrt{34}
\approx 5.83
$$

These distance measures help quantify “how different” or “how similar” points are in a dataset—an essential concept in clustering, nearest-neighbor searches, and many other AI applications.

### **Scalar Multiplication**

Another fundamental vector operation is **multiplying by a scalar** (a single number). If $\lambda$ is a scalar and  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n)
$$

then  

$$
\lambda \mathbf{x} 
= (\lambda x_1, \lambda x_2, \dots, \lambda x_n)
$$

- **Stretching (Positive Scalar):**  
  If $\lambda > 0$, you lengthen(or shrink) the vector **without changing its direction**. For instance,  
  
$$
3 \cdot (1, 2) = (3, 6)
$$

  which is just the original vector stretched by a factor of 3.

- **Flipping (Negative Scalar):**  
  If $\lambda < 0$, you both stretch and **reverse** the direction of the vector. For example,  
  
$$
-2 \cdot (1, 2) = (-2, -4)
$$

  has the same line of action but is pointing in the opposite direction—and is doubled in length.

You can think the word **scalar** as **scale + -ar**, then you can grasp the meaning of **scalar multiplication** as the operation of **scaling** the arrow.

### **Why These Operations Matter**

1. **Geometric Insight:** Adding vectors visually gives parallelograms or triangles, providing intuition for more complex concepts like transformations and rotations.
2. **Measuring Similarity:** Subtraction and norms (distances) are critical in machine learning to gauge how alike two data points are.  
3. **Scaling Data:** Scalar multiplication is frequently used to normalize or scale vectors, especially when preparing datasets.

As you continue your study, you’ll see these operations come up repeatedly. They form the foundation not just for understanding linear models but for everything from simple regressions to the most complex neural network layers.

**Next up:** We’ll delve deeper into dot products (and how they relate to angles between vectors) and then see how multiplying matrices by vectors can transform your data in powerful ways. Let’s keep building!

---

## **The Dot Product**

**Have you ever added up your grocery list for apples, bananas, and cherries, just by multiplying quantity by price and then summing everything up?** Congratulations—you’ve already performed a *dot product*! Although the example may seem trivial, the dot product is a foundational operation that appears everywhere in linear algebra and AI.

### **A Real-Life Fruit Example**

Imagine you bought:

- 2 apples, each costing \$3
- 4 bananas, each costing \$5
- 1 cherry, each costing \$2

You can represent the *quantities* of these fruits as a column vector:  

$$
\begin{pmatrix}
2 \\
4 \\
1
\end{pmatrix}
$$

and the *prices* as another column vector:  

$$
\begin{pmatrix}
3 \\
5 \\
2
\end{pmatrix}
$$

To find the total cost, you multiply each pair of corresponding entries and add them up:  

$$
2 \times 3 + 4 \times 5 + 1 \times 2 = 6 + 20 + 2 = 28
$$

This summation of products **is** the dot product. Often in mathematics, you’ll see it written in a more compact notation, for example:  

$$
(2, 4, 1) \cdot (3, 5, 2) = 28
$$

When we arrange one vector as a *row* and the other as a *column*, we get a 1×1 matrix (essentially a single number). However, the order doesn’t really change the essence of the calculation—you’re always pairing up components and summing their products.

### **Connecting the Dot Product to the Norm**

Let’s revisit an earlier vector:  

$$
\mathbf{u} = (4, 3)
$$

We know its **L2 norm** (Euclidean length) is:  

$$
|\mathbf{u}|^2 = \sqrt{4^2 + 3^2} = \sqrt{25} = 5
$$

Notice that $4^2 + 3^2$ can be seen as $\mathbf{u} \cdot \mathbf{u}$—the dot product of the vector with itself. Formally:  

$$
|\mathbf{u}|^2 = \sqrt{\mathbf{u} \cdot \mathbf{u}}
$$

This relationship holds for vectors in any dimension: the Euclidean norm is the square root of the dot product of a vector with itself.

### **Transpose: Turning Columns into Rows (and Vice Versa)**

Sometimes, you’ll see one vector written as a *row* and the other as a *column*, like so:  

$$
(2, 4, 1)  
\quad \text{and} \quad 
\begin{pmatrix}
3 \\
5 \\
2
\end{pmatrix}
$$

To switch between these forms, you use the **transpose** operation, denoted by a superscript $T$. For example,

$$
\begin{pmatrix}
2 \\
4 \\
1
\end{pmatrix}^T 
= (2, 4, 1)
$$

- **Row vector** $\rightarrow$ **Column vector**  
- **Column vector** $\rightarrow$ **Row vector**

#### **Matrix Transpose**

The idea generalizes beyond vectors to full matrices. For a 3×2 matrix,  

$$
A = 
\begin{pmatrix}
2 & 5 \\
4 & 7 \\
1 & 3
\end{pmatrix}
$$

its **transpose** $A^T$ is a 2×3 matrix:  

$$
A^T = 
\begin{pmatrix}
2 & 4 & 1 \\
5 & 7 & 3
\end{pmatrix}
$$

You swap rows and columns: the first column (2, 4, 1) becomes the first row, and the second column (5, 7, 3) becomes the second row.

### **Formal Definition of the Dot Product**

If you have two $n$-dimensional vectors  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n), 
\quad
\mathbf{y} = (y_1, y_2, \dots, y_n),
$$

their **dot product** is  

$$
\mathbf{x} \cdot \mathbf{y} 
= x_1y_1 + x_2y_2 + \dots + x_ny_n
$$

You’ll also see alternate notations, such as $\langle \mathbf{x}, \mathbf{y} \rangle$. The essential idea, though, is always the same: *multiply corresponding components and sum them up*.

### **Why the Dot Product Matters**

1. **Computational Efficiency**: With vectors (and later matrices), dot products let us compactly represent tasks like summing up weighted features—crucial in machine learning.  
2. **Geometric Insight**: Beyond the grocery example, the dot product ties closely to concepts like length (norm) and angles between vectors.  
3. **AI and ML Applications**: Almost every neural network layer uses dot products as part of its internal calculations.  

In essence, the dot product is the *workhorse* of linear algebra and a powerful tool in understanding data operations in AI. Once you see how elegantly it simplifies complex problems—like summing your grocery bills—you’ll realize how indispensable it is across many fields of science, engineering, and beyond.

Next, we’ll dive into **geometric interpretations** of the dot product, exploring how it relates to the angle between two vectors and how these ideas expand to more advanced applications in AI. Let’s keep going!

---

## **Geometric Dot Product**

**Ever wonder how to tell if two arrows in space are pointing orthogonally, sharply toward each other, or almost in the same direction?** The **dot product** holds the key, linking the geometry of vectors to their algebraic operations.

### **Orthogonality (Perpendicular Vectors)**

Two vectors are **orthogonal** (or *perpendicular*) if and only if their dot product is zero. For example, consider the 2D vectors  

$$
(-1, 3) 
\quad\text{and}\quad 
(6, 2)
$$

The dot product is:  

$$
(-1) \times 6 + 3 \times 2 = -6 + 6 = 0
$$

showing they are indeed perpendicular.

**Key Fact**:  

$$
\langle \mathbf{u}, \mathbf{v} \rangle = 0 
\quad\Longleftrightarrow\quad 
\mathbf{u} \perp \mathbf{v}
$$

### **Dot Product and Vector Magnitude**

Remember that the dot product of a vector with itself equals the square of its norm. If $\mathbf{u}$ is any vector, then

$$
\langle \mathbf{u}, \mathbf{u} \rangle 
= |\mathbf{u}|^2
$$

This is just another way of saying $u_x^2 + u_y^2 + \dots$ (in higher dimensions) is the square of the vector’s length.

### **Angle Between Two Vectors**

The dot product can also measure *how aligned* two vectors are. If $\mathbf{u}$ and $\mathbf{v}$ are two vectors making an angle $\theta$ with each other, then:  

$$
\langle \mathbf{u}, \mathbf{v} \rangle 
= |\mathbf{u}||\mathbf{v}| \cos(\theta)
$$

- If $\theta < 90^\circ$ (vectors point in the *same* direction), $\cos(\theta)$ is positive, so the dot product is positive.
  - if $\theta = 0^\circ$, $\cos(\theta) = 1$, so the dot product is $|\mathbf{u}||\mathbf{v}|$.
- If $\theta = 90^\circ$ (vectors are *perpendicular*), $\cos(\theta) = 0$, so the dot product is 0.  
- If $\theta > 90^\circ$ (vectors point in *opposite-ish* directions), $\cos(\theta)$ is negative, so the dot product is negative.

### **Interpreting Positive, Negative, and Zero Dot Products**

Let’s look at one reference vector, $\mathbf{u} = (6, 2)$, and see how the sign of $\mathbf{u} \cdot \mathbf{v}$ changes depending on $\mathbf{v}$:

1. **$\langle \mathbf{u}, \mathbf{v} \rangle > 0$:**  
   The angle between $\mathbf{u}$ and $\mathbf{v}$ is less than $90^\circ$, meaning $\mathbf{v}$ lies in a “forward” region with respect to $\mathbf{u}$.  
   - Example: $\mathbf{v} = (2,\,4)$ yields a dot product of $6 \times 2 + 2 \times 4 = 20$, which is *positive*.

2. **$\langle \mathbf{u}, \mathbf{v} \rangle = 0$:**  
   The vectors are orthogonal (perpendicular).  
   - Example: $\mathbf{v} = (-1,\,3)$ gives $6 \times (-1) + 2 \times 3 = 0$.

3. **$\langle \mathbf{u}, \mathbf{v} \rangle < 0$:**  
   The angle between $\mathbf{u}$ and $\mathbf{v}$ is greater than $90^\circ$, so $\mathbf{v}$ falls in a “behind” region with respect to $\mathbf{u}$.  
   - Example: $\mathbf{v} = (-4,\,1)$ produces $6 \times (-4) + 2 \times 1 = -22$.

Visually, you can think of a circle around $\mathbf{u}$:  
- Vectors pointing generally in the same direction (within 90°) give a *positive* dot product.  
- Vectors pointing *exactly* perpendicular give a dot product of *zero*.  
- Vectors pointing more than 90° away give a *negative* dot product.

---

### **Why This Matters**

- **Machine Learning & AI:** Knowing whether two vectors (data points) are “aligned” or “opposite” can help measure similarity (e.g., in recommendation systems or text analysis).  
- **Geometry & Transformations:** Projections and angles pop up in image processing, physics, robotics, and more.  
- **Neural Networks:** **Each neuron’s output is a dot product of weights and inputs**, so understanding dot products helps demystify neural computations.

When we talk about “closeness” or “similarity” between vectors, the dot product (and related concepts like cosine similarity) offers powerful insight. Keep this geometric view in mind as we move on to other matrix operations and transformations—those, too, will build on the dot product’s fundamental ideas.

---

## **Multiplying a Matrix by a Vector**

**Have you ever tried solving several linear equations at once and noticed the same set of variables shows up in each equation?** Matrices and vectors give us a neat way to handle these systems all at once, rather than dealing with each equation separately.

### **A System of Equations in Matrix Form**

Suppose we have three equations:

1. $a + b + c = 10$  
2. $a + 2b + c = 15$  
3. $a + b + 2c = 12$

We can think of each line as a **dot product**:

- $(1, 1, 1) \cdot (a, b, c) = 10$
- $(1, 2, 1) \cdot (a, b, c) = 15$
- $(1, 1, 2) \cdot (a, b, c) = 12$

Notice that the *right-hand vector*—(a, b, c)—is the same in all three dot products. The only differences are the **row vectors** on the left-hand side. If we stack those row vectors on top of each other, we get a **3×3 matrix**:  

$$
\begin{pmatrix}
1 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{pmatrix}
$$

Then the system above can be compactly written as:  

$$
\begin{pmatrix}
1 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{pmatrix}
\begin{pmatrix}
a \\
b \\
c
\end{pmatrix} = \begin{pmatrix}
10 \\
15 \\
12
\end{pmatrix}
$$

### **Matrix-Vector Product as “Stacked Dot Products”**

Each row of the matrix performs a dot product with the column vector $(a, b, c)$, and the outcome is a **single number**. Because there are three rows in the matrix, we end up with three such dot products—one for each equation—giving us the final **3×1 vector** of results.

In general, a matrix can have *any number of rows and columns*, as long as the number of **columns** in the matrix matches the **dimension** of the vector. For example, a 4×3 matrix can still multiply a 3×1 vector, producing a 4×1 result:  

$$
\begin{pmatrix}
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot
\end{pmatrix}
\begin{pmatrix}
\cdot \\
\cdot \\
\cdot
\end{pmatrix} = 
\begin{pmatrix}
\cdot \\
\cdot \\
\cdot \\
\cdot
\end{pmatrix}
$$

The key is that the “inner” dimensions must match—here, “3” in both cases. The “outer” dimensions (4 and 1) determine the shape of the result.

### **Why It Matters**

- **Simplicity**: Writing multiple equations in matrix form saves space and reveals patterns more clearly.  
- **Generalization**: Many problems in AI and machine learning involve large systems where each row can represent an example, and each column a feature.  
- **Computational Speed**: Computers (and libraries like NumPy) are optimized for matrix–vector multiplications, making data processing much faster.

This matrix–vector multiplication lays the groundwork for more advanced linear algebra operations you’ll see in upcoming sections—like matrix–matrix multiplication and finding inverses. But as you practice, keep in mind this core idea: a **matrix multiplied by a vector is just a collection of dot products**, each row acting on that vector in turn.

---

# ***Linear Transformations***

## **Matrices as Linear Transformations**

**What if you could take every point on a grid and “remap” it to a new position in a structured way?** That’s precisely what **linear transformations** do—and every 2×2 matrix defines one such transformation in the 2D plane. (You can extend these ideas to 3D or higher dimensions, too!)

### **From Systems of Equations to Geometric Transformations**

You’ve already seen matrices as arrays of numbers that can represent systems of linear equations. But there’s a second powerful interpretation: viewing each matrix as a **function** that moves points from one coordinate plane to another.

- **Old view**: Matrices help solve equations like $Ax = b$.  
- **New view**: Each 2×2 matrix $A$ transforms the coordinate system in which vector $\mathbf{x} = (x_1, x_2)$ exists, redefining how we measure and interpret distances and directions in 2D space. The resulting vector $A\mathbf{x}$ represents the same relative position in this transformed coordinate system.

### **A Simple Example in 2D**

Let’s say we have a 2×2 matrix:  

$$
A = \begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
$$

Think of two planes:

1. The *original plane* on the left, with coordinates labeled $(a, b)$.  
2. The *transformed plane* on the right, which shows where each point lands after applying $A$.

For any point in the left plane, you can represent it by a column vector  

$$
\begin{pmatrix} a \\ b \end{pmatrix}
$$

When you multiply that vector by $A$, you get its new position in the right plane.

#### **Checking Key Points**

1. **Origin** (0, 0)  

$$
A \begin{pmatrix} 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
$$

- The origin always maps to the origin for linear transformations.

2. **(1, 0)**  

$$
A \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 3 \\ 1 \end{pmatrix}
$$

- So the unit step on the $a$-axis moves to (3, 1).

3. **(0, 1)**  

$$
A \begin{pmatrix} 0 \\ 1 \end{pmatrix}
= \begin{pmatrix} 1 \\ 2 \end{pmatrix}
$$

- So the unit step on the $b$-axis goes to (1, 2).

4. **(1, 1)**  

$$
A \begin{pmatrix} 1 \\ 1 \end{pmatrix}
= \begin{pmatrix} 4 \\ 3 \end{pmatrix}
$$

- So the point (1, 1) shifts to (4, 3).

These four points (0,0), (1,0), (0,1), (1,1) form a unit square on the left. After multiplying by $A$, they define a **parallelogram** on the right.

### **Tessellating the Plane**

Because these unit squares (and parallelograms) *tile* the entire plane (imagine an infinite grid of them), a linear transformation can be viewed as a consistent *change of coordinates*:

- Every vector in the plane gets mapped to a **unique** new vector.  
- Straight lines remain straight lines, and parallel lines remain parallel (though lengths and angles may change).

For example, consider (-2, 3). In the original plane, that’s two steps left of the origin and three steps up. Applying the matrix:  

$$
A \begin{pmatrix}
-2 \\
3
\end{pmatrix}
= \begin{pmatrix}
-3 \\
4
\end{pmatrix}
$$

Hence, (-2, 3) maps to (-3, 4). Geometrically, you “walk” the same “-2 steps in the transformed a-direction, 3 steps in the transformed b-direction” on the right plane.

When we say A transforms (-2, 3) to (-3, 4), we're really saying:
- The point that was reached by going -2 steps in the original a-direction and 3 steps in the original b-direction
- Is now reached by going -2 steps in the new a-direction and 3 steps in the new b-direction

This is why linear transformations preserve:
- Parallel lines (they remain parallel after transformation)
- The grid structure (though squares become parallelograms)
- Relative positions (points that were aligned stay aligned)

Understanding this geometric interpretation helps visualize what matrices actually do: they don't just move points around randomly—they systematically reshape the entire coordinate space while maintaining its fundamental grid-like structure.

### **Understanding Linear Transformations Through Analogies**

**Think of a Rubber Sheet with a Grid**
Imagine you have a perfectly flat rubber sheet with a grid drawn on it (like graph paper). A linear transformation is like grabbing the corners of this rubber sheet and:
- Stretching it
- Rotating it
- Or squishing it

But here's the key: you can't:
- Tear it
- Fold it
- Create wrinkles or curves

**The Photo Studio Analogy**
Think of a photographer's studio with a backdrop grid:
- The original coordinate system is like taking a photo straight-on
- A linear transformation is like moving your camera to take the same photo from a different angle
- The grid lines in your photo might look slanted or stretched, but:
  - Straight lines stay straight
  - Grid lines that were parallel remain parallel
  - The spacing between lines stays proportional

**The City Block Analogy**
Imagine a city with a perfect grid of streets (like Manhattan):
- The original coordinate system is like looking at a map straight down
- A linear transformation is like tilting your view of the city
- Even though the blocks might look like diamonds instead of squares:
  - Streets that were parallel stay parallel
  - The proportional distances between streets remain consistent
  - You can still navigate using the same "2 blocks this way, 3 blocks that way" instructions

### **Apples, Bananas, and a Matrix**

A practical way to see this is through **prices**:

- Day 1: You buy 3 apples and 1 banana.  
- Day 2: You buy 1 apple and 2 bananas.

If an apple costs a dollars and a banana costs b dollars, the total cost on Day 1 is 3a + b, and on Day 2 it’s a + 2b. You can write this as:  

$$
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
\begin{pmatrix}
a \\ 
b
\end{pmatrix} = \begin{pmatrix}
3a + b \\
a + 2b
\end{pmatrix}
$$

- The plane on the left has coordinates $(a, b)$: the cost of apples and bananas.  
- The plane on the right has coordinates (Cost on Day 1, Cost on Day 2).

If $a = 1$ and $b = 1$, for instance,  

$$
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
\begin{pmatrix}
1 \\ 
1
\end{pmatrix} = \begin{pmatrix}
4 \\
3
\end{pmatrix}
$$

So the day-wise total costs are \$4 for Day 1 and \$3 for Day 2.

### **Why Linear Transformations Matter**

- **Unified View**: They tie together equation-solving, geometry, and practical applications (like pricing) under one mathematical umbrella.  
- **Higher Dimensions**: The same logic applies to 3D, 4D, or any $n$-dimensional space—vital for big data in AI.  
- **Foundation for Advanced Topics**: Concepts like eigenvalues, eigenvectors, and matrix decompositions all build on the idea that matrices *transform* spaces.

A 2×2 matrix might just look like four numbers, but remember: behind those numbers is a *complete blueprint* for reshaping an entire space.

---

## **Linear Transformations as Matrices**

**Did you know that if you know where a linear transformation sends just two special points, you can reconstruct its entire 2×2 matrix?** In 2D, these special points are the vectors (1,0) and (0,1), often called the **standard basis vectors**. Here’s how it works:

### **The Setup**

Suppose there’s an *unknown* 2×2 matrix M. You know that it transforms:

- (1, 0) into (3, -1)  
- (0, 1) into (2, 3)

Also, by definition of a *linear* transformation, (0, 0) always goes to (0, 0). Additionally, you might observe where (1, 1) goes—say (5, 2)—but it turns out you don’t even need that to identify the matrix!

### **Why Just (1, 0) and (0, 1)?**

In 2D space, any vector $(x, y)$ can be written as:  

$$
x \cdot (1, 0) + y \cdot (0, 1) = (x, y)
$$
  
Because the transformation is linear, it “respects” these kinds of combinations:  

$$
M \bigl[x \cdot (1, 0) + y \cdot (0, 1)\bigr]
= x \, M(1, 0) + y \, M(0, 1)
$$

So if you know exactly where (1, 0) and (0, 1) each go, you can figure out where **any** vector in 2D goes.

This is why (1, 0) and (0, 1) are called the **standard basis vectors** of the coordinate system.

#### **Quick Example**

Consider any 2D vector v = (3, 2). This vector can be expressed as:  

$$
3 \cdot (1, 0) + 2 \cdot (0, 1) = (3, 2)
$$

When we apply linear transformation M to this vector:

1. **Direct Way**  

$$
M(3, 2)
$$

2. **Using Basis Vectors**  

$$
M(3(1,0) + 2(0,1)) = 3M(1,0) + 2M(0,1)
$$

Both ways always give the same result! This is the "respect" property of linear transformations.

Let's say we have a linear transformation M where:
- M(1,0) = (2,1)
- M(0,1) = (1,2)

Computing the transformation of v = (3,2):  

$$
M(3,2) = 3M(1,0) + 2M(0,1) = 3(2,1) + 2(1,2) = (6,3) + (2,4) = (8,7)
$$

It's like building with LEGO blocks:
1. If you know how the basic blocks (basis vectors) transform
2. You can combine them appropriately
3. To find the transformation of any vector

#### **Why This Matters**

1. **Efficiency**: You only need to know the transformation of basis vectors, not every possible vector.

2. **Predictability**: You can predict the transformation result for any vector.

3. **Computer Implementation**: These properties allow linear transformations to be represented as matrices and computed efficiently.

This property of linear transformations is particularly important in machine learning. When each layer of a neural network performs a linear transformation, these properties enable efficient computation and make algorithms like backpropagation possible.

Now, Let's back to the problem.

### **Building the Matrix**

From our example:

- (1, 0) maps to (3, -1)
- (0, 1) maps to (2, 3)

In matrix form, the columns of M are simply the images of these basis vectors:  

$$
M = \begin{pmatrix}
| & | \\
M(1,0) & M(0,1) \\
| & |
\end{pmatrix}
= \begin{pmatrix}
3 & 2 \\
-1 & 3
\end{pmatrix}
$$

The first column  

$$
\begin{pmatrix}
3 \\
-1
\end{pmatrix}
$$

is where $(1,0)$ goes, and the second column  

$$
\begin{pmatrix}
2 \\
3
\end{pmatrix}
$$

is where $(0,1)$ goes.

### **Seeing It All Come Together**

- If you only know that (1,0) goes to (3, -1) and (0,1) goes to (2, 3), that’s enough to fill out the entire matrix.  
- Any other point—like (1,1) going to (5, 2)—just confirms the same linear rule:  

$$
M(1,1) = 1M(1,0) + 1M(0,1) = (3,-1) + (2,3) = (5,2)
$$

So turning a **linear transformation** back into a **matrix** is as simple as tracking where the **standard basis** vectors land. This is one of the most elegant aspects of linear algebra: *a few pieces of information can define an entire transformation*.

---

## **Matrix Multiplication**

**Have you ever needed to apply two different transformations—like a rotation and then a stretch—one right after the other?** In linear algebra, *multiplying matrices* is exactly how you “combine” (or *compose*) these transformations into one.

In previous sections, you learned how to multiply a matrix by a vector:  

$$
A \begin{pmatrix}x \\ y\end{pmatrix}
$$

Now, we’ll see how to multiply a **matrix by another matrix**. This isn’t just a new arithmetic rule—it’s also a powerful way to chain multiple linear transformations into a single operation.

### **Composition of Linear Transformations**

Imagine two linear transformations:

1. **First Transformation**: Described by a matrix  

$$
A = 
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
$$

- This takes any 2D vector, say $(x, y)$, and maps it to a new vector $A (x, y)$.  

- Geometrically, (1, 0) goes to (3, 1), and (0, 1) goes to (1, 2).

2. **Second Transformation**: Described by another matrix  

$$
B = 
\begin{pmatrix}
2 & -1 \\
0 & 2
\end{pmatrix}
$$
   - But it acts on the *transformed* coordinates after applying $A$ first.
   - So if $A$ sent $(1, 0)$ to $(3, 1)$, then $B$ acts on $(3, 1)$, sending it to some new point.

Putting these transformations **back to back** produces a *third* linear transformation that takes a vector all the way from the original plane to the final plane—skipping over the “middle” step. That third transformation **must** be representable by a matrix, say $C$. We say $C = BA$ (read from right to left!), because:

- First, you apply $A$ to a vector $\mathbf{v}$.
- Next, you apply $B$ to the result $A\mathbf{v}$.
- Overall: $\mathbf{v} \mapsto A\mathbf{v} \mapsto B(A\mathbf{v}) = (BA)\mathbf{v}$

Hence, matrix multiplication corresponds to composing the two transformations into a single one.

### **The 2×2 Matrix Multiplication Rule**

Let’s see it in action with the specific matrices:  

$$
A = \begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix},
\quad
B = \begin{pmatrix}
2 & -1 \\
0 & 2
\end{pmatrix}
$$

We want to find $BA$:  

$$
BA = 
\begin{pmatrix}
2 & -1 \\
0 & 2
\end{pmatrix}
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
$$

#### **Dot-Product Approach**

- Take each **row** of $B$ and each **column** of $A$, then do a dot product for each pair.

1. **Top-left entry**: (Row 1 of $B$) $\cdot$ (Column 1 of $A$)  

$$
(2, -1) \cdot (3, 1) = 2\cdot3 + (-1)\cdot1 = 6 - 1 = 5
$$

2. **Top-right entry**: (Row 1 of $B$) $\cdot$ (Column 2 of $A$)  

$$
(2, -1) \cdot (1, 2) = 2\cdot1 + (-1)\cdot2 = 2 - 2 = 0
$$

3. **Bottom-left entry**: (Row 2 of $B$) $\cdot$ (Column 1 of $A$)  

$$
(0, 2) \cdot (3, 1) = 0\cdot3 + 2\cdot1 = 2
$$

4. **Bottom-right entry**: (Row 2 of $B$) $\cdot$ (Column 2 of $A$)  

$$
(0, 2) \cdot (1, 2) = 0\cdot1 + 2\cdot2 = 4
$$

So,  

$$
BA 
= \begin{pmatrix}
5 & 0 \\
2 & 4
\end{pmatrix}
$$

Geometrically, **this** is the 2×2 matrix that sends $(1,\,0)$ straight to $(5,\,2)$, and $(0,\,1)$ straight to $(0,\,4)$—exactly matching the composition of the two transformations.

### **Dimension Rules for Matrix Multiplication**

Matrix multiplication isn’t limited to squares. For a product $AB$ to make sense:

1. The **number of columns** of $A$ must match the **number of rows** of $B$.
2. The **result** will have the same number of **rows** as $A$ and the same number of **columns** as $B$.

For instance, if $A$ is a **2×3** matrix and $B$ is a **3×4** matrix, then $AB$ is well-defined and produces a **2×4** matrix.

### **Example: 2×3 and 3×4**

Suppose we have:  

$$
A = \begin{pmatrix}
3 & 1 & 4 \\
2 & -1 & 2
\end{pmatrix}, 
\quad 
B = \begin{pmatrix}
3 & 0 & 1 & -2 \\
1 & 5 & 5 & 1 \\
-2 & 1 & 0 & 2
\end{pmatrix}
$$

- $A$ has 2 rows, 3 columns.
- $B$ has 3 rows, 4 columns.
- The “inner” dimensions (3 in both) match, so the product $AB$ is defined.
- The result is a 2×4 matrix.

Each entry of $AB$ is the dot product of a row from $A$ with a column from $B$. For example:

- **Bottom-left entry** of $AB$ = (Second row of $A$) $\cdot$ (First column of $B$):  

$$
(2,\,-1,\,2) \cdot (3,\,1,\,-2) 
= 2\cdot3 + (-1)\cdot1 + 2\cdot(-2) 
= 6 - 1 - 4 
= 1
$$

You follow the same pattern for all other entries, row by row, column by column, to fill out the final 2×4 matrix.

### **Key Takeaways**

- **Matrix Multiplication = Composing Transformations**: If $A$ and $B$ are square matrices (or even rectangular in a more general sense), then $BA$ is the one-step transformation combining first $A$ then $B$.  
- **Dot Products**: Each entry of the product is a dot product between a row of the first matrix and a column of the second.  
- **Dimension Rules**: For $A$ (size $m\times n$) and $B$ (size $n\times p$), the product $AB$ has size $m\times p$. If $n$ doesn’t match, you can’t multiply them.  

Understanding matrix multiplication opens the door to a huge world of applications: from **3D graphics** (where multiple transformations—rotate, then scale, then translate—are chained together) to **machine learning** (where large datasets are multiplied by weight matrices in neural networks).

As you continue, keep in mind this simple idea: *composing two linear operations is the same as multiplying their corresponding matrices.* That’s what makes matrix multiplication such an indispensable tool in AI, computer science, and beyond!

---

## **The Identity Matrix**

**What does the number 1 have in common with a special kind of matrix?** Both “do nothing” when multiplied by something else. Multiplying any number by 1 gives that same number back, and multiplying any matrix or vector by the **identity matrix** returns that original matrix or vector unchanged. 

### **The Role of “1” in Matrix Form**

If you have a regular number $x$ (like 7 or 3.5), then $1 \times x = x$. Similarly, if you have a matrix $A$ or a vector $\mathbf{v}$, then multiplying by the identity matrix $I$ leaves them as they are:  

$$
I \cdot A = A, 
\quad
I \cdot \mathbf{v} = \mathbf{v}
$$

### **What Does the Identity Matrix Look Like?**

In 2D, the identity matrix is  

$$
I_2 = 
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$

In 3D, you get  

$$
I_3 = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
$$

and so on for higher dimensions. Notice the pattern:
- **1s on the diagonal**  
- **0s everywhere else**

### **Verifying It “Does Nothing”**

Let’s say you have a 2D vector  

$$
\begin{pmatrix}a \\ b\end{pmatrix}
$$

Multiply it by $I_2$:  

$$
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
a \\
b
\end{pmatrix} = \begin{pmatrix}
1\cdot a + 0\cdot b \\
0\cdot a + 1\cdot b
\end{pmatrix} = \begin{pmatrix}
a \\
b
\end{pmatrix}
$$

This shows the vector remains the same, which is precisely what we mean by an *identity* operation.

### **Identity as a Linear Transformation**

Geometrically, the identity matrix is the transformation that **leaves every point in place**:

- $(0,0)$ maps to $(0,0)$  
- $(1,0)$ maps to $(1,0)$  
- $(0,1)$ maps to $(0,1)$  
- and so on for every possible coordinate.

No stretching, no rotation, no shifting—just an **unchanged plane**.

### **Why It Matters**

1. **Neutral Element**: In matrix multiplication, $I$ is the equivalent of “1” for numbers.  
2. **Foundation of Inverses**: To “undo” a transformation $A$, you look for a matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.  
3. **Computational Shortcut**: Checking if a matrix is the identity can be a quick test for whether any changes have (or have not) been made.

So the next time you see $I$, remember: it’s the quiet hero that holds everything in place, playing the same role as 1 does in ordinary multiplication.

---

## **Matrix Inverse**

**How do you “undo” a linear transformation?** Much like the multiplicative inverse of a number (e.g., the inverse of 2 is $1/2$), certain matrices have an *inverse* that, when multiplied with the original matrix, yields the **identity matrix**. In geometric terms, an inverse matrix is the one that *reverses* a transformation, returning you to your starting configuration.

### **Inverting a Transformation**

Imagine a matrix  

$$
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
$$

that warps a unit square into a parallelogram in the plane. There might be a second matrix that “unwarps” that parallelogram back into the original square. If we call the inverse matrix $A^{-1}$, then:  

$$
A \times A^{-1} = I
$$

where $I$ is the identity matrix (the matrix that leaves everything unchanged).

### **Notation and Definition**

- For numbers, the inverse of 2 is $2^{-1}$ = $\tfrac12$.  
- For matrices, if $A$ is invertible, we write its inverse as $A^{-1}$.  

Just like $2 \times \tfrac12 = 1$, we have:  

$$
A \times A^{-1} = A^{-1} \times A = I
$$

This means that applying the transformation $A$, followed by $A^{-1}$, does nothing overall—just like multiplying by 2 and then by $\tfrac12$ gets you back to 1.

### **Finding the Inverse by Solving a System**

Consider again the 2×2 matrix  

$$
A = \begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
$$

We seek a matrix  

$$
A^{-1} = 
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
$$

such that  

$$
A \times A^{-1} = 
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix}
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix} = \begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$

Equating the entries on both sides translates into four equations:

1. $3a + 1c = 1$  
2. $3b + 1d = 0$  
3. $1a + 2c = 0$  
4. $1b + 2d = 1$

Solving these yields $a = \tfrac{2}{5}$, $b = -\tfrac{1}{5}$, $c = -\tfrac{1}{5}$, $d = \tfrac{3}{5}$. Hence,  

$$
A^{-1} 
= \begin{pmatrix}
\tfrac{2}{5} & -\tfrac{1}{5} \\
-\tfrac{1}{5} & \tfrac{3}{5}
\end{pmatrix}
$$

### **When the Inverse Does (or Does Not) Exist**

- **Example of an Invertible Matrix**  
  Another matrix, say  
  
$$
\begin{pmatrix}
5 & 2 \\
1 & 2
\end{pmatrix}
$$

can also have an inverse. Solve the associated system of equations and you might find (for example)  

$$
a = \tfrac14,
b = -\tfrac14,
c = -\tfrac18,
d = \tfrac58.
$$

- **Example of a Non-Invertible Matrix**  
  Some matrices do *not* have inverses. Suppose  
  
$$
\begin{pmatrix}
1 & 1 \\
2 & 2
\end{pmatrix}
$$

Trying to find $A^{-1}$ leads to a contradictory system of equations (like $a + c = 1$ but also $2a + 2c = 0$). In such cases, there’s no matrix $A^{-1}$ satisfying $A \times A^{-1} = I$. We say **the matrix is singular** (or *non-invertible*).

### **Interpretation and Importance**

1. **Geometric Undoing**: An invertible matrix deforms space in some reversible way (e.g., rotates, shears, or scales but doesn’t collapse dimensions). The inverse transformation undoes that deformation.  
2. **Algebraic Condition**: For a 2×2 matrix  

$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
$$

- a quick way to check invertibility is the **determinant** $ad - bc$. If $ad - bc \neq 0$, the matrix is invertible; if $ad - bc = 0$, it’s not.  
3. **Everyday Relevance**: Inverse matrices are used in solving linear equations, cryptography (decoding messages), transformations in graphics, and much more.

### **Key Takeaway**

The inverse of a matrix—when it exists—is the matrix that brings you back to where you started, much like dividing by a nonzero number returns you to 1.

---

## **Neural Networks and Matrices**

**How do we use simple math (like dot products) to catch spam emails or perform logical operations?** In this section, we’ll see how one-layer neural networks—often called *perceptrons*—can classify inputs (like emails) by combining the ideas of matrices and thresholds.

### **A Mini “Spam Filter” Example**

Imagine you have a dataset of emails labeled “Spam” or “Not Spam.” Two words seem to pop up frequently in spam messages: “lottery” and “win.” You decide to build a **classifier** that counts how often these words appear and then decides whether an email is spam based on a *score*.

1. **Assign Scores**  
   - Give “lottery” a certain weight $w_{\text{lottery}}$  
   - Give “win” a certain weight $w_{\text{win}}$  
2. **Compute a Total Score**  
   - If an email has $L$ appearances of “lottery” and $W$ appearances of “win”, then  
   
$$
\text{Score} = (w_{\text{lottery}} \times L) + (w_{\text{win}} \times W)
$$

3. **Apply a Threshold**  
   - If Score > Threshold, the classifier marks the email as spam; otherwise, it’s considered not spam.

#### **Finding the Perfect Weights and Threshold**
From an example table of emails, you might discover that setting:
- $w_{\text{lottery}} = 1$  
- $w_{\text{win}} = 1$  
- Threshold = 1.5

correctly classifies every email in your tiny dataset. In other words, if the sum of “lottery” and “win” counts is more than 1.5, label it spam; otherwise, not spam.

### **Plotting the Classifier in 2D**

You can also visualize these emails on a 2D grid:
- The horizontal axis ($x$) = number of times “lottery” appears,
- The vertical axis ($y$) = number of times “win” appears.

With weights of 1 and 1, the “decision boundary” is the line  

$$
x + y = 1.5
$$

Points above or to one side of this line are classified as spam (Score > 1.5), and points below or on the other side are not spam (Score < 1.5). This is a **linear classifier**—the simplest form of neural network with a single layer.

### **From Dot Products to Bulk Predictions**

Instead of computing each email’s score separately, you can represent all emails (rows) and their “lottery”/“win” counts (columns) in a **matrix** $X$, and let your *model* be a 2D column vector:  

$$
\mathbf{w} = 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
$$

Then, to get every email’s score at once, compute:  

$$
X \times \mathbf{w}
$$

You’ll get a column vector of scores, one for each email. Finally, check if each score exceeds 1.5.

> **Scaling Up**  
> Have more words? Just widen the matrix $X$ (more columns) and the weight vector $w$ (more rows). The principle stays the same.

### **Threshold vs. Bias**

Sometimes, you’ll see the decision rule written with a *bias term* $b$ instead of a threshold. For example,  

$$
\text{Score} - \text{Threshold} \quad \text{vs.} \quad \text{Score} + b
$$

- If $b = -1.5$, then checking $\text{Score} + b > 0$ is equivalent to checking $\text{Score} > 1.5$.  
- In neural networks, *bias terms* are common: you might add an extra column of 1’s to $X$ and a corresponding entry in $\mathbf{w}$ that is negative for the threshold.

### **A Logical “AND” Operator**

A related mini-example is the **AND** function of two binary inputs $(x, y)$. The rule is: output **Yes** if and only if $x = 1$ and $y = 1$; otherwise **No**. If you treat $x$ and $y$ like “lottery” and “win” counts (just 0 or 1), the same approach works:
1. Use weights $w_x = 1$, $w_y = 1$,
2. Threshold = 1.5.

The only time $x + y$ is above 1.5 is when both $x$ and $y$ are 1. This exactly mimics the AND operator.

### **A Simple Perceptron**

You can depict this single-layer network as follows:

```
x (input) --- w1 ---\
                     --> Dot Product + b --> Activation --> Output
y (input) --- w2 ---/
```

1. **Inputs**: $x$ and $y$ are multiplied by their weights $w_1$ and $w_2$.  
2. **Sum & Bias**: Add them together and include a bias $b$ (which could be -1.5 in our examples).  
3. **Activation Check**: If the sum $\ge 0$, output 1 (Yes); otherwise 0 (No).

This structure is the foundation of more advanced neural networks. Just stack more layers, add more neurons, or use different activation functions, and you get the building blocks of modern AI systems.

### **Key Takeaways**

- **Linear Classifier**: A single-layer network with a dot product and threshold (or bias) is enough to separate data with a simple *line* (or hyperplane in higher dimensions).  
- **Matrix Multiplication**: Handling many data points at once is just a matter of multiplying a feature matrix by a weight vector.  
- **Scalability**: Adding more words (features) or more layers in the neural network extends the same principle—dot products plus activation checks.

This completes our exploration of how a basic neural network can classify simple inputs using *linear* math. Next steps might include learning about different activation functions (like ReLU or sigmoid) or more layers, which can tackle more complex patterns and datasets!