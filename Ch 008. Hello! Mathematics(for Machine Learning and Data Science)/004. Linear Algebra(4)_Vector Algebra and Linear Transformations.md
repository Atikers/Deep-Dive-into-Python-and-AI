# ***Vector Algebra***

## **Machine Learning Motivation**

**Have you ever wondered how a simple line—like the ones you see in high school algebra—could help predict the future?** For instance, imagine you want to anticipate how many hours a plant might grow given certain amounts of water and sunlight. It turns out that we can often represent problems like this with straightforward equations and then apply a branch of mathematics called *linear algebra* to make sense of them. 

### A Quick Overview of Vectors and Matrices

To warm up, recall how you handle everyday numbers: you can add, subtract, multiply, and even invert them (for example, the inverse of 2 is 1/2). With vectors and matrices—just like with single numbers—you can perform similar operations. You can add two vectors, multiply a vector by a matrix, or even find the inverse of a matrix under specific conditions.

But why are these operations so important for machine learning? One big reason is that any dataset can be expressed in terms of rows and columns—think of rows as individual examples and columns as different features of the data. Matrices and vectors are the natural building blocks for this representation.

### Linear Regression as a Motivating Example

Early in your journey with machine learning, you might encounter **linear regression**. In linear regression, you assume your data roughly follows a relationship that looks like:  

$$
y = W \cdot X + b
$$

- $X$ is the matrix of *features*. Each row in $X$ could be one example (like one plant in your garden), and each column is a particular characteristic (like water input, sunlight hours, etc.).
- $W$ is a vector (or sometimes a matrix) of *weights*, which are the values the model adjusts to fit the data. You can think of them like knobs that the model tweaks to better match reality.
- $b$ (the *bias*) is a constant (or sometimes a small vector) that shifts the output up or down.

If you have multiple examples—say, $m$ plants—then $y$ might be another vector of length $m$, capturing the predicted growth for each plant. Notice how the equation $W \cdot X + b$ is just a more sophisticated version of the line equation you studied in algebra class ($y = mx + c$). 

In practice, real-world data are often *not* perfectly linear, so we rarely solve these equations as a simple one-shot. Instead, algorithms iteratively adjust $W$ and $b$ to best fit the data. Even so, the linear model remains a powerful starting point and is surprisingly effective for many tasks.

### The Leap to Neural Networks

What if your data is *highly nonlinear*—say you’re tracking not just water and sunlight but also soil chemistry, temperature variations, and genetic factors? In that case, a single linear equation might not capture the complexity of your plant growth data. **Neural networks** step in to handle these richer relationships.

Despite their fancy diagrams, neural networks are essentially a **large collection of linear equations** layered together. You can imagine each layer of a neural network as a transformation:  

$$
\text{Layer Output} = \sigma\bigl(W^{[l]} \cdot (\text{Previous Layer}) + b^{[l]}\bigr)
$$

where:
- $W^{[l]}$ is the set of weights for layer $l$,
- $b^{[l]}$ is the bias term for layer $l$,
- $\sigma(\cdot)$ is the *activation function* that injects nonlinearity into the model,
- and “Previous Layer” is just another vector or matrix of data outputs from the prior step.

If you’ve taken a physics or engineering class, you might see an analogy in how multiple filters or transformations act on a signal in sequence. Each layer in the network filters or alters its inputs slightly differently, and together they reveal more complex patterns than any one filter could on its own.

### Why Care About Linear Algebra Here?

All these steps—multiplying inputs by weights, adding biases, stacking layers—boil down to operations on vectors and matrices. Instead of writing dozens (or thousands!) of equations separately, you can represent them compactly with linear algebra. This is exactly why understanding vectors, matrices, and their properties is so crucial to machine learning. 

1. **Efficiency**: Matrix operations can be computed quickly, especially on modern hardware like GPUs.
2. **Clarity**: One matrix equation can replace a huge block of separate equations, making ideas easier to see and manipulate.
3. **Generality**: From linear regression to neural networks to advanced AI models, matrices and vectors show up everywhere.

### Looking Ahead

*"004. Linear Algebra(4)_Vector Algebra and Linear Transformations"* section focuses on:
- Understanding the geometry of vectors (magnitude and direction).
- Learning about matrix operations (such as multiplication and finding inverses).
- Exploring the concept of **linear transformations**, which is a powerful way to visualize what matrices do.

Ultimately, whether you’re constructing a simple linear regression model or designing a cutting-edge neural network, you’ll keep returning to the same essential *linear building blocks*. Mastering them now will make the rest of your AI journey much smoother.

We’ll explore vectors, matrices, and linear transformations step by step. Don’t worry if some parts feel abstract. Remember, each concept you learn here will have a place in real-world applications—be it in predicting plant growth, diagnosing medical images, or creating art with machine learning.

Let’s start exploring these powerful ideas!

---

## **Vectors and their Properties**

**How can a simple list of numbers help us measure distance—whether we’re walking through city blocks or flying in a helicopter?** That’s exactly what vectors do for us. A vector can be thought of as an arrow on a plane (2D) or in a higher-dimensional space (3D, 4D, and so on). In any dimension, a vector has two key attributes:

1. **Magnitude (Size):** How “long” the arrow is.  
2. **Direction:** Where the arrow is pointing.

Below, we’ll explore how these attributes are defined and why they’re so useful.

### **What Is a Vector?**

A vector is simply an ordered list (or *tuple*) of numbers. If you have a vector with two coordinates, like (4, 3), you can picture it in the 2D plane as an arrow pointing to the point (4, 3) starting from the origin (0, 0). If you have a 3D vector like (4, 3, 1), you can visualize it in three-dimensional space.

The number of coordinates in a vector is called its **dimension**.  
- 2D vectors have the form $(x_1, x_2)$.  
- 3D vectors have the form $(x_1, x_2, x_3)$.  
- In general, $n$-dimensional vectors have the form $(x_1, x_2, \dots, x_n)$.

### **Magnitude: Taxicab vs. Helicopter Distance**

Consider a 2D vector (a, b). One way to measure its size (or *magnitude*) is to imagine walking through a city grid:

- **Taxicab Distance (L1-norm)**  
  Think of city blocks: if you can only travel along streets going north-south or east-west, the distance between (0, 0) and (a, b) is simply  
  
$$
|a| + |b|
$$

  This is called the **L1-norm** of the vector. We use absolute values because distance must be positive whether $a$ or $b$ is negative or positive.

- **Helicopter Distance (L2-norm)**  
  If you have a helicopter, you can take a direct flight from $(0, 0)$ to $(a, b)$. According to the Pythagorean theorem, the distance is:  
  
$$
\sqrt{a^2 + b^2}
$$  

  This is called the **L2-norm** of the vector, and it matches our usual notion of “length.” Unless otherwise specified, when we say “the norm” of a vector, we usually mean the L2-norm.

> **Taxicab distance** is how far you must drive on a grid of streets.  
> **Helicopter distance** is a straight line that cuts across everything.

#### **Generalizing to Higher Dimensions**

For an $n$-dimensional vector  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n),
$$

the two norms extend naturally:

- **L1-norm:**  
  
$$
\|\mathbf{x}\|_1 = |x_1| + |x_2| + \dots + |x_n|
$$

- **L2-norm:**  

$$
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}
$$

### **Direction of a Vector**

Along with how large a vector is, its **direction** is equally important. For a 2D vector (4, 3), you can find the angle $\theta$ it makes with the horizontal $x$-axis by noting that  

$$
\tan(\theta) = \frac{3}{4}
$$

Hence,  

$$
\theta = \arctan\!\bigl(\tfrac{3}{4}\bigr)
$$

which works out to roughly 0.64 radians (or about $36.87^\circ$).

Two vectors may have different magnitudes but still point in the same direction. For instance, (2, 1.5) points the same way as (4, 3)—just half the “length.”

### **Vector Notation**

There are many valid ways to write a vector:

1. **Row vector:** $(x_1, x_2, \dots, x_n)$  
2. **Column vector:**  

$$
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
$$

3. **Brackets:** $[x_1, x_2, \dots, x_n]$

Also, different resources may write a vector name in bold (e.g. **x**) or with a little arrow above it (e.g. $\vec{x}$). This chapter uses a simple italic $x$ with subscripts for its components, such as $x_1$, $x_2$, and so on.

The notation might look different, but the underlying idea is the same: a vector is just a collection of numbers.

### **Key Takeaways**

- A vector’s **magnitude** (or *norm*) can be measured in more than one way; the most common is the L2-norm, which corresponds to the direct “as-the-crow-flies” distance.  
- The **direction** of a vector in 2D can be found via trigonometry, and similar principles extend to higher dimensions with more advanced geometry.  
- Vectors can be written in multiple formats, but the concept remains the same: they are building blocks of much of the math in AI and machine learning.

Next, we’ll explore how to perform arithmetic with vectors and see how these operations give us insights into complex datasets and transformations. Let’s get started!

---

## **Vector Operations**

**What happens when you combine two arrows—do they just magically give you another arrow?** In fact, they do! This is one of the central ideas of *vector operations*: you can add and subtract vectors, and even stretch or flip them by multiplying by scalars. Let’s see how this works step by step.

### **Adding and Subtracting Vectors**

Suppose you have two 2D vectors:  

$$
\mathbf{u} = (4, 1), \quad \mathbf{v} = (1, 3)
$$

The **sum** of these vectors is found by adding their components:  

$$
\mathbf{u} + \mathbf{v} = (4 + 1, 1 + 3) = (5, 4)
$$

Geometrically, if you draw both $\mathbf{u}$ and $\mathbf{v}$ starting from the same point, their sum $\mathbf{u} + \mathbf{v}$ is the diagonal of the parallelogram whose sides are $\mathbf{u}$ and $\mathbf{v}$. If you imagine walking along one arrow and then turning to follow the second arrow, you end up at the tip of the summed vector.

Similarly, the **difference** between two vectors—say, $(4, 1)$ minus $(1, 3)$—is calculated by subtracting each component:  

$$
(4, 1) - (1, 3) = (4 - 1, 1 - 3) = (3, -2)
$$

Here, the result $(3, -2)$ corresponds to the other diagonal of the parallelogram. You can think of it as the arrow you’d travel if you start at $(1, 3)$ and move to negative direction of $(4, 1)$.

#### **General Form**

For two $n$-dimensional vectors  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n) 
\quad\text{and}\quad 
\mathbf{y} = (y_1, y_2, \dots, y_n),
$$

the operations are:

- **Addition:**  

$$
\mathbf{x} + \mathbf{y} 
= (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n) 
$$

- **Subtraction:**  

$$
\mathbf{x} - \mathbf{y} 
= (x_1 - y_1, x_2 - y_2, \dots, x_n - y_n)
$$

Both vectors must have the same dimension for these operations to make sense.

### **Distances Between Vectors**

In many machine learning tasks(think about clustering, recommendation systems, etc.), we care about **how far apart** two vectors are. For example, consider the vectors $(1, 5)$ and $(6, 2)$:

1. **L1 Distance:**  
   Also called the taxicab distance, it’s the sum of the absolute differences of each component:  
   
$$
\|\,(1, 5) - (6, 2)\,\|_1
= |1 - 6| + |5 - 2|
= 5 + 3
= 8
$$

2. **L2 Distance:**  
   This is the straight-line (Euclidean) distance:  
   
$$
\|\,(1, 5) - (6, 2)\,\|_2
= \sqrt{(1-6)^2 + (5-2)^2}
= \sqrt{(-5)^2 + 3^2}
= \sqrt{25 + 9}
= \sqrt{34}
\approx 5.83
$$

These distance measures help quantify “how different” or “how similar” points are in a dataset—an essential concept in clustering, nearest-neighbor searches, and many other AI applications.

### **Scalar Multiplication**

Another fundamental vector operation is **multiplying by a scalar** (a single number). If $\lambda$ is a scalar and  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n)
$$

then  

$$
\lambda \mathbf{x} 
= (\lambda x_1, \lambda x_2, \dots, \lambda x_n)
$$

- **Stretching (Positive Scalar):**  
  If $\lambda > 0$, you lengthen(or shrink) the vector **without changing its direction**. For instance,  
  
$$
3 \cdot (1, 2) = (3, 6)
$$

  which is just the original vector stretched by a factor of 3.

- **Flipping (Negative Scalar):**  
  If $\lambda < 0$, you both stretch and **reverse** the direction of the vector. For example,  
  
$$
-2 \cdot (1, 2) = (-2, -4)
$$

  has the same line of action but is pointing in the opposite direction—and is doubled in length.

You can think the word **scalar** as **scale + -ar**, then you can grasp the meaning of **scalar multiplication** as the operation of **scaling** the arrow.

### **Why These Operations Matter**

1. **Geometric Insight:** Adding vectors visually gives parallelograms or triangles, providing intuition for more complex concepts like transformations and rotations.
2. **Measuring Similarity:** Subtraction and norms (distances) are critical in machine learning to gauge how alike two data points are.  
3. **Scaling Data:** Scalar multiplication is frequently used to normalize or scale vectors, especially when preparing datasets.

As you continue your study, you’ll see these operations come up repeatedly. They form the foundation not just for understanding linear models but for everything from simple regressions to the most complex neural network layers.

**Next up:** We’ll delve deeper into dot products (and how they relate to angles between vectors) and then see how multiplying matrices by vectors can transform your data in powerful ways. Let’s keep building!

---

## **The Dot Product**

**Have you ever added up your grocery list for apples, bananas, and cherries, just by multiplying quantity by price and then summing everything up?** Congratulations—you’ve already performed a *dot product*! Although the example may seem trivial, the dot product is a foundational operation that appears everywhere in linear algebra and AI.

### **A Real-Life Fruit Example**

Imagine you bought:

- 2 apples, each costing \$3
- 4 bananas, each costing \$5
- 1 cherry, each costing \$2

You can represent the *quantities* of these fruits as a column vector:  

$$
\begin{pmatrix}
2 \\
4 \\
1
\end{pmatrix}
$$

and the *prices* as another column vector:  

$$
\begin{pmatrix}
3 \\
5 \\
2
\end{pmatrix}
$$

To find the total cost, you multiply each pair of corresponding entries and add them up:  

$$
2 \times 3 + 4 \times 5 + 1 \times 2 = 6 + 20 + 2 = 28
$$

This summation of products **is** the dot product. Often in mathematics, you’ll see it written in a more compact notation, for example:  

$$
(2, 4, 1) \cdot (3, 5, 2) = 28
$$

When we arrange one vector as a *row* and the other as a *column*, we get a 1×1 matrix (essentially a single number). However, the order doesn’t really change the essence of the calculation—you’re always pairing up components and summing their products.

### **Connecting the Dot Product to the Norm**

Let’s revisit an earlier vector:  

$$
\mathbf{u} = (4, 3)
$$

We know its **L2 norm** (Euclidean length) is:  

$$
|\mathbf{u}|^2 = \sqrt{4^2 + 3^2} = \sqrt{25} = 5
$$

Notice that $4^2 + 3^2$ can be seen as $\mathbf{u} \cdot \mathbf{u}$—the dot product of the vector with itself. Formally:  

$$
|\mathbf{u}|^2 = \sqrt{\mathbf{u} \cdot \mathbf{u}}
$$

This relationship holds for vectors in any dimension: the Euclidean norm is the square root of the dot product of a vector with itself.

### **Transpose: Turning Columns into Rows (and Vice Versa)**

Sometimes, you’ll see one vector written as a *row* and the other as a *column*, like so:  

$$
(2, 4, 1)  
\quad \text{and} \quad 
\begin{pmatrix}
3 \\
5 \\
2
\end{pmatrix}
$$

To switch between these forms, you use the **transpose** operation, denoted by a superscript $T$. For example,

$$
\begin{pmatrix}
2 \\
4 \\
1
\end{pmatrix}^T 
= (2, 4, 1)
$$

- **Row vector** $\rightarrow$ **Column vector**  
- **Column vector** $\rightarrow$ **Row vector**

#### **Matrix Transpose**

The idea generalizes beyond vectors to full matrices. For a 3×2 matrix,  

$$
A = 
\begin{pmatrix}
2 & 5 \\
4 & 7 \\
1 & 3
\end{pmatrix}
$$

its **transpose** $A^T$ is a 2×3 matrix:  

$$
A^T = 
\begin{pmatrix}
2 & 4 & 1 \\
5 & 7 & 3
\end{pmatrix}
$$

You swap rows and columns: the first column (2, 4, 1) becomes the first row, and the second column (5, 7, 3) becomes the second row.

### **Formal Definition of the Dot Product**

If you have two $n$-dimensional vectors  

$$
\mathbf{x} = (x_1, x_2, \dots, x_n), 
\quad
\mathbf{y} = (y_1, y_2, \dots, y_n),
$$

their **dot product** is  

$$
\mathbf{x} \cdot \mathbf{y} 
= x_1y_1 + x_2y_2 + \dots + x_ny_n
$$

You’ll also see alternate notations, such as $\langle \mathbf{x}, \mathbf{y} \rangle$. The essential idea, though, is always the same: *multiply corresponding components and sum them up*.

### **Why the Dot Product Matters**

1. **Computational Efficiency**: With vectors (and later matrices), dot products let us compactly represent tasks like summing up weighted features—crucial in machine learning.  
2. **Geometric Insight**: Beyond the grocery example, the dot product ties closely to concepts like length (norm) and angles between vectors.  
3. **AI and ML Applications**: Almost every neural network layer uses dot products as part of its internal calculations.  

In essence, the dot product is the *workhorse* of linear algebra and a powerful tool in understanding data operations in AI. Once you see how elegantly it simplifies complex problems—like summing your grocery bills—you’ll realize how indispensable it is across many fields of science, engineering, and beyond.

Next, we’ll dive into **geometric interpretations** of the dot product, exploring how it relates to the angle between two vectors and how these ideas expand to more advanced applications in AI. Let’s keep going!

---

## **Geometric Dot Product**

**Ever wonder how to tell if two arrows in space are pointing orthogonally, sharply toward each other, or almost in the same direction?** The **dot product** holds the key, linking the geometry of vectors to their algebraic operations.

### **Orthogonality (Perpendicular Vectors)**

Two vectors are **orthogonal** (or *perpendicular*) if and only if their dot product is zero. For example, consider the 2D vectors  

$$
(-1, 3) 
\quad\text{and}\quad 
(6, 2)
$$

The dot product is:  

$$
(-1) \times 6 + 3 \times 2 = -6 + 6 = 0
$$

showing they are indeed perpendicular.

**Key Fact**:  

$$
\langle \mathbf{u}, \mathbf{v} \rangle = 0 
\quad\Longleftrightarrow\quad 
\mathbf{u} \perp \mathbf{v}
$$

### **Dot Product and Vector Magnitude**

Remember that the dot product of a vector with itself equals the square of its norm. If $\mathbf{u}$ is any vector, then

$$
\langle \mathbf{u}, \mathbf{u} \rangle 
= |\mathbf{u}|^2
$$

This is just another way of saying $u_x^2 + u_y^2 + \dots$ (in higher dimensions) is the square of the vector’s length.

### **Angle Between Two Vectors**

The dot product can also measure *how aligned* two vectors are. If $\mathbf{u}$ and $\mathbf{v}$ are two vectors making an angle $\theta$ with each other, then:  

$$
\langle \mathbf{u}, \mathbf{v} \rangle 
= |\mathbf{u}||\mathbf{v}| \cos(\theta)
$$

- If $\theta < 90^\circ$ (vectors point in the *same* direction), $\cos(\theta)$ is positive, so the dot product is positive.
  - if $\theta = 0^\circ$, $\cos(\theta) = 1$, so the dot product is $|\mathbf{u}||\mathbf{v}|$.
- If $\theta = 90^\circ$ (vectors are *perpendicular*), $\cos(\theta) = 0$, so the dot product is 0.  
- If $\theta > 90^\circ$ (vectors point in *opposite-ish* directions), $\cos(\theta)$ is negative, so the dot product is negative.

### **Interpreting Positive, Negative, and Zero Dot Products**

Let’s look at one reference vector, $\mathbf{u} = (6, 2)$, and see how the sign of $\mathbf{u} \cdot \mathbf{v}$ changes depending on $\mathbf{v}$:

1. **$\langle \mathbf{u}, \mathbf{v} \rangle > 0$:**  
   The angle between $\mathbf{u}$ and $\mathbf{v}$ is less than $90^\circ$, meaning $\mathbf{v}$ lies in a “forward” region with respect to $\mathbf{u}$.  
   - Example: $\mathbf{v} = (2,\,4)$ yields a dot product of $6 \times 2 + 2 \times 4 = 20$, which is *positive*.

2. **$\langle \mathbf{u}, \mathbf{v} \rangle = 0$:**  
   The vectors are orthogonal (perpendicular).  
   - Example: $\mathbf{v} = (-1,\,3)$ gives $6 \times (-1) + 2 \times 3 = 0$.

3. **$\langle \mathbf{u}, \mathbf{v} \rangle < 0$:**  
   The angle between $\mathbf{u}$ and $\mathbf{v}$ is greater than $90^\circ$, so $\mathbf{v}$ falls in a “behind” region with respect to $\mathbf{u}$.  
   - Example: $\mathbf{v} = (-4,\,1)$ produces $6 \times (-4) + 2 \times 1 = -22$.

Visually, you can think of a circle around $\mathbf{u}$:  
- Vectors pointing generally in the same direction (within 90°) give a *positive* dot product.  
- Vectors pointing *exactly* perpendicular give a dot product of *zero*.  
- Vectors pointing more than 90° away give a *negative* dot product.

---

### **Why This Matters**

- **Machine Learning & AI:** Knowing whether two vectors (data points) are “aligned” or “opposite” can help measure similarity (e.g., in recommendation systems or text analysis).  
- **Geometry & Transformations:** Projections and angles pop up in image processing, physics, robotics, and more.  
- **Neural Networks:** **Each neuron’s output is a dot product of weights and inputs**, so understanding dot products helps demystify neural computations.

When we talk about “closeness” or “similarity” between vectors, the dot product (and related concepts like cosine similarity) offers powerful insight. Keep this geometric view in mind as we move on to other matrix operations and transformations—those, too, will build on the dot product’s fundamental ideas.

---

## **Multiplying a Matrix by a Vector**

**Have you ever tried solving several linear equations at once and noticed the same set of variables shows up in each equation?** Matrices and vectors give us a neat way to handle these systems all at once, rather than dealing with each equation separately.

### **A System of Equations in Matrix Form**

Suppose we have three equations:

1. $a + b + c = 10$  
2. $a + 2b + c = 15$  
3. $a + b + 2c = 12$

We can think of each line as a **dot product**:

- $(1, 1, 1) \cdot (a, b, c) = 10$
- $(1, 2, 1) \cdot (a, b, c) = 15$
- $(1, 1, 2) \cdot (a, b, c) = 12$

Notice that the *right-hand vector*—(a, b, c)—is the same in all three dot products. The only differences are the **row vectors** on the left-hand side. If we stack those row vectors on top of each other, we get a **3×3 matrix**:  

$$
\begin{pmatrix}
1 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{pmatrix}
$$

Then the system above can be compactly written as:  

$$
\begin{pmatrix}
1 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{pmatrix}
\begin{pmatrix}
a \\
b \\
c
\end{pmatrix} = \begin{pmatrix}
10 \\
15 \\
12
\end{pmatrix}
$$

### **Matrix-Vector Product as “Stacked Dot Products”**

Each row of the matrix performs a dot product with the column vector $(a, b, c)$, and the outcome is a **single number**. Because there are three rows in the matrix, we end up with three such dot products—one for each equation—giving us the final **3×1 vector** of results.

In general, a matrix can have *any number of rows and columns*, as long as the number of **columns** in the matrix matches the **dimension** of the vector. For example, a 4×3 matrix can still multiply a 3×1 vector, producing a 4×1 result:  

$$
\begin{pmatrix}
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot
\end{pmatrix}_{4 \times 3}
\begin{pmatrix}
\cdot \\
\cdot \\
\cdot
\end{pmatrix}_{3 \times 1} = \begin{pmatrix}
\cdot \\
\cdot \\
\cdot \\
\cdot
\end{pmatrix}_{4 \times 1}
$$

The key is that the “inner” dimensions must match—here, “3” in both cases. The “outer” dimensions (4 and 1) determine the shape of the result.

### **Why It Matters**

- **Simplicity**: Writing multiple equations in matrix form saves space and reveals patterns more clearly.  
- **Generalization**: Many problems in AI and machine learning involve large systems where each row can represent an example, and each column a feature.  
- **Computational Speed**: Computers (and libraries like NumPy) are optimized for matrix–vector multiplications, making data processing much faster.

This matrix–vector multiplication lays the groundwork for more advanced linear algebra operations you’ll see in upcoming sections—like matrix–matrix multiplication and finding inverses. But as you practice, keep in mind this core idea: a **matrix multiplied by a vector is just a collection of dot products**, each row acting on that vector in turn.

---

# ***Linear Transformations***

## **Matrices as Linear Transformations**

---

## **Linear Transformations as Matrices**

---

## **Matrix Multiplication**

---

## **The Identity Matrix**

---

## **Which Matrices Have an Inverse?**

---

## **Neural Networks and Matrices**