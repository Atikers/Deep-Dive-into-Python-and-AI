# ***Linear Algebra(2) - Solving Systems of Linear Equations - Elimination***

## **Solving Non-Singular Systems of Linear Equations**

**Have you ever tried to figure out the cost of different items at a grocery store when you only know their combined prices in certain bundles?** For instance, “Two apples and one banana cost \$10, but one apple and two bananas cost \$12.” How do we tease out the cost of each individual item from this information alone?

As you learned in the previous chapter, a *non-singular* system is one that has exactly one unique solution—like finding one specific value for the price of an apple and one specific value for the price of a banana.

### **Why Elimination?**
Solving a system of linear equations often involves the **elimination** method. The idea is to methodically combine or modify equations so that one variable is “eliminated” from an equation, allowing you to solve for the other variable(s) more easily. It’s similar to how, in physics, we try to balance forces by canceling them out to see what net force remains.

### **Simple Example (Apples and Bananas)**
Imagine we have:
- $a + b = 10$  
- $a + 2b = 12$

Here, $a$ and $b$ might represent the cost of apples and bananas, respectively. By comparing these two equations directly, you see that going from the first purchase (apple + banana) to the second purchase (apple + 2 bananas) adds exactly one more banana at an extra \$2, implying $b = 2$. Plugging $b = 2$ into $a + b = 10$ gives $a = 8$. Thus, the solution is:
- $a = 8$
- $b = 2$

When you arrive at solutions like $a = 8$ and $b = 2$, you’ve created what’s sometimes called a “solved system,” where each variable is explicitly identified.

### **General Steps for Elimination**
Let’s jump to a slightly more involved system:
1. **Start with two equations:**
   - $5a + b = 17$
   - $4a - 3b = 6$
   
2. **Divide to simplify (optional):** Sometimes, you can divide each equation by the coefficient of $a$ (as long as it’s not zero) so that $a$ has a coefficient of 1 in each equation. For instance:
   - $a + 0.2b = 3.4$
   - $a - 0.75b = 1.5$
   
   This step isn’t mandatory, but it often makes the math a bit cleaner.

3. **Eliminate one variable:** Subtract (or add) equations to remove $a$. Subtracting the first from the second:
   - $(a - 0.75b) - (a + 0.2b) = 1.5 - 3.4$
   - $a - a = 0$
   - $-0.75b - 0.2b = -0.95b$
   - $1.5 - 3.4 = -1.9$
   
   So you get $-0.95b = -1.9$. Solving for $b$:
   - $b = \dfrac{-1.9}{-0.95} = 2$

4. **Back-substitute to find the other variable:** Once $b = 2$, use either original equation (or the simplified one) to solve for $a$. From $a + 0.2b = 3.4$:
   - $a + 0.2 \times 2 = 3.4$
   - $a + 0.4 = 3.4$
   - $a = 3$

Hence, the solution is $a = 3$, $b = 2$. That’s the single unique pair that works—meaning this system is **non-singular**.

### **What if a Coefficient is Zero?**
Consider another scenario:
- $5a + b = 17$
- $3b = 6$

Here, the second equation has no $a$ term (we can think of the coefficient for $a$ as 0). You can’t divide by 0 to force $a$ into having a coefficient of 1. But this is actually a stroke of luck—$a$ is already eliminated in the second equation! From $3b = 6$, it’s immediate that $b = 2$. Then substitute $b = 2$ back into the first equation to find $a = 3$ again.

### **Key Takeaways**

**The elimination method** is like carefully orchestrating a duet in music : You merge and modify separate “melodies” (equations) so that one “voice” (variable) fades out, letting the other notes shine until the entire piece (the solution) becomes clear. This method underpins many AI algorithms that involve large systems of equations—because if we can systematically eliminate and solve for variables on a small scale, we can extend these ideas to massive datasets and hundreds or thousands of unknowns.

---

## **Solving Singular Systems of Linear Equations**

**Have you ever tried to solve two problems that seemed almost the same—so similar, in fact, that they gave you no new information to actually pinpoint a single answer?**

That scenario describes a **singular system** of linear equations. A singular system does *not* have a unique solution. Instead, it either has infinitely many solutions or none at all.

### **When There Are Infinitely Many Solutions**

Consider this system:

- $a + b = 10$  
- $2a + 2b = 20$

At first glance, these look like two different equations. However, the second one is just two times the first. In practical terms, if you multiply the first equation by 2, you get $2a + 2b = 20$, which is exactly the second equation. This means the second equation offers **no additional information** about $a$ and $b$. 

1. If you try to eliminate $a$ by subtracting one equation from the other, you might end up with:
   - $0 = 0$

   This statement is always true but does nothing to help you solve for $a$ or $b$.  

2. As a result, you have one equation ($a + b = 10$) for two unknowns. This is like saying “any pair of $a$ and $b$ that adds up to 10 works.” If you pick $a = 3$, then $b$ must be $7$; pick $a = 4.5$, then $b = 5.5$, and so on.

This situation describes a single line (for two variables) on a coordinate plane. Every point on the line satisfies the equation, giving you infinitely many solutions.

Imagine you have a circuit with two equations that are essentially the same measurement repeated. You can’t pinpoint the exact current and voltage; you only know they must fit one single relationship.

### **When There Are No Solutions**

Another possibility is that a singular system is **contradictory**—the equations demand conditions that can’t be satisfied at the same time. For example:

- $a + b = 10$  
- $2a + 2b = 24$

Again, you might multiply the first equation by 2 to compare with the second. But:

- First equation $\times 2$ becomes $2a + 2b = 20$  
- Second equation is $2a + 2b = 24$

These two lines look parallel in geometry. Subtracting the first from the second might yield something like $0 = 4$, which is impossible. Thus, no pair $(a, b)$ can satisfy *both* equations simultaneously.

If you claim, “A pen and a notebook cost \$10 together,” but then say, “Twice that exact same set costs \$24,” you’re clearly dealing with contradictory statements. No real prices of a pen and a notebook can make both conditions true.

### **Key takeaways**

- **Singular systems** do not yield a single unique solution. They either have:
  1. **Infinitely many solutions** (redundant equations describing the same relationship).
  2. **No solutions** (contradictory equations, like parallel lines that never intersect).
- A telltale sign of a singular system is when one equation can be obtained by multiplying or adding constants to the other.  
- In many AI and data problems, singularities can occur if your data or constraints overlap too much (redundant information) or if there’s a direct contradiction in the data.  
- Recognizing whether your system is singular or non-singular (unique solution, infinite solutions, or none) is crucial before applying more complex AI algorithms.

---

## **Solving System of Linear Equations with more variables**

**Ever tried to juggle three different unknowns—like tracking three expenses in a budget—and only had a few combined totals?** How do you figure out each individual cost without making random guesses?

Extending what we’ve done for two variables, we can solve **three (or more) variables** using similar elimination techniques. Think of each equation as describing a plane in 3D space. When these planes intersect at a single point, that point represents the unique solution to all three equations. 

### **The General Approach**

Let’s say we have a system of three equations in three unknowns $a$, $b$, and $c$. For example:

1. $6a + 2b + c = 25$
2. $2a + 4b + 3c = 28$
3. $3a + b + 5c = 33$

The steps to solve can mirror what we do with two variables, but in a slightly extended way:

1. **Focus on one variable (say $a$) and isolate it in the first equation.**  
   - Sometimes, you might divide the entire first equation by the coefficient in front of $a$, so that it becomes $a + (\text{something}) = (\text{some value})$.  
2. **Eliminate $a$ from the remaining equations.**  
   - Subtract or add multiples of the “new first equation” from the second and third equations to remove the $a$ term.  
3. **Solve the resulting two-variable system (now just $b$ and $c$).**  
4. **Once you have $b$ and $c$, substitute back** into one of the earlier equations to solve for $a$.  

### **Detailed Example**

Suppose we have:

1. $4a + 2b + c = 25$
2. $2a + 4b + 2c = 28$
3. $6a + 3b + 2c = 38$

**Step A: Normalize the first equation (optional).**  
- Divide the entire first equation by $4$ (the coefficient of $a$):
  - $a + 0.5b + 0.25c = 6.25$

**Step B: Eliminate $a$ from the second and third equations.**  
- Subtract the new first equation (appropriately scaled if needed) from the second and third equations to ensure $a$ is removed there.

**Step C: Solve the two resulting equations in $b$ and $c$.**  
- You’ll end up with something like:  
  - (Equation in $b$ and $c$)  
  - (Another equation in $b$ and $c$)

**Step D: Back-substitute to find $a$.**  
- Once you have $b$ and $c$, put them into one of the earlier forms (like the normalized first equation) to get $a$.

This procedure might sound mechanical, but it’s actually very powerful—it's the bedrock of many algorithms in AI and data science whenever we need to find specific numeric values for multiple variables.

### **Example With a Known Solution**

Let’s illustrate with the specific system where the solution is known to be $a=4$, $b=2$, $c=3$:

1. $6a + 3b + 3c = \dots$ (Equation 1)  
2. $2a + 6b + c = \dots$ (Equation 2)  
3. $4a + 2b + 7c = \dots$ (Equation 3)

*(We’re leaving out the final constants on the right side momentarily; assume they’re set so that $a=4$, $b=2$, and $c=3$ solves the system.)*

- **Normalize** the first equation to make $a$’s coefficient 1 (if desired).  
- **Subtract** that normalized equation from the second and third to remove $a$.  
- Now solve the **two-variable** system in $b$ and $c$.  
- **Back-substitute** $b$ and $c$ into the first equation to find $a$.  

You’d get $a=4, b=2, c=3$. In geometric terms, these three planes intersect precisely at the point $(4, 2, 3)$ in 3D space.

### **Visualizing in Three Dimensions**

- Each equation $Ax + By + Cz = D$ can be seen as a plane in 3D.  
- Where all three planes intersect—if they do so at a single point—you have **one unique solution** $(x, y, z)$.  
- If they never intersect (e.g., two are parallel or all three form some impossible arrangement), there may be **no solution**.  
- If they overlap in a line or a plane, there could be **infinitely many solutions**.

### **Key takeaways**

- **Elimination scales** naturally from 2D ($a, b$) to 3D ($a, b, c$) and beyond.  
- **Normalization** (dividing by a leading coefficient) often makes the process simpler but is optional.  
- **Systematically eliminate** variables until you reduce the problem from three unknowns to two, then from two to one.  
- **Back-substitute** once you’ve found a variable to solve for the rest.  
- **Geometric perspective**: In 3D, each equation represents a plane. A single solution means all planes intersect at exactly one point.  

Learning how to manage these multi-variable systems empowers you to tackle higher-dimensional problems in AI, where you might have dozens—or even millions—of unknown parameters!

---

## **Matrix row-reduction**

**Ever wondered if there’s a single, systematic “shortcut” to solving all sorts of linear systems, without having to juggle variables individually?** That’s exactly what **matrix row-reduction** (also known as **Gaussian elimination**) does.

When we solve linear equations by hand—swapping equations, multiplying an equation by a constant, or adding one equation to another—we’re already performing the same fundamental operations that define matrix row-reduction. Instead of writing out variables each time, we embed all the coefficients in a matrix and carefully “reduce” it to a simpler form. 

### **From System of Equations to Matrix**

Let’s revisit a simple system:  

$$
\begin{cases}
5a + b = 17 \\
4a - 3b = 6
\end{cases}
$$

Normally, you’d isolate one variable, plug it back in, and find the other. In **matrix form**, you can list just the coefficients:  

$$
\begin{pmatrix}
5 & 1 \\
4 & -3
\end{pmatrix}
$$

You can also imagine attaching the constants on the right side in a separate column if you’re explicitly doing augmented matrices (though sometimes you focus only on coefficients first).

### **Row Operations**

**Matrix row-reduction** uses three main row operations:

1. **Swap two rows.**  
2. **Multiply an entire row by a nonzero constant.**  
3. **Add or subtract a multiple of one row from another row.**  

These are precisely the actions you perform when solving equations “the usual way,” but now in a streamlined matrix perspective.

### **Row Echelon Form (REF)**

As you apply row operations, you aim for the **Row Echelon Form (REF)** of the matrix. For a 2×2 system, that typically means:

$$
\begin{pmatrix}
1 & * \\
0 & 1
\end{pmatrix}
\quad \text{or} \quad
\begin{pmatrix}
1 & * \\
0 & 0
\end{pmatrix}
\quad \text{or} \quad
\begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}.
$$

- The first scenario (two 1s on the diagonal) indicates a **non-singular system** with a unique solution.  
- The second scenario (only one leading 1) can hint at **infinitely many solutions** (if the last row corresponds to something like $0=0$) or **no solutions** (if a row corresponds to $0=$ non-zero).  
- The third scenario (all zeros) can appear if the system is so “empty” that you have no constraints left (leading again to infinitely many solutions), or it’s part of a bigger matrix with contradictory rows elsewhere.

A **reduced row echelon form (RREF)** goes a step further, making every pivot a 1 and forcing zeros not just below but also **above** the pivots. In many applications, RREF neatly reveals the final solutions directly.

### **Singular vs. Non-Singular Systems in Matrix Form**

1. **Non-Singular Example**  
   If the matrix reduces to something like:  
   
$$
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\quad \longrightarrow 
\begin{cases}
a = 3\\
b = 2
\end{cases}
$$  

- That means there’s one unique solution.

2. **Singular Example**  
   - **Infinitely many solutions**: The matrix might reduce to a row of zeros (like $[0\ \ 0]$), indicating $0=0$. You can’t solve for a unique value of each variable since you lose an independent equation.  
   - **No solutions**: A row might reduce to $[\,0\ 0\,]$ on the left and a nonzero on the right (in the augmented matrix sense), implying $0 = \text{nonzero}$, which is impossible.

### **Beyond 2×2**

The same ideas extend to 3×3 or larger systems. You aim to create a “staircase” of leading 1s with zeros below (and possibly above) them, systematically reducing your matrix. Whether you’re dealing with 2 variables, 3 variables, or 100 variables, the concept remains the same—row operations streamline your equations into a form that clearly shows if there’s a unique solution, infinitely many solutions, or none at all.

### **Key takeaways**

- **Matrix row-reduction (Gaussian elimination)** is just a more compact, systematic way of doing the same manipulations you use to solve linear equations.  
- **Row Echelon Form (REF)** gives a “staircase” pattern of leading 1s (called pivots) and zeros below these pivots.  
- **Reduced Row Echelon Form (RREF)** pushes the idea further, clearing out values *above* the pivots as well.  
- **Non-singular systems** typically reduce to a form with a pivot in every row, revealing a unique solution.  
- **Singular systems** often reveal either a row of all zeros (leading to infinitely many solutions) or a contradictory equation like $0 = \text{nonzero}$ (no solutions).  
- Row-reduction scales seamlessly to higher dimensions, which is crucial for more advanced AI and data problems.

---

## **Row Operations that preserve singularity**

**Have you ever rearranged or scaled a set of instructions but noticed that the fundamental outcome still never changed?** In the context of matrices, this happens when we perform **row operations**—the matrix might look different, but whether it’s singular or non-singular remains the same.

Row operations are the building blocks of matrix transformations. They include:
1. **Swapping two rows**
2. **Multiplying a row by a nonzero scalar**
3. **Adding a multiple of one row to another**

These operations are key in **Gaussian elimination** or **row-reduction**. But an especially important fact is that **they preserve whether a matrix is singular or non-singular**. In other words, if you start with a matrix whose determinant is zero (singular), applying any row operation will keep it at zero; if the determinant is nonzero (non-singular), it remains nonzero.

### **Swapping Two Rows**

Consider a 2×2 matrix with entries:  

$$
\begin{pmatrix}
5 & 1 \\
4 & 3
\end{pmatrix}.
$$

The determinant is $5 \times 3 - 1 \times 4 = 15 - 4 = 11$. Since $11 \neq 0$, this matrix is **non-singular**.

- **Swap Rows**:  

$$
\begin{pmatrix}
4 & 3 \\
5 & 1
\end{pmatrix}.
$$

The determinant becomes $4 \times 1 - 3 \times 5 = 4 - 15 = -11$. Still nonzero! 

In fact, **swapping two rows multiplies the determinant by $-1$**, which does not affect whether it is zero or nonzero. So singularity or non-singularity is preserved.

### **Multiplying a Row by a Nonzero Scalar**

Using the same original matrix,  

$$
\begin{pmatrix}
5 & 1 \\
4 & 3
\end{pmatrix}
$$
let’s multiply the first row by 10:  

$$
\begin{pmatrix}
50 & 10 \\
4 & 3
\end{pmatrix}
$$

The new determinant is:  

$$
50 \times 3 - 10 \times 4 = 150 - 40 = 110,
$$

which is $10 \times 11$, or 10 times the original determinant. Again, if the original determinant was zero, multiplying a row by 10 would yield $10 \times 0 = 0$—it remains zero. If it was nonzero, it stays nonzero (just scaled by a factor). Thus, **multiplying by a nonzero scalar** also preserves singularity.

### **Adding One Row to Another**

Finally, consider adding one row to the other. For example:  

$$
\begin{pmatrix}5 & 1 \\ 4 & 3\end{pmatrix} \longrightarrow \begin{pmatrix}5+4 & 1+3 \\ 4 & 3\end{pmatrix} = \begin{pmatrix}9 & 4 \\ 4 & 3\end{pmatrix}
$$

The determinant of this new matrix is $9 \times 3 - 4 \times 4 = 27 - 16 = 11$, which is exactly the same as the original determinant **11**. Since adding one row to another row doesn’t change the determinant’s zero or non-zero status, **this operation also preserves singularity**.

### **Key takeaways**
- **Row operations** (swap, scale by a nonzero constant, add one row to another) are core to solving linear systems and transforming matrices.  
- Each of these operations **preserves whether a matrix is singular or non-singular**.  
  - **Swapping rows** multiplies the determinant by $-1$.  
  - **Multiplying a row by a nonzero scalar** multiplies the determinant by that scalar.  
  - **Adding a multiple of one row to another** leaves the determinant unchanged.  
- This property is why row-reduction techniques do not alter the fundamental nature of a matrix or the solutions of the corresponding system of equations.