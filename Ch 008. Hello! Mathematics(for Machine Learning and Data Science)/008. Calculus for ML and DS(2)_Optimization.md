# ***Calculus for ML and DS(2)_Optimization***

## Introduction to Optimization

> **Have you ever wandered around a library trying to find the quietest possible seat?** 

You might walk from one corner to another, comparing how much noise you hear. Each spot has a certain ‚Äúnoise level,‚Äù and your goal is to find the place with the *lowest* noise. That everyday quest is a perfect illustration of *optimization*: searching for the best (maximum or minimum) value of something.

### Why Does Optimization Matter in AI?

As you've learned in previous chapters - machine learning -, in machine learning, we define a *loss function/cost function* (do you remember the difference between them?üòä) or *error function* to measure how far our model‚Äôs predictions are from being correct. We want to **minimize** that loss so the model‚Äôs predictions become as accurate as possible.

Just like you want to minimize how much chatter or rustling you hear in the library, a machine learning algorithm wants to minimize errors.

### A Quiet Library Example

Imagine you‚Äôre trying to study, and the library has varying noise levels:

1. **Initial Spot**: You sit near the entrance. The noise level feels high because people are frequently walking in and out.
2. **First Move**: You move toward the back. You notice it‚Äôs quieter‚Äîmaybe you hear fewer footsteps and whispers.
3. **Further Moves**: You keep relocating, each time noticing whether the noise gets better (lower) or worse (higher). 
4. **Quietest Spot**: Eventually, you find a seat where moving in any direction brings more noise. You decide that must be the quietest place in the library.

This mirrors **how machine learning algorithms ‚Äúwalk‚Äù around the parameter space**‚Äîmaking small changes to model parameters (like weight adjustments in a neural network) to see if they get better (reduce error) or worse (increase error).

### The Key Role of Derivatives

How do you know you‚Äôve *really* found that quiet spot? **Derivatives** come in handy:

- The *derivative* $f'(x)$ tells you how quickly the function (e.g., noise) changes with respect to your position $x$.
- At a **minimum** point (the quietest seat), the slope of the noise function is zero:
  - Mathematically, if $f'(x) = 0$, then $x$ is a candidate for a minimum or a maximum.
  
#### Reading ‚ÄúSlopes‚Äù in the Library

- **Slope < 0 (Negative)**: Moving in one direction reduces noise, so you keep going that way.
- **Slope > 0 (Positive)**: Moving in the opposite direction reduces noise instead.
- **Slope = 0**: Neither side offers improvement. This could mean you‚Äôre at the quietest spot‚Äîor the loudest! We have to check carefully.

### Local Minima vs. Global Minima

Sometimes, you might find a cozy table in a dim corner, but there might be an even quieter nook you don‚Äôt see right away. In optimization:

- **Local Minima**: A point that is quieter than any other seat *nearby*, but not necessarily the best in the entire library.
- **Global Minimum**: The absolute quietest seat in the whole library.

In complex machine learning problems, sometimes we settle for a local minimum if finding the true global minimum is extremely difficult. But spotting these special points where $f'(x) = 0$ narrows down our search significantly.

### Putting It All Together

1. **Define a Function**: In machine learning, this is usually a loss(cost) function we want to *minimize*.
2. **Take the Derivative**: The derivative $f'(x)$ indicates how the function changes if we adjust the model parameters.
3. **Set the Derivative to Zero**: Solving $f'(x) = 0$ gives us points that might be minima or maxima.
4. **Evaluate Each Candidate**: Check which one truly gives the lowest value (global minimum) or just a local dip (local minimum).

### Key Takeaway

If you‚Äôve ever paced around a library looking for silence, you already grasp the essence of optimization. We just use the language of derivatives to formalize the process‚Äîand that‚Äôs the foundation of how machine learning algorithms fine-tune their parameters to get the best performance!

---

## Optimization of Squared Loss



### The One Powerline Problem

### The Three Powerline Problem

## Optimization of Log-Loss