# Importing(and Exporting) Data Sets

## 1. Python Packages for Data Science

### 1) What is a Library in Python?

Think of a **Python library** like a toolbox you might use for DIY projects. Instead of making your own tools from scratch, you have a set of ready-made tools designed to help you get the job done faster and easier. In programming, a library provides pre-written code (tools) that you can use to perform common tasks, allowing you to focus on the main project rather than reinventing the wheel.

### 2) Three Groups of Python Libraries for Data Analysis

Python libraries can be grouped into different categories based on their purpose. Here are three main groups:

#### *1. Scientific Computing Libraries*

- **Pandas**: 
  - Imagine you’re organizing your tools in a workshop. You need a good storage system that lets you easily find and use the tools when needed. **Pandas** is that storage system for data. It helps you store, manage, and analyze structured data (like spreadsheets) in a way that’s both efficient and easy to work with. The main structure is called a **DataFrame**—think of it as a well-organized tool chest.
  
- **NumPy**:
  - Think of **NumPy** as the precise measuring tools and rulers in your workshop. It’s designed to handle large arrays of numbers with speed and accuracy. If you need to do some quick and accurate cutting or measuring, NumPy is your go-to tool.

- **SciPy**:
  - When your project involves more advanced tasks—like making sure your cuts are at the perfect angle or your joints are aligned just right—**SciPy** is the toolkit you turn to. It helps with complex mathematical problems and ensures everything fits together perfectly in your analysis.

#### *2. Data Visualization Libraries*

- **Matplotlib**:
  - If you need to draw out plans or create detailed blueprints for your project, **Matplotlib** is your drawing kit. It helps you create visual representations of your data, much like sketching out a plan before starting the actual work.

- **Seaborn**:
  - **Seaborn** is like an artist’s set of paints and brushes. It builds on Matplotlib and allows you to create beautiful, polished visuals easily. If Matplotlib is for drawing blueprints, Seaborn is for adding color and style, making your visuals not just functional but also attractive.

#### *3. Algorithmic Libraries*

- **Scikit-learn**:
  - When it’s time to add some intelligence to your project, like building a smart tool that can predict what materials you’ll need next, **Scikit-learn** is your kit. It provides tools for creating machine learning models, helping you build systems that can learn and make decisions based on data.

- **Statsmodels**:
  - For those times when you need to run tests and make sure everything is statistically sound—like testing the strength of materials or ensuring your measurements are accurate—**Statsmodels** is your toolkit. It helps you explore data, run statistical tests, and ensure that your conclusions are solid.

In summary, these libraries are like specialized toolkits designed to make your data science projects easier and more efficient, allowing you to focus on the big picture without getting bogged down in the details.

## 2. Importing and Exporting Data in Python

### 1) What is Data Importing?

Think of **data importing** like bringing materials into your workshop. Before you can start working on your project, you need to gather all the necessary materials. In the world of data science, importing data is the first crucial step in any analysis. It’s like bringing in raw materials (data) that you will later shape, process, and analyze using various tools (Python libraries).

### 2) How to Import Data Using Pandas

When it comes to importing data into Python, **Pandas** is like your main workbench where all your materials come together for processing. Pandas provides various methods to read in data from different formats, much like how you might bring in wood, metal, or plastic to work on different aspects of a project.

#### *1. Common Data Formats*

Just like materials come in different forms (e.g., sheets of wood, metal bars), data comes in various formats. Some common formats you might encounter:

- **CSV (Comma-Separated Values)**:  
  Imagine sheets of wood, where each piece represents a structured grid. CSV files are like these sheets, with each row and column representing a different part of the structure. Pandas’ `read_csv` method is used to bring these sheets into your workshop for further processing.

- **JSON (JavaScript Object Notation)**:  
  JSON files are like a collection of smaller, organized boxes, each holding different components. They are flexible and can store complex data structures. Pandas’ `read_json` method allows you to unpack these boxes and lay out the materials for use.

- **XLSX (Excel Files)**:  
  Excel files are like a large, compartmentalized toolbox where you store different tools in separate compartments. Each sheet in an Excel file is like a drawer in the toolbox, holding specific data. Pandas’ `read_excel` method helps you access and organize the tools you need from these drawers.

#### *2. The Importing Process*

The process of importing data in Pandas :

- **Step 1: Import the Pandas Library**:  
  Before you can start working, you need to open your workshop. Similarly, you need to import the Pandas library using
  ```python
  import pandas as pd
  ```

- **Step 2: Define the File Path**:  
  Think of the file path as the address of your materials supplier. It tells Pandas where to go to find the data. You **define the path to the file** you want to import, whether it’s stored locally or online.
  ```python
  file_path = 'path_to_your_file.csv'
  ```

- **Step 3: Read the Data**:  
  Finally, you bring the materials into your workshop. The `read_csv` method (or `read_json`, `read_excel`, etc.) **reads the data and loads it into a Pandas DataFrame**, which is like a workbench where you’ll conduct your analysis.
  ```python
  df = pd.read_csv(file_path, header=None)  # `Header=None` is used when the first row of the data is not the header.
  ```
  ```python
  df = pd.read_csv(file_path, header=0)  # `Header=0` is used when the first row of the data is the header.
  ```

### 3) What is Data Exporting?

After you’ve finished your project, you might want to send it out for use or store it for reference. Exporting data is like packaging up your finished product and labeling it for shipment.

- **To CSV**:  
  If you want to store your processed data in a simple, structured format (like flat-packed furniture ready for assembly), you can use the `to_csv` method. This exports the DataFrame to a CSV file, which can easily be shared or stored.
  ```python
  df.to_csv('output_file.csv', index=False)  # `index=False` is used to exclude the row numbers from the output.
  ```

- **To Other Formats**:  
  Pandas also allows you to export data to other formats like Excel (`to_excel`), JSON (`to_json`), and more. Each method is tailored to the specific format, ensuring that your data is packed correctly and efficiently.

In summary, importing and exporting data in Python using Pandas is like managing the flow of materials in and out of your workshop. Pandas provides the tools to efficiently bring in raw data, process it on your workbench (DataFrame), and export the final product in a variety of formats.

## 3. Getting Started Data Analysis with Python(Pandas)

In this section, we introduce some simple Pandas methods that all data scientists and analysts should know when using Pandas. At this point, we assume that the data has been loaded, and it's time for us to explore the dataset.

Pandas has several built-in methods that can be used to understand the data type of features or to examine the distribution of data within the dataset. Using these methods gives an overview of the dataset and can also highlight potential issues, such as incorrect data types, which may need to be resolved later on.

### 1) Understanding Data Types in Pandas

Data can have a variety of types. The main types stored in Pandas objects are `object`, `float`, `int`, and `datetime64`. The data type names in Pandas are somewhat different from those in native Python. Below is a comparison of these types:

| **Pandas Type** | **Python Type** | **Description** |
| --------------- | --------------- | --------------- |
| `object`        | `string`        | Numbers and strings |
| `int64`         | `int`           | Numeric characters |
| `float64`       | `float`         | Numeric characters with decimals |
| `datetime64`, `timedelta[ns]`     | `N/A`      | Time data |

Some types are very similar, such as the numeric data types `int` and `float`. The `object` type in Pandas functions similarly to a `string` in Python, with the exception of the name change. The `datetime64` type in Pandas is particularly useful for handling time series data(But see the `datetime` module in Python for more general date and time handling).

>#### Why do we need to check data types?
> There are two primary reasons to check data types in a dataset:
> 1. **Automatic Assignment by Pandas**: Pandas automatically assigns types based on the encoding it detects from the original data table. This assignment may sometimes be incorrect. For example, it would be awkward if the `car_price` column, which should contain continuous numeric values, is assigned the data type `object`. It would be more natural for it to have the `float` type. In this case, you may need to manually change the data type to `float`.
> 2. **Applicability of Python Functions**: Knowing the data types allows an experienced data scientist to see which Python functions can be applied to specific columns. For example, some mathematical functions can only be applied to numerical data. If these functions are applied to non-numerical data, an error may result.
> 
> When the `df.dtypes` method is applied to the dataset, the data type of each column is returned in a series. A good data scientist's intuition tells us that most of the data types make sense. For instance, the `make_of_cars`, which are names, should be of type `object`. However, if the `bore` column, representing a dimension of an engine, is of type `object`, that would be an issue. This should be a numerical data type.

### 2) Statistical Summary of the Dataset

Next, you would like to check the statistical summary of each column to learn about the distribution of data in each column. The statistical metrics can indicate if there are mathematical issues such as extreme outliers or large deviations that the data scientist may need to address later.

#### *1. Using the `describe` Method*

**To quickly obtain statistics**, you can use the `df.describe` method. This method returns the number of terms in the column as `count`, the average column value as `mean`, the column's standard deviation as `std`, and the maximum and minimum values, as well as the boundaries of each of the quartiles.

By default, the DataFrame `describe` function skips rows and columns that do not contain numbers. However, it is possible to make the `describe` method work for `object`(='string' in python type) type columns as well. To enable a summary of all the columns, we can add the argument `(include='all')` inside the `describe` function : `df.describe(include='all')`.

Some values in the table may be shown as `NaN`, which stands for "Not a Number." This is because that particular statistical metric cannot be calculated for that specific column's data type.

#### *2. Using the `info` Method*

Another method you can use to check your dataset is the `DataFrame.info`(or `df.info`) function. This function gives a **concise summary** of the DataFrame. It prints information about a DataFrame, including the index dtype, columns, non-null values, and memory usage.

## 4. Accessing Databases with Python(Please refer to ***Ch 5. "Hello! SQL"*** for more details)

Think of accessing a database as reaching out to a warehouse full of data. Instead of keeping all your materials in your small workshop (local files), you’re now connecting to a massive storage facility (database) where you can retrieve, store, and manage large amounts of data. Python provides tools that make this connection and interaction with databases as smooth and efficient as possible.

### 1) The Role of Databases in Data Science

Databases are powerful tools for data scientists, acting as centralized storage locations where vast amounts of data can be kept, organized, and accessed. Once you establish a connection to a database, you can easily retrieve the data you need for analysis, just like fetching specific items from a well-organized warehouse.

### 2) How Python Connects to Databases

When you needs to access a database using Python, it’s like sending a request to the warehouse to fetch specific materials. Python uses a mechanism called an **API (Application Programming Interface)** to communicate with the Database Management System (DBMS), which manages the warehouse. This API is like the set of protocols and guidelines that ensure your requests are understood and processed correctly by the warehouse staff.

* SQL APIs connect to DBMS with one or more API calls, build SQL statements as a text string, and use API calls to send SQL statements to the DBMS and retrieve results and statuses.

* DB-API is Python standard API for accessing relational databases. It uses **connection objects** to establish and manage database connections and **cursor objects** to execute queries and manage results.

#### *1. Concepts of the Python DB API*

* Connection Objects
  * Database connections
  * Manage transactions
  * Here are the mehods used with connection objects:
    * `cursor()`: Create a new cursor object using the connection.
    * `commit()`: Commit the current transaction.
    * `rollback()`: Roll back the current transaction.
    * `close()`: Close the connection.

* Cursor Objects : The cursor works similar to a cursor in a text processing system, where you scroll down in your result set and get your data into the application. Cursors are used to scan through the results of a database
  * Database queries

#### *2. Using DB-API to query a Database*

Let’s walk through the typical steps involved in accessing a database using Python:

- **Step 1: Importing database module**:  
You need to establish a connection to the database. This is similar to opening a communication line with the warehouse. In Python, you use the **DB-API** to create a connection object.
  ```python
  from dmodule import connect
  connection = connect('databasename', 'username', 'pswd') # Create connection object
  ```

- **Step 2: Creating a Cursor Object**:  
Once connected, you need a way to interact with the data. Think of a cursor object as a forklift in the warehouse, used to navigate through the aisles and pick up the items (data) you need. In Python, you create a cursor object from the connection object.
  ```python
  cursor = connection.cursor()  # Create a cursor object
  ```

- **Step 3: Running Queries**:  
With your cursor ready, you can run commands to interact with the database. This is like instructing the forklift to retrieve certain items from the warehouse shelves.
  ```python
  cursor.execute('SELECT * FROM table_name')
  resluts = cursor.fetchall()  # Fetch the results
  ```

- **Step 4: Freeing Resources**:  
  Once you’ve finished working with the data, it’s important to free up resources and close the connection to the database. This is like cleaning up your workspace after completing a project.
  
  ```python
  cursor.close()  # Close the cursor
  connection.close()  # Close the connection
  ```

In summary, Accessing databases with Python is like extending your workshop to include a massive, organized warehouse. By using Python’s DB API, you can efficiently manage and interact with large datasets stored in databases, allowing you to scale your data science projects seamlessly.