# Supervised Learning(6) - Gradient Descent

## ***Recap: Cost Function***

In linear regression, our goal is to find the best **line** that fits the data. To do this, we use something called the **cost function**, which tells us how good or bad our line is. The **lower** the cost, the better the fit.

But how do we find the best line with the **lowest cost**? One way to do that is by using an algorithm called **gradient descent**.

## ***What is Gradient Descent?***

Imagine you’re hiking in the mountains, and you want to find the **lowest point** in the valley. But you don’t know exactly where it is, and the terrain is a bit tricky. You have to figure out the best way to **walk downhill** to reach the lowest point.

Gradient descent works in a similar way:
1. You start somewhere on the mountain (this represents your **initial guess** for the line).
2. You look around and see which direction goes **downhill** the fastest (this is the **direction of steepest descent**).
3. You take a small step in that direction.
4. You repeat this process until you reach the **bottom of the valley**, which is the point where the cost is the lowest.

In machine learning, gradient descent helps us find the best values for **$w$** (slope) and **$b$** (Intercept) that give us the lowest cost. The goal is to keep adjusting these values until we find the best fit.

## ***Step-by-Step: How Gradient Descent Works***

### ***1. Starting Point***

You begin with some **initial guesses** for **$w$** and **$b$**. You can start with **$w$ = 0** and **$b$ = 0**, which means your first line might not fit the data well, but it’s a good place to start.

### ***2. Take a Step Downhill***

Next, you calculate which direction you should adjust **$w$** and **$b$** to make the line fit better. This is like deciding which way to **walk downhill** on the mountain to get closer to the lowest point.

- If you’re too far **left**, you need to go **right**.
- If your line is too steep, you need to make it **less steep** by adjusting **$w$**.

### ***3. Keep Moving Downhill***

After taking the first step, you check again: "Which direction now leads me downhill the fastest?" Then, you take another small step in that direction. You keep repeating this process, taking small steps, until you reach the lowest point, which represents the best fit for the data.

## ***Example: Rolling A Ball Down a Hill***

Let’s imagine you’re **rolling a ball** down a hill. The ball represents your **model** (the line you are trying to fit), and the hill represents the **cost function**. Your goal is to let the ball roll down the hill to find the **lowest point**.

- At first, the ball starts high up on the hill (the cost is high).
- As the ball rolls down, it moves towards the **valley** (the cost gets lower).
- The ball keeps rolling downhill until it reaches the **lowest point**, where the cost is the smallest.

In gradient descent, this process is repeated as the algorithm adjusts the values of **$w$** and **$b$** to find the best possible fit for the data.

## ***Multiple Valleys: Local Minima***

Sometimes, the mountain isn’t smooth. There might be more than one **valley**. If you start rolling the ball from a different point, it might end up in a **different valley**. These are called **local minima**.

- A **local minimum** is a low point, but it’s not necessarily the **lowest** point.
- Gradient descent can get stuck in these local minima, but for simple problems like linear regression, this usually isn’t a big issue.

## ***Step-by-Step: How Gradient Descent Works***

### ***1. Starting Point***

First, we need to **start somewhere**. We pick initial values for **$w$** and **$b$**. Often, we start by setting **$w$ = 0** and **$b$ = 0**. This is just the starting point, and the algorithm will help us improve these values step by step.

### ***2. Take a Small Step***

Now, we need to **adjust** the values of **$w$** and **$b$** to improve the fit of the line. To do this, we calculate which direction will help us go **downhill** (reduce the cost function).

- We calculate how much we need to adjust **$w$** and **$b$** by using a small number called **alpha** (also called the **learning rate**).
- **Alpha** controls the size of the steps we take. If alpha is **too large**, we might overshoot the lowest point. If alpha is **too small**, we will take tiny steps and it will take a long time to reach the bottom.

### ***3. Update w and b***

We adjust **$w$** and **$b$** using these formulas:


$$w = w - \alpha \cdot \frac{\partial}{\partial w} J(w, b)$$


$$
b = b - \alpha \cdot \frac{\partial}{\partial b} J(w, b)
$$

What this means is:
- We update **$w$** by subtracting a small amount (based on the learning rate **alpha**) from the current value of **$w$**.
- We do the same for **$b$**, adjusting it step by step to make the cost smaller.

## ***Key Concepts in Gradient Descent***

### ***1. Learning Rate (Alpha)***

The **learning rate**, **alpha**, controls how big our steps are. Think of it like deciding how fast to walk downhill:
- If **alpha** is **too big**, it’s like running down the hill and you might miss the lowest point.
- If **alpha** is **too small**, it’s like taking tiny baby steps and it will take a long time to reach the bottom.

Choosing the right value for **alpha** is important to ensure that gradient descent works efficiently.

### ***2. Derivatives***

The symbols $\frac{\partial}{\partial w} J(w, b)$ and $\frac{\partial}{\partial b} J(w, b)$ are called **derivatives**.

For now, just know that derivatives help us figure out **which direction** to adjust **$w$** and **$b$** to reduce the cost. You can think of them like a compass that tells you which way is downhill.

## ***Simultaneous Update: Moving Together***

In gradient descent, it’s important to update both **$w$** and **$b$** at the **same time**. Imagine you’re adjusting both the slope of the line (**$w$**) and its position (**$b$**) simultaneously. This ensures that we are always moving in the best direction to reduce the cost.

Here’s the **correct way** to do it:
1. **Calculate** the new values for **$w$** and **$b$**.
2. **Store** the new values in temporary variables (like `temp_w` and `temp_b`).
3. **Update** both **$w$** and **$b$** at the same time using these temporary values.

This way, the changes to **$w$** and **$b$** don’t affect each other until both have been updated.

### ***What Happens if We Don’t Do This?***

If we update **$w$** first and then **$b$** (without storing them temporarily), the result might not be as accurate. It’s like changing one thing on a bicycle (like the position of the seat) and then adjusting the pedals afterward—it could mess up the balance! That’s why it’s important to update **both w and b at the same time**.

## ***Example: Walking Downhill Together***

Imagine you and your friend are walking down a hill at the same time, holding hands. You both need to adjust your pace together to reach the bottom safely. If one of you speeds up while the other goes too slow, you might trip!

In gradient descent, **$w$** and **$b$** are like two people walking together. They need to move at the same pace, adjusting at the same time to find the lowest point (the best fit for the line).

# Understanding Gradient Descent

## ***What is Gradient Descent?***

We learned that **gradient descent** is a method we use to find the best values for **$w$** (slope) and **$b$** (intercept) to fit a line to the data. Now, We’ll dive a bit deeper to understand **why** gradient descent works and **how** it adjusts **$w$** and **$b$** to reduce the cost.

## ***Learning Rate (Alpha) and Derivative***

### ***Learning Rate (Alpha)***

The **learning rate** (denoted by the Greek letter **$α$**) controls **how big** the steps are that we take when adjusting the values of **$w$** and **$b$**. 

If you take **big steps**, you get closer to the lowest point faster, but if the steps are too big, you might miss the lowest point. If you take **small steps**, it will take longer to reach the lowest point, but you're less likely to miss it.

### ***Derivative: What It Does***

The **derivative** (denoted as $\frac{d}{dw}$) is like a guide that tells us **which direction** to move in to reduce the cost. You can think of it as a **compass** that tells us which way is "downhill," so we know whether to **increase** or **decrease** **$w$**.

## ***Visualizing Gradient Descent: Rolling Down a Slope***

Imagine you’re on a **hill** and your goal is to roll a ball down to the **lowest point** in the valley. The **gradient descent algorithm** is like pushing the ball down the hill step by step, so it eventually rolls to the lowest point. Let’s see how this works in two different scenarios.

### ***Example 1: Starting at the Right Side of the Hill***

Let’s say you start on the **right side** of the hill.

1. **Look Around**: The **derivative** tells you that the slope is pointing down to the **left**. This means you should **decrease** **$w$** to move toward the lowest point.
2. **Take a Step**: You take a step to the **left**, and the ball rolls closer to the valley.
3. **Repeat**: Each time you take a step, the ball gets closer and closer to the lowest point.

### ***Example 2: Starting at the Left Side of the Hill***

Now, let’s say you start on the **left side** of the hill.

1. **Look Around**: The **derivative** tells you that the slope is pointing down to the **right**. This means you should **increase** **$w$** to move toward the lowest point.
2. **Take a Step**: You take a step to the **right**, and the ball rolls closer to the valley.
3. **Repeat**: Each time you take a step, the ball gets closer to the lowest point, just like before.

In both cases, gradient descent helps you figure out which direction to move in and how to adjust **$w$** to get closer to the lowest point (the best fit for the line).

## ***Why Does Gradient Descent Work?***

The key reason gradient descent works is because of the **derivative**. It tells us the **slope** of the cost function, which helps us figure out if we should **increase** or **decrease** **$w$**.

- If the **derivative** is **positive**, the slope is going up, so we need to **decrease** **$w$** to move downhill.
- If the **derivative** is **negative**, the slope is going down, so we need to **increase** **$w$** to move downhill.

Each step we take brings us closer to the lowest point on the curve, where the cost is the smallest.

# Understanding the Learning Rate

The learning rate is very important because it affects how quickly or slowly the algorithm finds the lowest point (the **minimum**) in the cost function.

Let’s explore what happens if the learning rate is **too small** or **too large**, and how it impacts the efficiency of the gradient descent process.

## ***Case 1: If the Learning Rate is Too Small***

### ***Example: Tiny Steps Down the Mountain***

Imagine you’re walking down a **mountain** but you’re taking **tiny, tiny baby steps**. Each step is so small that it barely moves you closer to the bottom. You’re technically moving in the right direction, but it’s going to take forever to reach the bottom of the mountain.

In gradient descent, if the learning rate **$α$** is **too small**, the algorithm will make very **tiny steps**, and it will take a very long time to find the minimum (the best fit for the line). The process will work, but it will be **slow**.

### ***Visual Example:***

- You start at the top of the mountain.
- You take a tiny step down.
- Then another tiny step...
- And another... It takes a **lot of steps** to get to the bottom.

**Conclusion**: If the learning rate is too small, gradient descent will work, but it will take a **very long time** to reach the minimum.

## ***Case 2: If the Learning Rate is Too Large***

### ***Example: Giant Leaps and Overshooting***

Now, imagine you’re walking down the mountain, but instead of taking small steps, you take **huge leaps**. With each leap, you **overshoot** the bottom and end up on the other side of the mountain. Then you leap again, but you jump too far once more. You keep leaping back and forth, but you never reach the bottom because your steps are too big.

In gradient descent, if the learning rate **$α$** is **too large**, the algorithm will take **huge steps** and **overshoot** the minimum. This means the cost will actually **increase**, and the algorithm may never find the minimum.

### ***Visual Example:***

- You start near the bottom of the mountain.
- You take a huge leap and **overshoot** the bottom.
- You leap again and **miss the minimum** once more.
- You keep leaping back and forth, never reaching the lowest point.

**Conclusion**: If the learning rate is too large, gradient descent may **overshoot** the minimum and never find the best solution.

## ***What Happens When You Reach the Minimum?***

### ***Example: Standing Still at the Bottom***

Now, let’s say you’ve finally reached the **bottom** of the mountain. What happens next? Should you keep moving?

When you reach the **minimum**, the **slope** of the cost function becomes **zero**, which means there is no need to keep adjusting **$w$**. In gradient descent, if you reach the minimum, the algorithm **stops** moving because there is no more downhill to go.

### ***Visual Example:***

- You’ve reached the lowest point.
- There’s no more downhill, so the algorithm **stays still**.
- You’ve found the best fit for the data.

**Conclusion**: When you reach the minimum, gradient descent stops adjusting **w** and stays at the best possible solution.

## ***Choosing the Right Learning Rate***

Choosing the **right learning rate** is important for gradient descent to work efficiently. 

- If the learning rate is **too small**, the algorithm will take **forever** to find the minimum.
- If the learning rate is **too large**, the algorithm will **overshoot** and never find the minimum.
- When you choose a **good learning rate**, the algorithm will take **reasonable steps** and quickly find the best solution.

As the algorithm gets closer to the minimum, it naturally takes **smaller steps**, which helps it slow down and reach the best fit smoothly.

# Gradient Descent for Linear Regression

## ***Recap: What is Linear Regression?***

In **linear regression**, we try to fit a **straight line** through a set of data points. This line helps us make predictions. 

For example, if you know the size of a house, the line can help you predict the price of that house. The goal is to find the best line that fits the data as closely as possible.

To find the best line, we use something called the **squared error cost function**. This cost function tells us how far off our predictions are from the actual values. The smaller the cost, the better the line fits the data.

## ***Using Gradient Descent to Find the Best Line***

You can think of **Gradient descent** is like a method for **adjusting** the line step by step until it fits the data perfectly. It's like trying to walk down a hill to the lowest point—each step gets you closer to the bottom.

In gradient descent, we have two main parameters:
- **$w$**: This controls the **slope** of the line (how steep it is).
- **$b$**: This controls the **intercept** of the line (where the line crosses the y-axis).

The goal is to keep adjusting **$w$** and **$b$** using gradient descent until the line fits the data as well as possible.

### ***The Squared Error Cost Function***

The **squared error cost function** tells us how good or bad the line is at predicting the values. If the cost is **high**, it means the line is not fitting the data well. If the cost is **low**, it means the line fits the data well.

Here's the formula for the cost function:

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2
$$

Where:
- **m** is the number of data points.
- $ f(x^{(i)})$ is the predicted value using our line (this is equal to $w \cdot x + b$).
- $y^{(i)}$ is the actual value from the data.

The smaller this cost function, the better our line is fitting the data.

## ***Gradient Descent Algorithm: Adjusting w and b***

To make our line fit better, we use gradient descent to adjust **$w$** and **$b$**. We do this by using the following update formulas:

1. **Update w (slope)**:
$
w = w - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)}) \cdot x^{(i)}
$

2. **Update b (intercept)**:
$
b = b - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})
$

Here:
- **$α$** (alpha) is the **learning rate**. It controls how big or small the steps are when we update **w** and **b**. It's like deciding whether to take big steps or small steps while walking down the hill.

## ***Why Does This Work?***

### ***What the Derivative Does***

In gradient descent, we use something called a **derivative**. You can think of it as a tool that tells us whether we should **increase** or **decrease** **w** and **b** to reduce the cost.

If the cost is getting **smaller**, we keep moving in that direction. If the cost gets **bigger**, we change direction. By using the derivatives, we can figure out which direction gets us closer to the best line.

### ***No Multiple Local Minima***

One great thing about the **squared error cost function** for linear regression is that it's shaped like a **bowl**. 

This means it has **only one lowest point** (called the **global minimum**). We don’t have to worry about getting stuck in other valleys (local minima) because there’s only one valley to reach.

So, as long as we choose the learning rate **$α$** carefully, we will always find the **best possible line**.

### Batch Gradient Descent

What we just described is called **Batch Gradient Descent**. This means that at each step, the algorithm looks at all the training data to compute the cost and adjust **$w$** and **$b$**. It’s called "batch" because we update the parameters after evaluating the entire "batch" of data.

> 'Batch' means 'a quantity or consignment of goods produced at one time'.

There are other types of gradient descent that use smaller subsets of data at each step, but for linear regression, batch gradient descent is the one we use most often.

#### Why It Matters

Gradient descent is like a smart way of guessing and checking. By following the direction of the steepest slope (the gradient), it finds the best values for **$w$** and **$b$** efficiently. Once the model is trained, you can use it to make predictions, such as predicting the price of a house based on its size.

#### Example: Predicting House Prices

Let’s say your friend's house is 1,250 square feet. Once the gradient descent has optimized the parameters, you can use the final function, **$f(x)$**, to predict the house price. If the line fits the data well, the model might predict that the house could be sold for $300,000.