# **Supervised Learning (6) – Gradient Descent**

> Have you ever tried balancing yourself on a skateboard or a bike on a slight slope? 

You might lean a bit one way, realize you’re rolling downhill, and then adjust your position so you don’t end up going too fast. Each small movement teaches you whether you need to shift more left or right. **Gradient descent** works a lot like that—it’s a method to find the “best spot,” especially when we talk about **fitting a line** (linear regression) to data.

## The Big Picture: Cost Function and Why We Minimize It

In **linear regression**, we aim to find the best **straight line** that predicts outcomes from given inputs. This line has two main parameters:

- $w$ (the slope)
- $b$ (the intercept)

To measure whether our line is doing a good job, we use a **cost function**. A common choice is the **squared error cost**:  

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2
$$

where:

- $m$ is the number of data points. The reason we divide by $2m$ is to make the math easier - The derivative of the squared error cost function is linear in $w$ and $b$.
- $f(x^{(i)}) = w \cdot x^{(i)} + b$ is the **predicted** value for the $i$-th data point.
- $y^{(i)}$ is the **actual** value for the $i$-th data point.

### Why Minimize This?

The **smaller** $J(w, b)$ is, the closer our predictions are to the real data. We want to **lower** this cost, much like trying to get the skateboard/bike to balance perfectly without drifting off.

## What Is Gradient Descent?

**Gradient descent** is an **iterative** technique used to **minimize** a function, such as our cost function $J(w, b)$. Think of it like **walking downhill** in a foggy valley:

1. **Choose a Starting Point**: You might not start at the best spot, but pick somewhere to begin.
2. **Look Around**: Check the slope or “direction” in which the cost decreases. That direction is given by the **gradient** (or **derivative**).
3. **Take a Step**: Move **a small amount** toward the direction that makes $J(w, b)$ smaller.
4. **Repeat**: Each step moves you further downhill until you can’t go any lower.

### The Foggy Valley Analogy

- If you were in a clear valley, you’d spot the lowest point right away. But in machine learning, we often have **complex** or **high-dimensional** terrains, so we rely on local slope information (the derivative) to guide us, step by step, to the bottom.

## A Deeper Dive: Key Components of Gradient Descent

### Learning Rate: $\alpha$

The **learning rate** is a small number that controls the **step size** when you move downhill. You can think of $\alpha$ in two ways:

- **Large $\alpha$**: You take **big strides**. It’s faster if you’re lucky, but you might overshoot the valley floor and bounce around without settling.
- **Small $\alpha$**: You take **baby steps**. You’ll get there eventually, but it could take a very long time.

Finding a **balanced** learning rate is crucial so you move steadily down the slope without flying over the minimum.

### Partial Derivatives: Our Directional Compass

When dealing with functions of multiple variables like $w$ and $b$, we use **partial derivatives** to find out how changes in each variable affect the cost. Symbolically, we have:

- $\frac{\partial}{\partial w} J(w, b)$: how $J$ changes if we tweak $w$
- $\frac{\partial}{\partial b} J(w, b)$: how $J$ changes if we tweak $b$

The sign (+ or –) tells us whether to **increase** or **decrease** each parameter to move downhill.

### Simultaneous Update

We **calculate** the new $w$ and $b$ first, then **update** both at the same time:

1. Calculate the partial derivatives (the slopes) at the current $(w, b)$.
2. Store them temporarily.
3. Adjust $w$ and $b$ using those stored values **together**.

If you were adjusting only $w$ first and **then** $b$, you’d be mixing information from two different moments in time, which can lead to a zig-zag path or inaccurate results.

## Rolling Down the Hill: An Expanded Example

Let’s make it more concrete with a **mental picture** of a rolling ball:

1. **Initial Drop**: You place the ball (your initial guess of $w$ and $b$) somewhere on the hill. You don’t know if it’s near the bottom or far away.
2. **Observe the Slope**: If the hill’s surface slopes downward to your left, that tells you $w$ might need to **decrease**. If it slopes to the right, $w$ needs to **increase**.
3. **Roll**: You let the ball roll a bit (apply a step with the learning rate).  
   - If the slope is steep, the ball gains speed (bigger adjustment).  
   - If the slope is gentle near the bottom, it slows (smaller adjustment).
4. **Multiple Dimensions**: In reality, you often have a slope for $w$, $b$, and possibly many more parameters. Gradient descent “rolls” in this multi-dimensional space.
5. **Reach the Bottom**: Eventually, you find a place where the slope is effectively zero (no more downhill), which ideally is the **global minimum** of the cost function.

## Applying Gradient Descent to Linear Regression

### The Cost Function’s “Bowl Shape”

For linear regression with squared error cost:
- The shape of $J(w, b)$ forms a **bowl** (or paraboloid in multiple dimensions).
- This bowl has **only one** deepest point—the global minimum.
- So, as you iterate with gradient descent, you **will** find that minimum (given a reasonable learning rate and enough steps).

### The Parameter Updates

In **batch gradient descent** for linear regression, the updates often look like:  

$$
w \leftarrow w - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})\cdot x^{(i)}
$$  

$$
b \leftarrow b - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})
$$

Here, each data point $(x^{(i)}, y^{(i)})$ influences how much we shift $w$ and $b$. Notice that both are updated by **subtracting** something, guiding us “downhill” in the cost landscape.

## Handling Different Starting Points & Multiple Valleys

- In **linear regression**, the cost function is nicely shaped (a single valley), so no matter where you start, you’ll end up in the same “lowest spot.”
- In more complex models, you might have multiple valleys (local minima). Then, starting in different places or using different strategies can matter a lot. But for basic linear regression, you get a clean run to the unique minimum.

## Tying It All Together

1. **Initialize**:
   - Pick a starting guess for $w$ and $b$. It might be 0, or a small random number.
2. **Compute the Slopes**:
   - Find $\frac{\partial}{\partial w}J(w, b)$ and $\frac{\partial}{\partial b}J(w, b)$ using your training data.
3. **Update**:
   - Apply $w \leftarrow w - \alpha\cdot \frac{\partial}{\partial w}J(w, b)$
   - Apply $b \leftarrow b - \alpha\cdot \frac{\partial}{\partial b}J(w, b)$
4. **Repeat**:
   - Keep iterating until the changes are very small or you reach a maximum number of steps.
5. **Result**:
   - $w$ and $b$ converge to values that minimize the cost. Now you have a line that best fits your data!

## Wrap-Up: Key Insights

- **Gradient Descent = Walking Downhill**: It’s an intuitive approach where you move in small steps guided by the slope.
- **Learning Rate Matters**: 
  - Too big: you might bounce around and never settle.
  - Too small: progress is slow, but you won’t miss the minimum.
- **Linear Regression’s Sweet Spot**: 
  - The squared error cost is shaped like a bowl, so gradient descent smoothly finds the global minimum.
- **One Unified Update**: Always update both $w$ and $b$ (and any other parameters) at the same moment, or you risk inconsistent moves.
- **Practical Benefit**: Once trained, your linear model can **predict** answers for new inputs—for example, house prices given sizes.

By understanding gradient descent for linear regression, you’ve grasped a **core tool** in machine learning. The same logic extends to more complicated models where gradient descent (with some tweaks) is still the go-to method for finding optimal parameters.