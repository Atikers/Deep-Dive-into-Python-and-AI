# ***Recommender Systems(15) - Recommander Systems***

## ***Collaborative Filtering***

### ***Making Recommendations***

#### ***Have You Ever Wondered How Streaming Sites Know Which Movie You’ll Love Next?***

Imagine you have a huge movie library and your friends are constantly rating the movies they watch with stars (from 0 to 5). Now think of a streaming service that wants to guess which new movies each friend might enjoy. How does it figure that out without just randomly suggesting titles?

In this section, we’ll explore an approach often called **recommendation systems** or **collaborative filtering**. Even if you’re new to these ideas, don’t worry! We’ll walk through everything step by step!

---

#### ***1. The Basics of Our Setup***

Let's say we have:
- **4 users**
- **5 movies**

These users have rated some of the movies from **0 to 5 stars**. We’ll mark a movie with a “?” if a user hasn’t watched or hasn’t provided a rating. For instance, if User 1 hasn’t watched Movie 3, we’ll mark it as “?”.

A table might look like this:

| Movie    | User 1 | User 2 | User 3 | User 4 |
|----------|--------|--------|--------|--------|
| Movie 1  | 5      | 5      | 0      | 0      |
| Movie 2  | 5      | ?      | ?      | 0      |
| Movie 3  | ?      | 4      | 0      | ?      |
| Movie 4  | 0      | 0      | 5      | 4      |
| Movie 5  | 0      | 0      | 5      | ?      |

- $n_u$ = number of users (4 here).
- $n_m$ = number of movies (5 here).

We also define:
- $r(i,j) = 1$ if user $j$ has rated movie $i$, and $r(i,j) = 0$ otherwise.
- $y(i,j)$ = the actual rating (0 to 5 stars) **only** if user $j$ has rated movie $i$.

For example:
- $r(1,1) = 1$ because User 1 rated Movie 1.
- $r(3,1) = 0$ because User 1 did **not** rate Movie 3.
- $y(3,2) = 4$ means User 2 gave Movie 3 a rating of **4 stars**.

If you remember **User** `j`, **Movie** `i`, then you can understand easily.

---

#### ***2. Why This Is So Valuable***

This setup is the foundation of many recommendation engines. Think about the times you’ve used:
- A streaming service that suggests shows (like Netflix or Max).
- An online store that suggests products (like Amazon).
- A food delivery app that recommends restaurants or dishes.
  
**In all of these cases**, the platform tries to figure out what you might enjoy next based on the ratings (or clicks, watch times, etc.) that **you and other users** have provided.

---

#### ***3. Making Predictions for Unrated Movies***

The challenge is: **How do we guess what rating a user would give to a movie they haven’t seen yet?** In the table above, we have question marks (“?”) where a rating is missing. If we can guess (or predict) those values, we can:
1. Sort the predicted ratings from highest to lowest.
2. Show the user the top-rated items first (because we think they’ll like them the most).

In more advanced settings, we might also have “features” that describe each movie (like its genre—romance, action, comedy, etc.). We’ll see that these can help us make better guesses. But **even if** we don’t have those movie features, there are still ways to learn from just the ratings themselves (we’ll explore that soon).

---

#### ***4. An Analogy: Classmates and Science Projects***

To make this easier:
- Think of each **movie** as a **science project** in your class.
- Each **user** is a **classmate** who reviews these science projects with “stars.”
- Some classmates review (rate) multiple projects, others might not have the time to review them all (so some ratings are missing).
  
If you want to **recommend** which projects a new student should check out, you’d look at everyone’s past ratings and guess which remaining projects might be top-rated by certain classmates—even though those classmates haven't officially rated them yet.

---

#### ***5. Where We’re Headed Next***

We’ll soon talk about:
1. **How to mathematically predict** the missing “?” ratings using the information we have.
2. **What to do if we do (or do not) have extra details** about each movie (like genre).
3. **How large companies** integrate this approach into their websites or apps to keep users interested and drive sales (or views).

**Key idea**: By focusing on user preferences (the ratings they provide), we can make surprisingly accurate guesses about what they’d love to see, buy, or do next—even if they haven’t explicitly told us.

---

#### ***6. More Real-World Examples***

Beyond movies, you can use the same approach for:
- **Music suggestions**: Predict a listener’s rating for songs or playlists they haven’t heard yet.
- **Restaurant discovery**: Estimate which restaurants you might rate highly based on others with similar tastes.
- **Clothing or product recommendations**: Help a store guess which clothes or gadgets you’d love, based on your previous purchases and ratings.

The possibilities are **enormous** and form a huge part of how modern apps keep us engaged!

---

#### ***7. Recap***

- We have **users** and **items** (movies), each with a **user–item rating table**.
- Many ratings are **missing** (users haven’t rated every item).
- We define $r(i,j)$ and $y(i,j)$ to keep track of which movies are actually rated and their star values.
- **Goal**: Predict the missing ratings to make recommendations.

---

### ***Using Per-Item Features***

Imagine you have a bunch of movies, each with a “personality”—like how **romantic** or how **action-packed** it is. Now picture a handful of people, each with their own tastes in movies. If we know a user’s likes and dislikes so far, **can we guess** what star rating they’ll give to a brand-new movie just by looking at those personality (feature) scores?

In this section, we’ll explore:
1. **How** we represent these “personalities” (features) for each movie.
2. **How** we build a simple linear model for each user to predict ratings.
3. **How** we define the **cost function** (the math that helps us learn good parameters for our model).
4. **Why** regularization helps avoid wacky or overfitted predictions.

Let’s dive in step by step.

---

#### ***1. Setting Up Movie Features***

Say we have:
- $n_u$ = number of users (User 1 to User 4)
- $n_m$ = number of movies (Movie 1 to Movie 5)
- $n$ = number of features describing each movie.  
- $j$ = User number
- $i$ = Movie number

##### ***Example: Romance and Action***
- $x_1(i)$: “romance level” of movie $i$.
- $x_2(i)$: “action level” of movie $i$.
- remember $i$ is the movie number, so $x_1(i)$ is the romance level of Movie $i$. e.g. $x_1(1)$ is the romance level of Movie 1, $x_1(2)$ is the romance level of Movie 2.

So each movie $i$ is described by a **feature vector**:  

$$
x^{(i)} = \begin{bmatrix} 
x_1(i) \\ 
x_2(i)
\end{bmatrix}
$$

This means each movie $i$ has 2 features: romance level and action level.

> **Analogy**: Think of each movie like a **soup**. The “romance” level is how sweet and gentle the soup is, while the “action” level is how spicy or punchy it tastes. A high romance score means a very sweet flavor; a high action score means a spicy kick!

---

#### ***2. A Different “Mini Linear Model” for Each User***

We want to predict user $j$’s rating for movie $i$. We do that by:  

$$
\text{Predicted rating} = w^{(j)} \cdot x^{(i)} + b^{(j)}
$$

also, remember $j$ is the user number, so $w^{(j)}$ is the parameter vector for User $j$, and $b^{(j)}$ is the bias term for User $j$.

where:
- $w^{(j)}$ is **user $j$’s** parameter vector (same size as $x^{(i)}$).
- $b^{(j)}$ is a **bias** term for user $j$ (like an offset in their rating style).

Each user gets their own $w^{(j)}$ and $b^{(j)}$ because not everyone likes the same mix of romance and action. For example:
- User 1 might love romance and hate action, so $w^{(1)}$ might be heavily weighted on the romance feature.
- User 2 might like a bit of both, so $w^{(2)}$ might be more balanced.

> **Analogy**: Think of $w^{(j)}$ and $b^{(j)}$ like **taste buds** for user $j$. They interpret the soup’s “ingredients” (the movie’s romance/action levels) differently for each person.

---

#### ***3. Cost Function for a Single User***

How do we **learn** $w^{(j)}$ and $b^{(j)}$? We need a **cost function** that says how close our predictions are to the **actual** star ratings, $y^{(i,j)}$.

Remember $j$ is the user number, so $w^{(j)}$ is the parameter vector for User $j$, and $b^{(j)}$ is the bias term for User $j$. And $i$ is the movie number. I'll tell you this again and again :)

1. **Focus on movies the user actually rated**: If $r(i,j) = 1$, it means user $j$ **did** rate movie $i$.  
2. **Sum of squared errors**: We compare $\bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)^2$ for each rated movie.  
3. **Regularization**: We add a term to discourage $w^{(j)}$ from growing **too large** (overfitting is problem always). 

Putting it all together, for user $j$:  

$$
J\bigl(w^{(j)}, b^{(j)}\bigr) = \frac{1}{2}\sum_{i : r(i,j)=1} \Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2 + \frac{\lambda}{2} \sum_{k=1}^{n} \Bigl(w_{k}^{(j)}\Bigr)^2.
$$

- The summation $\sum_{i : r(i,j)=1}$ means we **only** sum over those movies $i$ that user $j$ actually rated.  
- $\lambda$ is the **regularization parameter**: a small positive number that helps control the size of $w^{(j)}$.
- original cost function $J\bigl(w^{(j)}, b^{(j)}\bigr)$ is  

$$
\frac{1}{2m^{(j)}}\sum_{i : r(i,j)=1} \Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2 + \frac{\lambda}{2m^{(j)}} \sum_{k=1}^{n} \Bigl(w_{k}^{(j)}\Bigr)^2
$$

but we remove the $m^{(j)}$ from the denominator because:
  - Users have different numbers of rated movies (some might rate 100 movies, others only 10)
  - Not dividing by $m^{(j)}$ allows users with more ratings to have a stronger influence on the cost function, which is desirable since their preferences are more reliable
  - It simplifies the optimization process while maintaining the effectiveness of the model

---

#### ***4. Cost Function Across All Users***

We don’t just have one user; we have $n_u$(number of users) of them. We do the same cost calculation for each user $j$, then **sum** them all up:  

$$
J\bigl(w^{(1)}, b^{(1)}, \dots, w^{(n_u)}, b^{(n_u)}\bigr)
=\sum_{j=1}^{n_u} \left[\frac{1}{2}\sum_{i : r(i,j)=1} \Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2 + \frac{\lambda}{2}\sum_{k=1}^{n} \Bigl(w_{k}^{(j)}\Bigr)^2 \right]
$$

> **Analogy**: Imagine each user has their own personal mini “taste test” to see how well our soup predictions line up with their actual ratings. Then we combine everyone’s taste tests for one giant scoreboard.

---

#### ***5. Training: Finding the Best Parameters***

To **train** the system, we:
1. Start with some guess for each $w^{(j)}$ and $b^{(j)}$.
2. Use **gradient descent** (or a similar optimization method) to adjust them **all** and **minimize** the total cost $J$.

###### ***(Recap) Training with Gradient Descent***
- The weight vector $w^{(j)}$
- The bias term $b^{(j)}$

###### ***computing the gradients***
- For each user j, we compute the partial derivatives of the cost function J:
1. **Partial derivative with respect to** $w_k^{(j)}$:  
  
$$
\frac{\partial}{\partial w_k^{(j)}} J = \sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)x_k^{(i)} + \lambda w_k^{(j)}
$$

2. **Partial derivative with respect to** $b^{(j)}$:  
  
$$
\frac{\partial}{\partial b^{(j)}} J = \sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)
$$

###### ***update rules***

Using learning rate $\alpha$, we update our parameters:

1. **Weight update rule**:  

$$
w_k^{(j)} := w_k^{(j)} - \alpha \left[\sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)x_k^{(i)} + \lambda w_k^{(j)}\right]
$$

2. **Bias update rule**:  

$$
b^{(j)} := b^{(j)} - \alpha \sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)
$$

###### ***Implementation Example***

Here's how we might implement this in code:

```python
def gradient_descent(X, y, r, learning_rate=0.01, lambda_reg=0.1, num_iterations=1000):
    num_users = r.shape[1]
    num_features = X.shape[1]
    
    # initialize parameters
    w = np.zeros((num_users, num_features))  # weight vector for each user
    b = np.zeros(num_users)                  # bias for each user
    
    for iteration in range(num_iterations):
        for j in range(num_users):  # for each user
            # indices of movies rated by the user
            rated_items = np.where(r[:, j] == 1)[0]
            
            if len(rated_items) > 0:
                # update weight
                for k in range(num_features):
                    grad_w = 0
                    for i in rated_items:
                        pred = np.dot(w[j], X[i]) + b[j]
                        grad_w += (pred - y[i,j]) * X[i,k]
                    grad_w += lambda_reg * w[j,k]  # regularization term
                    w[j,k] -= learning_rate * grad_w
                
                # update bias
                grad_b = 0
                for i in rated_items:
                    pred = np.dot(w[j], X[i]) + b[j]
                    grad_b += (pred - y[i,j])
                b[j] -= learning_rate * grad_b
    
    return w, b
```

###### ***Key Considerations***

When implementing gradient descent for recommendation systems, keep in mind:

1. **Batch vs. Mini-batch**
   - The implementation above uses batch gradient descent
   - For large datasets, mini-batch processing might be more efficient
   - Mini-batches can help escape local minima and converge faster

2. **Learning Rate Selection**
   - Too large: may cause divergence
   - Too small: slow convergence
   - Consider using adaptive learning rates or learning rate scheduling

3. **Regularization Strength ($\lambda$)**
   - Controls the balance between fitting the data and preventing overfitting
   - Can be selected using cross-validation
   - Typically starts with small values (0.01 to 0.1)

4. **Convergence Monitoring**
   - Track the cost function $J$ across iterations
   - Stop when changes become sufficiently small
   - Consider implementing early stopping based on validation set performance

---

#### ***6. What If We Don’t Have (Good) Features?***

So far, we assumed we know exactly how “romantic” or “action-packed” each movie is. But what if we **don’t**? In many real-world scenarios, you only have user rating data, with no explicit features. The **great news** is there’s a more advanced approach (often called “collaborative filtering with learned features”) that can discover these hidden factors **automatically**—but that’s a story for another time!

> **Teaser**: You can learn these features **directly** from the rating patterns, letting the system figure out how to group movies (and users) in a way that best predicts the missing ratings.

---

#### ***7. Key Takeaways***

- **Linear Regression Per User**: We treat each user’s ratings as its own mini linear regression problem.
- **Cost Function**: Sum of squared errors over all rated movies, plus a **regularization** term.
- **Global Cost**: Add up individual user costs to measure the overall performance.
- **Optimization**: We tune $w^{(j)}$ and $b^{(j)}$ for each user to minimize the total cost.
- **Features**: If we have them (like romance level, action level), we can make direct predictions. If we don’t, more advanced methods can learn them from data.

Remember: The magic is that the system learns how each user’s tastes align with a movie’s features—so it can **predict** new ratings, even if the user never rated that particular movie before.

---

#### ***Collaborative Filtering Algorithm***

#### ***Have You Ever Wondered How a Streaming App Discovers Movie “Personalities” on Its Own?***

Previously, we saw how having features like “romance level” or “action level” for each movie can help predict a user’s rating. But what if we **don’t** already know those features? **Collaborative Filtering** steps in, allowing the system to **learn** these hidden features *directly* from user ratings—no hand-labeling required!

---

#### ***1. The Mystery of Unknown Features***

Imagine you only have:
- A list of **users** (User 1 to User 4).
- A set of **movies** (Movie 1 to Movie 5).
- **Star ratings** each user gives to some (but not all) of these movies.

But you **don’t** know the movies’ “romance scores” or “action scores” ahead of time. It’s like you have a puzzle with missing pieces, and yet you still want to predict the missing ratings—**and** figure out each movie’s hidden traits.

> **Analogy**: Suppose you’re trying to guess each movie’s “flavor.” Some might be sweet (romance) or spicy (action). But no one has labeled “this is sweet,” “this is spicy.” Instead, you only know how each person reacted: “User 1 gave it 5 stars, User 2 gave it 5 stars, User 3 gave it 0, User 4 gave it 0.” From these reactions alone, can you figure out the movie’s flavor profile?

---

#### 2. Parameters for Each User: $w^{(j)}$ and $b^{(j)}$

In earlier notes, we introduced **per-user parameters**:
- $w^{(j)}$ (a vector showing how user $j$ weighs each feature, like romance or action).
- $b^{(j)}$ (an offset for user $j$, in case they generally rate higher or lower than average).

> **Do you remember?** $j$ is the user number, so $w^{(j)}$ is the parameter vector for User $j$, and $b^{(j)}$ is the bias term for User $j$. And $i$ is the movie number :)

##### ***Predicting a Rating***
For movie $i$ with **feature vector** $x^{(i)}$, user $j$’s predicted rating is:  

$$
w^{(j)} \cdot x^{(i)} + b^{(j)}.
$$

If $b^{(j)}$ is small or zero, that just means user $j$ doesn’t have a big offset in their ratings. Maybe they consistently rate in the mid-range. We’ll keep it in the formulas for completeness.

> **Analogy**: Think of $w^{(j)}$ and $b^{(j)}$ like a custom pair of glasses for each user. The “lens” ($w^{(j)}$) focuses on different movie features (romance or action), and the “frame” ($b^{(j)}$) shifts everything up or down (a person who generally rates high or low).

---

#### 3. Learning Movie Features ($x^{(i)}$) from Ratings

Without any predefined romance or action scores, we **treat $x^{(i)}$ as unknown parameters**. That is, each movie $i$ has a feature vector we want to learn:  

$$
x^{(i)} =
\begin{bmatrix}
x_{1}^{(i)} \\
x_{2}^{(i)} \\
\vdots \\
x_{n}^{(i)}
\end{bmatrix}.
$$

We choose a dimension $n$ (like 2 for romance/action, or maybe more). Then we want to figure out the best values of $x_{1}^{(i)}, x_{2}^{(i)}, \ldots, x_{n}^{(i)}$ that *explain* the ratings.

> **Key Idea**: Because multiple users have rated **the same** movie, we can “reverse-engineer” what the movie’s features must be so that the user predictions come out close to the observed star ratings.

---

#### 4. Cost Function to Learn $x^{(i)}$

Let’s focus on a single movie $i$. Suppose several users rated it. For each user $j$ who rated movie $i$, we have:
- **Predicted rating**: $w^{(j)} \cdot x^{(i)} + b^{(j)}$
- **Actual rating**: $y^{(i,j)}$
- **Indicator** $r(i,j)$ = 1 if user $j$ rated movie $i$, else 0.

We define a cost function for movie $i$’s features:
$$
J\bigl(x^{(i)}\bigr)
\;=\;
\frac{1}{2}
\sum_{j : r(i,j)=1}
\Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2
\;+\;
\frac{\lambda}{2}
\sum_{k=1}^{n}
\Bigl(x_{k}^{(i)}\Bigr)^2.
$$

- **First term**: sum of squared errors (predictions vs. actual) for all users who rated this movie.
- **Second term**: regularization on the feature vector $x^{(i)}$, which helps prevent runaway solutions or overfitting.
- $\lambda$ is the regularization parameter (a small positive number).

To find $x^{(i)}$, we minimize this cost function using something like gradient descent. This yields a set of features for movie $i$ that best explains the ratings from the users who watched it.

---

#### ***5. Doing This for All Movies***

We repeat the process for each movie $i = 1, 2, \ldots, n_m$. Summing up all those costs gives us:  

$$
J\bigl(x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}\bigr)
\;=\;
\frac{1}{2}
\sum_{i=1}^{n_m} \sum_{j : r(i,j)=1}
\Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2
\;+\;
\frac{\lambda}{2}
\sum_{i=1}^{n_m}\sum_{k=1}^{n}
\Bigl(x_{k}^{(i)}\Bigr)^2.
$$

where $n_m$ is the number of movies, $j$ is the user number, $i$ is the movie number, $k$ is the feature number.

But remember, the **$w^{(j)}$** and **$b^{(j)}$** for users might also need to be learned (they’re part of the puzzle too!). So in fact, we combine:
- The cost for learning user parameters ($w^{(j)}, b^{(j)}$).
- The cost for learning movie features ($x^{(i)}$).

All rolled into one big function. Minimizing that single function with respect to **both** sets of parameters is the essence of **collaborative filtering**.

---

#### ***6. Collaborative Filtering: The Big Picture***

**Step by Step**:
1. **Initialization**: Start with random guesses for:
   - $w^{(j)}$ and $b^{(j)}$ for each user $j$.
   - $x^{(i)}$ for each movie $i$.
2. **Compute the Cost** $J(w,b,x)$ which includes:
   - Errors between predicted and actual ratings for all $(i,j)$ where $r(i,j)=1$.
   - Regularization terms for $w^{(j)}$ and $x^{(i)}$.
3. **Update Parameters** with gradient descent:
   - Adjust each $w^{(j)}$, $b^{(j)}$, and $x^{(i)}$ in the direction that **reduces** $J$.
   - Repeat until the cost stops changing much.

> **Analogy**: You have a giant puzzle where each **user** has a piece describing their tastes, and each **movie** has a piece describing its hidden “recipe.” By shifting all these puzzle pieces around (via gradient descent), they “click” into positions that best match the star ratings we observe.

---

#### ***7. Why Is It Called “Collaborative” Filtering?***

Because **multiple users** rating the **same** item “collaborate” to figure out the item’s features. For example:
- If Users have each rated a particular movie, their combined feedback guides the system to deduce that movie’s hidden vector $x^{(i)}$.
- In turn, those deduced features help predict how *other* users (who haven’t rated that movie yet) might respond.

Essentially, everyone’s input helps shape the features of each item, and each item’s learned features help predict **everyone else’s** future ratings.

---

#### ***8. Key Takeaways***

1. **Learning Movie Features**  
   - Instead of **manually** labeling each movie with “romance = 0.9, action = 0.1,” the algorithm **learns** these from rating data.
2. **Joint Optimization**  
   - We optimize over both user preferences ($w^{(j)}, b^{(j)}$) and item features ($x^{(i)}$) at the same time.
3. **Gradient Descent**  
   - We apply updates to all parameters, just like in linear regression, but now with more moving parts.
4. **Collaboration**  
   - Because many users rate the same movie, the system can infer the movie’s hidden personality and then use that to predict other users’ ratings.

With **collaborative filtering**, you don’t need a movie critic to define romance or action scores. The system **figures it out** by learning from everyone’s ratings. Next time you see a spot-on recommendation for a show or product you’ve never heard of, remember there’s a network of user-item interactions powering those predictions—**no manual labels required**!

---

### ***Binary Labels: favs, likes and clicks***

### ***Have You Ever Wondered How Apps Decide “Like” or “Dislike” Without Star Ratings?***

In many recommendation scenarios, users don’t provide a star rating of 1–5. Instead, the system sees **binary** actions (like a “thumbs up” or “no response”), or even no record at all if the user hasn’t been shown the item. How can we adapt our recommendation algorithms—originally designed for numerical ratings—to handle this 1/0 (plus “?”) setup? Let’s find out!

---

#### ***1. Binary Labels in Recommender Systems***

Instead of giving each movie 0 to 5 stars, users might:
- **Like** or **Favorite** an item (label = 1).
- **Not** engage with the item (label = 0).
- **Not** have been shown the item yet (label = ?).

You can think of this as a **binary classification** problem: for each (user, item) pair, is there a **positive** interaction (1) or **negative/none** (0)? The question mark means the user hasn’t even seen the item—so we don’t have any direct feedback.

##### ***Real-World Examples***
- **Online Shopping**: Did the user purchase the item after seeing it?  
- **Social Media**: Did the user click “like” on a post after viewing it?  
- **Video Platforms**: Did they watch at least 30 seconds or skip immediately?  
- **Advertisements**: Did they click on the ad or ignore it?

In all these cases, we store a **1** if they engaged, **0** if they saw but didn’t engage, and **?** if never shown.

---

#### ***2. From Linear Predictions to Logistic Predictions***

Previously, for star ratings, we used a **linear** model:  

$$
\text{Predicted rating} = w^{(j)} \cdot x^{(i)} + b^{(j)}.
$$

where $j$ is the user number, $i$ is the movie number, $w^{(j)}$ is the parameter vector for User $j$, $x^{(i)}$ is the feature vector for Movie $i$, and $b^{(j)}$ is the bias term for User $j$.

But with binary labels (0 or 1), it’s more natural to predict a **probability** that the user will “like” (i.e., label 1). So we switch to a **logistic** model:  

$$
\text{Probability}(y^{(i,j)}=1) = g\bigl(w^{(j)} \cdot x^{(i)} + b^{(j)}\bigr),
$$

where $g(z)$ is the logistic (sigmoid) function:  

$$
g(z) = \frac{1}{1 + e^{-z}}
$$

is the logistic (sigmoid) function.

- If $w^{(j)} \cdot x^{(i)} + b^{(j)}$ is large **positive**, the probability is near 1.
- If it’s large **negative**, the probability is near 0.

> **Analogy**: Think of $w^{(j)} \cdot x^{(i)} + b^{(j)}$ as a “like-hunch.” A high positive hunch means the algorithm strongly believes user $j$ will like item $i$. The logistic function just squeezes that hunch into a probability range (0 to 1).

---

#### ***3. Cost Function for Binary Labels***

When we used star ratings, we minimized **squared error**. But for 0/1 labels, a **logistic regression**-style cost works better:

1. **Prediction** for user $j$, item $i$:  

$$
f_{w,b,x}(i,j) = g\bigl(w^{(j)} \cdot x^{(i)} + b^{(j)}\bigr).
$$  

2. **Loss for a single (i,j)**:

$$
L\bigl(f_{w,b,x}(i,j), y^{(i,j)}\bigr) = - y^{(i,j)} \,\log\bigl(f_{w,b,x}(i,j)\bigr) -\bigl(1 - y^{(i,j)}\bigr)\log\bigl(1 - f_{w,b,x}(i,j)\bigr)
$$

- This is the **binary cross-entropy** (or log loss) we see in standard logistic regression.
- If $y^{(i,j)}=1$, the first log term matters more; if $y^{(i,j)}=0$, the second log term matters more.

3. **Summation Over Observed Interactions**: We only sum over $(i,j)$ where $r(i,j)=1$ (meaning user $j$ was actually shown item $i$):  

$$
J(w,b,x)= \sum_{(i,j) : r(i,j)=1} L\bigl(f_{w,b,x}(i,j), y^{(i,j)}\bigr) \;+\; \text{(regularization terms)}
$$

> **Analogy**: If you recall logistic regression from earlier sections, each data point’s “mistake” is measured by $-\log(\hat{p})$ if the label is 1, or $-\log(1-\hat{p})$ if the label is 0. Summing these up punishes overly confident but incorrect predictions more harshly than smaller mistakes.

---

#### 4. Regularization for $w$ and $x$

Just like before, we still want to prevent our parameters from blowing up. So we add terms like:

$$
\frac{\lambda}{2}
\sum_{j=1}^{n_u} \sum_{k=1}^{n} \bigl(w_{k}^{(j)}\bigr)^2
\quad\text{and}\quad
\frac{\lambda}{2}
\sum_{i=1}^{n_m} \sum_{k=1}^{n} \bigl(x_{k}^{(i)}\bigr)^2,
$$

to keep $w^{(j)}$ and $x^{(i)}$ from getting too large.

---

#### ***5. Putting It All Together***

1. **Initialize** all parameters ($w^{(j)}$, $b^{(j)}$, $x^{(i)}$) randomly.  
2. **Compute** the logistic cost $J(w,b,x)$ by:
   - Summing the binary cross-entropy over all known (i.e., shown) pairs $(i,j)$.
   - Adding regularization for $w$ and $x$.
3. **Use Gradient Descent** (or another optimization) to update $w^{(j)}$, $b^{(j)}$, and $x^{(i)}$.  
4. Keep iterating until the cost stabilizes.

After training, $g\bigl(w^{(j)} \cdot x^{(i)} + b^{(j)}\bigr)$ gives the **probability** user $j$ will “like” item $i$ if shown. This probability then guides the recommender system on **which items** to prioritize.

---

#### ***6. Why This Matters***

Binary-based feedback is **everywhere**:
- Shopping sites want to know if you **buy** (1) or **don’t buy** (0).
- Social platforms track if you **click** or **don’t click** on content.
- Video apps track if you **watch** for a while or **quit** quickly.

By extending collaborative filtering to handle **logistic-style** predictions, we open the door to a huge range of recommendation scenarios where users’ interactions aren’t star ratings but simple yes/no signals.

---

#### ***7. Key Takeaways***

- **Binary Labels**: 1 (engaged) or 0 (did not engage), plus “?” if not shown.
- **Logistic Function**: Converts a linear combination $(w^{(j)} \cdot x^{(i)} + b^{(j)})$ into a probability of “like.”
- **Binary Cross-Entropy Cost**: Replaces squared error to better handle yes/no labels.
- **Regularization**: Still crucial to keep user vectors ($w^{(j)}$) and item features ($x^{(i)}$) in a reasonable range.
- **Result**: A flexible system that learns how likely a user is to engage with new items—even if all you track is “like” vs. “not like.”

Next time you see a “thumbs up / thumbs down” system or an “Add to Favorites” button, remember: behind the scenes, a logistic-based recommender might be busy calculating the probability that you’ll love it—and showing it to you only if it’s high enough!

---