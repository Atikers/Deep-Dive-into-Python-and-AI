# Supervised Learning(5) - Cost Function

## ***Cost Function: Measuring How Good the Line Is***

After the computer draws a line, we need a way to measure how **good** the line is. Does it predict the prices accurately? This is where the **cost function** comes in. 

The **cost function** tells us how far off the predictions are from the actual prices. 

If the line makes perfect predictions, the cost will be low. If the predictions are bad, the cost will be high.

In machine learning, **minimizing the cost function** is the goal. We want the computer to find the best line (the one that makes the most accurate predictions) by making the cost as low as possible.

Ok. Let's Dive into the Cost Function.

The cost function tells us **how far off** our predictions are from the actual prices, and it helps us improve the line.

### ***Step-by-Step Explanation:***

1. **Predictions**: For each house in the dataset, our model makes a prediction $\hat{y}$ (y-hat) based on the size of the house.

2. **Errors**: The error is the difference between the model’s prediction $\hat{y}$ and the actual price $y$. In other words:

    $$Error = \hat{y} - y$$

3. **Squaring the Errors**: To avoid negative errors and make everything positive, we **square** the errors:

   $$(\hat{y} - y)^2$$

4. **Summing the Errors**: Since we have many houses in the dataset, we calculate the squared error for each house and **add** them all up.

5. **Averaging**: To avoid the sum from becoming too big, we take the **average** of these squared errors by dividing by the number of houses, **$m$**.

6. **Cost Function Formula**: Finally, we divide the whole thing by 2 (this makes some calculations easier later). The final **cost function** looks like this:

    $$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2$$

   Here, **$m$** is the number of houses, $\hat{y}^{(i)}$ is the predicted price for the i-th house, and ${y}^{(i)}$ is the actual price for the i-th house.

## ***Why Do We Use the Cost Function?***

The **cost function** helps us figure out if the line is fitting the data well. If the cost function is **small**, it means the line is doing a good job predicting prices. If the cost function is **large**, it means our predictions are far from the actual prices, and we need to adjust the line.

In machine learning, the goal is to **minimize** the cost function by adjusting **$w$** and **$b$** so that the line fits the data as closely as possible.

## ***Visualizing the Process***

Let’s imagine you are playing a game of **mini-golf**. You’re trying to hit the ball into the hole, but you start by hitting it too far or too short. 

Each time you hit the ball, you’re trying to **minimize** the distance between the ball and the hole. The smaller the distance, the closer you are to winning!

In the same way, the cost function helps us measure the **distance** between our predictions (the golf ball) and the actual prices (the hole). The goal is to keep adjusting the line (just like hitting the ball) until our predictions are as close as possible to the actual prices.

## ***Trying Different Values of w***

Let’s say we have a simple dataset with three points: (1, 1), (2, 2), and (3, 3). Now, we’ll see how the cost function changes when we try different values of **$w$** (and keep **$b$** equal to 0).

1. **When $w$ = 1**:  
   The line perfectly fits the data, so the error for each point is 0. This makes the total cost **0**, which is the best possible outcome.
   
2. **When $w$ = 0.5**:  
   The line doesn’t fit as well. For example, when $x$ = 2, our model predicts $\hat{y}$ = 1, but the actual value is $y$ = 2. The errors are bigger, so the cost increases.
   
3. **When $w$ = 0**:  
   The line is flat at 0, meaning it predicts the price is always 0, no matter the size. This creates even bigger errors, and the cost is much higher.

4. **When $w$ is negative**:  
   If **$w$** is a negative number, the line slopes downward, making the predictions very far from the actual values. This gives us an even higher cost.

As we change **$w$**, the cost function tells us which value of **$w$** gives us the best fit for the data by having the **smallest** cost.

## ***Visualizing the Cost Function in 3D***

To understand how the cost function works with both **$w$** and **$b$**, we can use a 3D graph. Imagine a big **bowl**. The bottom of the bowl represents the point where the cost function is the smallest, meaning our line fits the data perfectly.

- The **sides** of the bowl represent higher cost, meaning our line is not fitting well.
- The **bottom** of the bowl represents the lowest cost, meaning our line is fitting well.

If you adjust **$w$** and **$b$** and the point moves lower in the bowl, your line is getting better. If the point moves higher up the bowl, the line is getting worse.

## ***Contour Plots: A 2D View of the Cost Function***

To make it easier to see, we can also look at the **contour plot**, which is like a **top-down view** of the bowl. Think of it as if you are looking down at the bowl from a helicopter. Instead of seeing the full 3D bowl, you see **circles** (or ovals) that represent different heights.

- **Smaller circles**: Closer to the bottom of the bowl, where the cost is low (good fit).
- **Larger circles**: Higher up the bowl, where the cost is high (bad fit).

Each point on the contour plot represents a combination of **$w$** and **$b$**. The closer the point is to the center of the smallest circle, the better the fit of the line.