# ***Beoynd Supervised Learning(1) - Clustering***

## ***What Is Clustering?***  

**Have you ever wondered how an algorithm might organize a bunch of images, songs, or people’s profiles into neat little groups—even when you never told it which group they should belong to?** That’s the essence of **clustering**, a key technique in *unsupervised learning*.

Clustering is an **unsupervised learning** method. Unlike **supervised learning**, where you have input data $\mathbf{x}$ and *labels* $y$ (telling the algorithm which category something belongs to, or what value it should predict), in clustering you **only** have input data. There are **no labels** to guide the algorithm.  

> **Analogy**:  
> - **Supervised learning**: It’s like taking a cooking class where the teacher tells you exactly what dish to make (the *label*) and shows you examples. You learn to replicate that dish, following a clear recipe.  
> - **Unsupervised learning**: It’s like opening a **mystery box** of random ingredients with **no** recipe. You must figure out interesting ways to group them—perhaps by flavor, color, or region of origin. You’re discovering *patterns* on your own.

A **clustering** algorithm looks for data points that are naturally **similar** to each other, grouping them into **clusters**. For instance, if you have a dataset of many items—like news articles, market customers, or celestial objects—clustering tries to find subgroups or “clusters” of items that share **comparable** traits.

---

### ***1. Clustering vs. Supervised Learning***  

- **Supervised Learning**:  
  - You have labeled examples $\{(x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(m)})\}$.  
  - You might train a classifier (like **logistic regression** or a **neural network**) to predict whether each point belongs to category “A” (like the red crosses in a chart) or category “B” (like the blue circles).  
  - The model learns a **decision boundary** between these known categories.  

- **Unsupervised Learning (Clustering)**:  
  - You only have $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$. No labels.  
  - The algorithm tries to **discover** hidden structure in the data—often by grouping similar points together.  
  - You might end up with clusters, such as a cluster of very round items in one group and elongated items in another.

---

### ***2. Why Is Clustering Useful?***  

1. **Grouping Similar News Articles**  
   - Clustering can help sort breaking stories into categories (e.g., “Pandas at the zoo,” “Election updates,” “Celebrity news”), even when you haven’t told it the labels.  
   - Readers can quickly find news on the topic they’re interested in, without anyone manually labeling articles.

2. **Market Segmentation**  
   - You might have customers who buy a lot of tech gadgets, others who love sports gear, and still others who prefer art supplies.  
   - A clustering algorithm identifies these subgroups so you can tailor marketing strategies or product recommendations.

3. **DNA Analysis**  
   - Genetic data can be clustered to find people with similar gene expressions or risk factors, helping in **personalized medicine**.  
   - Scientists use clustering to discover hidden patterns in large genomic datasets.

4. **Astronomical Data**  
   - Clustering is used to analyze celestial objects—like which stars or galaxies form related groups in space.  
   - This helps astronomers identify **galaxies** or star clusters, fueling deeper insights about the universe’s structure.

> **Real-World Example**: Suppose you run a music service. You have thousands of songs but no labeling about genres. A clustering algorithm might group them by **tempo** or **instrumentation**—helping you discover hidden clusters like “jazzy-blues” or “fast techno” tracks, even though nobody labeled them as such.

---

### ***3. Key Idea Behind Clustering***  

- The algorithm measures how **close** or **similar** the data points are, often using distances (like **Euclidean distance**) between points in a feature space.  
- It **iteratively** forms clusters by grouping nearby points, then refines these clusters until each group is internally coherent (i.e., the points in it are more similar to each other than to points in other clusters).

Think of each data point as a **dot** in a 2D or multi-dimensional space:
- If a cluster is forming in the “top-right” region, points in that region are likely more similar to each other than to points in the “bottom-left.”  
- The algorithm dynamically adjusts which cluster each point belongs to until it finds a stable arrangement.

---

### ***4. Big Picture***  

Clustering solves the **puzzle**:  
> *“How can I automatically group items in my dataset without any labels?”*  

It’s especially powerful when you have:
- **Large volumes** of unlabeled data (e.g., mountains of news articles, music tracks, or sensor readings).  
- **No pre-existing labels** or categories.  
- **Curiosity** about how the data might be organized or segmented.

In the next sections, we’ll often explore **k-means**, one of the most common clustering algorithms. It uses a simple but powerful process to group data points around central “means” (like the “center” of a cluster).

---

### ***5. Looking Ahead***  

Clustering is just the **tip of the iceberg** in unsupervised learning:
- Other methods might compress data (like **dimensionality reduction**), discover outliers, or learn hidden patterns in advanced ways.  
- By learning clustering, you gain the ability to explore unknown data sets, leading to **fresh insights** or new ways to arrange and understand your information.

So if you’ve ever had a **big pile** of unlabeled data—be it texts, images, or numeric signals—and you wanted to see if there’s a **natural grouping** among them, clustering is your best friend. You’ll be amazed at how it can spot connections you never explicitly taught it.

---

## ***K-means Intuition***  

**Have you ever thought about how an algorithm might split a bunch of unlabeled points into two neat groups—just by guessing and then adjusting those guesses step by step?** That’s exactly what the **K-means** algorithm does. It repeatedly **assigns** each point to a cluster, then **repositions** the cluster’s center until things settle into a stable arrangement.

---

### ***1. The Two Steps of K-means***  

Imagine you have a set of data points $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$ plotted on a 2D graph, but **no labels** telling you which points belong together. If you want **two** clusters, K-means will:

1. **Initialize**:  
   - Pick two random “centroid” positions (they can be anywhere). Think of these as the centers of two clusters.  
   - Mark them with, say, a red cross and a blue cross.

2. **Repeat Two Key Operations**:  
   1. **Assign each point** to the **closest** centroid (using a distance measure like Euclidean distance).  
      - Paint each point red if it’s closest to the red centroid, or blue if it’s closer to the blue centroid.  
   2. **Recompute each centroid** as the **average** (mean) of all points assigned to it.  
      - Gather all the red points, find their average location, and *move* the red cross there.  
      - Do the same for the blue points and move the blue cross to that average position.

You keep doing this—reassigning points to the centroid they’re nearest to, then moving the centroids to the mean of their points—until no more changes occur. At that moment, **K-means has converged**.

> **Analogy**: Think of two friends trying to meet in a field full of scattered items. Each friend stands somewhere (two random spots), and every item in the field “walks” to the friend it’s closest to. Each friend then realizes they’re standing in the middle of all these items, so they move to the *average* position of their items. Items then re-check which friend is now closer and shuffle around. This repeats until nobody moves anymore. It is like a **game** of **"둥글게 둥글게"** in Korea.

---

### ***2. How It Plays Out***  

1. **Initial Guess**  
   - You place the red cross and the blue cross in random spots.  

2. **Assign Points**  
   - Every data point checks: “Which cross am I closest to?”  
   - That point takes on the color of the nearest cross, meaning it’s assigned to that cluster.

3. **Recompute Centroids**  
   - For the red cluster: find the average (mean) location of all red points, then move the red cross there.  
   - For the blue cluster: do the same with the blue points.  

4. **Repeat**  
   - With the new cross locations, points might switch clusters if they become closer to the other cross.  
   - The crosses move again. Eventually, the system settles so points no longer change color and the crosses stop moving.

In the end, you get two stable clusters.

---

### ***3. Why Does This Work?***  

- **Distance Minimization**: K-means tries to minimize the sum of the distances from each point to its assigned centroid. By repeatedly **assigning** points and then **moving** centroids, the algorithm systematically reduces that total distance—until no further improvement is possible.  
- **Natural Grouping**: When you look at your data, you might see that it forms natural “blobs.” K-means typically finds these **blobs** or groups by concentrating each centroid in the thick of a set of points.

> **Extra Example**: If you have a thousand pictures of faces with no labels, K-means might separate them into two big groups: faces with glasses (cluster 1) and faces without glasses (cluster 2). You didn’t tell the algorithm about glasses, but the distance between pictures might capture that similarity.

---

### ***4. Choosing the Number of Clusters***  

Here, we chose **2** clusters for illustration. But you can ask K-means to find **3**, **4**, or more. The right number often depends on:
- **Domain knowledge** (e.g., you know you have three main categories of something).  
- **Heuristics** (like the “elbow method”) to see how well the data is being grouped as you vary the number of clusters.

---

### ***5. Limitations and Tips***  

- **Initialization**: Because the starting positions of the centroids are random, K-means can converge to **different** solutions. Running K-means multiple times with different initial guesses can help find a better overall solution.  
- **Shape of Clusters**: K-means works best for somewhat **circular** clusters. If your data clusters are long and skinny or not blob-like, K-means may split them awkwardly.
- **Outliers**: A few extreme points can pull the centroid positions in surprising ways.

---

### ***6. Why Use K-means?***  

- **Simplicity**: Easy to understand and implement.  
- **Efficiency**: Works fairly quickly on large datasets.  
- **Versatility**: Useful in marketing, image processing, biology, astronomy, and more when you need a *quick, automatic grouping* of data.

> **Real-World Example**:  
> A streaming service might apply K-means to user viewing data to **cluster** customers with similar taste in movies. One cluster might love action films, another cluster might love romantic comedies, and so on. The service can then recommend content relevant to each user’s cluster.

---

### ***7. Summary***  

K-means **clusters** data by iteratively:
1. **Assigning** each point to its closest centroid.
2. **Recomputing** each centroid as the average of its points.

It repeats these two steps until convergence, revealing a hidden grouping in your data—without any labels or human supervision. Despite its simplicity, K-means is a powerful tool for discovering **natural structures** and patterns in large, unlabeled datasets.

---

## ***K-means Algorithm***

**Have you ever wondered how your favorite online store might decide on three different T-shirt sizes—small, medium, and large—when everyone’s body shape is unique?** That’s where an algorithm like **K-means** comes in. Even if people’s heights and weights form a big messy cloud of points, K-means can find three “centers,” letting you design three shirt sizes that fit most people. Let’s see **how** K-means works step by step.

---

### ***1. Overview of the Algorithm***

1. **Randomly Initialize Cluster Centroids**  
   - Suppose you want to find $K$ clusters. You’ll pick $K$ random points in the same space as your data.  
   - These points, called **centroids**, are each represented by a vector $\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \ldots, \boldsymbol{\mu}_K$.  
   - For example, with $K=2$, you might have a **red centroid** ($\boldsymbol{\mu}_1$) and a **blue centroid** ($\boldsymbol{\mu}_2$) placed randomly.

2. **Repeat Two Steps**  
   1. **Assign Points to Clusters**  
      - For each data point $\boldsymbol{x}^{(i)}$, figure out which centroid is closest by comparing distances.  
      - More precisely, we set: $c^{(i)} = \underset{k \in \{1,\dots,K\}}{\mathrm{argmin}}\;\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\|^2$

      - If $\boldsymbol{x}^{(i)}$ is closest to $\boldsymbol{\mu}_2$, we call it part of cluster 2, and so on.  
      - **Hint**: Minimizing squared distance $\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\|^2$ is equivalent to minimizing the actual distance $\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\|$.  

   2. **Move Each Centroid**  
      - After assigning all points, recalculate each centroid’s position as the **mean** of the points in that cluster.  
      - For centroid $k$, if it has, say, $r$ points assigned, then: $\boldsymbol{\mu}_k := \frac{1}{r}\sum_{\{\boldsymbol{x}^{(i)} \mid c^{(i)} = k\}} \boldsymbol{x}^{(i)}$
      - This shifts $\boldsymbol{\mu}_k$ to the center of its assigned points.

3. **Convergence**  
   - Keep **repeating** these two steps (assigning points, then moving centroids) until no points change their cluster assignment—or the centroids no longer move.  
   - At that point, K-means has “settled,” and you have $K$ stable clusters.

---

### ***2. Dealing with a Cluster That Loses All Points***

- **Corner Case**: What if none of the points get assigned to centroid $k$? Then you can’t compute an average for that centroid because there are $0$ points!  
- **Common Fix**: Simply remove that cluster (now you have $K-1$ clusters), or randomly reposition that centroid to break the tie.

---

### ***3. Summary of the Steps***

1. **Initialize** $\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_K$ randomly.  
2. **Repeat**:
   1. **Assign** each $\boldsymbol{x}^{(i)}$ to its **closest** centroid:
```math
c^{(i)} = \underset{k}{\mathrm{argmin}}\;\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\|^2
```
   2. **Recompute** each $\boldsymbol{\mu}_k$ as the mean of its assigned points:
```math
\boldsymbol{\mu}_k := \frac{1}{|\{\boldsymbol{x}^{(i)} : c^{(i)} = k\}|} \sum_{\{\boldsymbol{x}^{(i)} : c^{(i)} = k\}} \boldsymbol{x}^{(i)}
```

Repeat until **no changes** occur or you decide to stop after a certain number of iterations.

---

### ***4. Wrapping Up***

With **K-means**, you can discover **natural clusters** in your data—entirely unlabeled. Whether you’re dividing customers into groups, organizing music tracks by style, or choosing T-shirt sizes, K-means helps you find meaningful “centers” based on the **similarities** between data points. And all you need to start are the data points themselves—no labels required.

So, the next time you face a sea of unlabeled data and want to spot **hidden patterns**, remember: K-means can guide you to the **centers** of those patterns, one iteration at a time!

---

## ***Optimization Objective***

**Have you ever wondered if there's a way to measure how “good” your clusters are—so you can be sure your K-means algorithm is doing the best job possible?** That’s where the **K-means cost function** comes in!

---

### ***1. Why a “Cost Function”?***

When you learned about **supervised** algorithms (like linear regression), you might have seen a **cost function** that measures how well your model fits the data. K-means has a similar idea: we define a number that measures how “spread out” the data points are *within* their clusters. The algorithm then tries to **minimize** this number, so points end up *close* to their cluster centers.

---

### ***2. K-means Cost Function (a.k.a. “Distortion”)***

In K-means, each data point $\boldsymbol{x}^{(i)}$ is assigned to a cluster $c^{(i)}$, which is an integer from $1$ to $K$. Each cluster has a **centroid** (center) $\boldsymbol{\mu}_k$. If $\boldsymbol{x}^{(i)}$ is in cluster $k$, we’ll say $ c^{(i)} = k$, and the centroid for that cluster is $\boldsymbol{\mu}_{c^{(i)}}$.

The cost function (also called “distortion”) is:
```math
J\bigl(c^{(1)}, \dots, c^{(m)}, \boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_K\bigr) 
= \frac{1}{m} \sum_{i=1}^m \bigl\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_{c^{(i)}}\bigr\|^2
```

- The term $\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_{c^{(i)}}\|^2$ is the **squared distance** between point $i$ and the centroid of the cluster it’s assigned to.
- We **average** this squared distance over all $m$ points to get one final number.

> **Idea**: If each point is *really close* to its centroid, then $\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_{c^{(i)}}\|$ is small, so the cost $J$ is small. If some points are *far* from their centroid, the cost is higher.

---

### ***3. How K-means Minimizes This Cost***

K-means does **two steps** again and again, each time shrinking the cost function:

1. **Assign Points**  
   - For each point $\boldsymbol{x}^{(i)}$, we choose the cluster $k$ that makes the distance $\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\|$ the smallest.  
   - In math form, we say:
```math
c^{(i)} = \underset{k \in \{1,\dots,K\}}{\mathrm{argmin}} \;\bigl\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\bigr\|
```
   - This step tries to **reduce** the cost by putting each point into the cluster whose centroid is nearest.

2. **Update Centroids**  
   - After assigning points, for each cluster $k$, move $\boldsymbol{\mu}_k$ to the **average** of the points that ended up in cluster $k$.  
   - Example: If cluster $k$ has points $\boldsymbol{x}^{(5)}, \boldsymbol{x}^{(8)}, \boldsymbol{x}^{(9)}$, then
```math
\boldsymbol{\mu}_k := \frac{1}{3}\Bigl(\boldsymbol{x}^{(5)} + \boldsymbol{x}^{(8)} + \boldsymbol{x}^{(9)}\Bigr)
```
   - Placing the centroid at the mean makes all those distances $\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_k\|$ as small as possible for cluster $k$.

**Each** of these steps either decreases the cost function $J$ or keeps it the same. Since you can’t decrease it forever, eventually the changes stop, which means K-means has *converged*.

---

### ***4. Visual Example***

Imagine you have a group of **red** points and **blue** points. If you compute the cost $J$:
- You measure how far each red point is from the **red** centroid and square that distance.
- You measure how far each blue point is from the **blue** centroid and square that distance.
- Then you **average** all those squared distances to get one final score.

As K-means runs, it *reduces* this average. In the end, each group of points is clustered as tightly as possible around its centroid.

---

### ***5. Corner Cases and Convergence***

- **No Points in a Cluster**: If a cluster ends up with **no** points, you might remove that cluster or pick a new random centroid location.
- **Convergence**: Because every step tries to **lower** or maintain the cost $J$, once $J$ stops decreasing, you know K-means has finished. You can then use the final centroids and assignments as your *clusters*.

---

### ***6. Why a Cost Function Is Helpful***

1. **Monitors Progress**: You can watch $J$ go down each iteration to see if the algorithm is still improving.  
2. **Stops at Convergence**: Once $J$ barely changes, you can stop because the clusters won’t get better.  
3. **Compares Multiple Initializations**: If you run K-means multiple times with different random starting points, you can pick the result that has the **lowest** cost $J$.

---

### ***7. Real-World Example***

Imagine you want to group **photos** with no labels. By minimizing the K-means cost function, you’ll end up with pictures that look alike (similar colors or shapes) in the same cluster. That’s because each cluster’s centroid ends up close to the pictures that share the same traits—making the overall cost lower.

---

### ***8. Wrap-Up***

The K-means **cost function**:
```math
J = \frac{1}{m} \sum_{i=1}^m \bigl\|\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_{c^{(i)}}\bigr\|^2
```
tells us exactly **how “good” or “tight”** our clusters are. Each step of K-means (assigning points, then updating centroids) tries to **shrink** this cost. Eventually, you get stable clusters that keep your data points close to their respective centroids.

So whenever you want to cluster unlabeled data—pictures, user information, or anything else—K-means “knows” it’s doing a good job by measuring this average squared distance. The *lower* it goes, the *better* your clusters.
