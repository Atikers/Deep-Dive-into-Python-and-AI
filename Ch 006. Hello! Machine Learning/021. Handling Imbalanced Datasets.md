# Handling Imbalanced Datasets

![alt text](images/image-21.png)

## ***Error Metrics for Imbalanced Datasets***

When working on **classification** tasks where the **positive** class (e.g., presence of a rare disease) is very **uncommon**, standard metrics like **accuracy** can be **misleading**. Let's explore why‚Äîand how **precision** and **recall** give a more meaningful view.

---

### ***1. The Problem with Accuracy on Rare Classes***

Imagine you‚Äôre trying to detect a **rare disease**:

- **Labels**:  
  - $y = 1$ if disease is present  
  - $y = 0$ otherwise

- **Apparent Success**:  
  Suppose you achieve **1% error** on your test set (i.e., **99% accuracy**). That might sound great at first.

- **But...**:  
  If only **0.5%** of patients actually have the disease, a naive program that **always predicts $y=0$** (disease absent) would be correct for **99.5%** of patients‚Äîbecause 99.5% don‚Äôt have the disease!  
  - This ‚Äúdumb‚Äù strategy yields **0.5% error**, which **outperforms** your 1% error classifier on sheer accuracy alone.  
  - But it‚Äôs clearly **useless**‚Äîit never catches any true cases.

> **Analogy**:  
> Imagine there‚Äôs a school of 1,000 students, and only 5 of them cheat on tests. If you accuse **no one** of cheating, you‚Äôre ‚Äúcorrect‚Äù 995 times out of 1,000 ‚Üí 99.5% accuracyüòÇ. But you fail to identify **any** actual cheaters.

Hence, with **rare positives**, high accuracy does **not** necessarily mean you‚Äôre good at **finding** those positives.

---

### ***2. Confusion Matrix Basics***

Instead of relying on raw accuracy, it‚Äôs helpful to examine a **confusion matrix**, which breaks predictions down into:

|               | **Actual: 1** | **Actual: 0** |
|---------------|---------------|---------------|
| **Predicted: 1** | True Positive (TP) | False Positive (FP) |
| **Predicted: 0** | False Negative (FN)| True Negative (TN)  |

- **True Positive (TP)**: The model correctly predicts $y=1$ for a positive example.  
- **False Positive (FP)**: The model incorrectly predicts $y=1$ when it‚Äôs actually 0.  
- **False Negative (FN)**: The model incorrectly predicts $y=0$ when it‚Äôs actually 1.  
- **True Negative (TN)**: The model correctly predicts $y=0$ for a negative example.

For example, if you have 100 test examples and your confusion matrix ends up being:

- TP = 15  
- FP = 5  
- FN = 10  
- TN = 70  

That means:
- 25 examples (15 + 10) were actually $y=1$, and
- 75 examples (5 + 70) were actually $y=0$.

---

### ***3. Precision and Recall***

To judge performance **beyond** accuracy, we define two key metrics:

1. **Precision**  
   - Measures: ‚ÄúOut of all the positives you **predicted**, how many are **actually** positive?‚Äù  
   - Formula:  
     $$\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$$
   - In the example:  
     $$\text{TP} = 15, \quad \text{FP} = 5 \quad\Longrightarrow\quad\text{Precision} = \frac{15}{15 + 5} = 0.75$$
   - Interpretation: If the model says, ‚ÄúPatient has disease,‚Äù there‚Äôs a **75%** chance they really have it.

2. **Recall**  
   - Measures: ‚ÄúOut of all the **actual** positives, how many did you **correctly** identify?‚Äù  
   - Formula:  
     $$\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$$
   - In the example:  
     $$\text{TP} = 15, \quad \text{FN} = 10 \quad\Longrightarrow\quad\text{Recall} = \frac{15}{15 + 10} = 0.60$$
   - Interpretation: Among patients who **actually** have the disease, the model catches **60%** of them.

> **Analogy**:
> - **Precision**: If you **claim** 20 people are infected, are you correct on most of those claims?  
> - **Recall**: Out of the **whole** infected population, how many did you **find**?

---

### ***4. Why Precision & Recall Help***

- **Avoids ‚ÄòAll Negative‚Äô Pitfall**  
  - If your model **never** predicts $y=1$, your recall becomes **0%**‚Äîhelping you spot a trivial strategy that ‚Äúachieves‚Äù high accuracy by ignoring positives entirely.  
- **Balance Both**  
  - You want a model that, when it says ‚ÄúYes, disease,‚Äù it‚Äôs **trustworthy** (high precision), **and** it actually **finds** a decent portion of diseased patients (high recall).  
- **Trade-offs**  
  - Often, you‚Äôll tweak thresholds to improve recall (catch more positives) but risk lowering precision (more false alarms). We‚Äôll discuss that further next.

---

### ***5. Additional Examples***

- **Fraud Detection**  
  - Only 0.1% of transactions are fraudulent. High accuracy can be achieved by labeling almost everything ‚Äúlegitimate,‚Äù but that‚Äôs not **useful**‚Äîyou‚Äôll never catch actual fraudsters. Precision and recall let you measure how well you‚Äôre flagging real fraud.

- **Defect Detection in Manufacturing**  
  - If only a small fraction of items are defective, a naive approach of labeling all as ‚Äúgood‚Äù yields high accuracy. But you want good recall (finding actual defects) **and** good precision (not discarding too many good items).

---

### ***6. Key Takeaways***

1. **Accuracy** can be **misleading** with skewed classes.  
2. **Precision** (quality of positive predictions) and **Recall** (coverage of actual positives) are crucial.  
3. If your model is ‚Äúmissing‚Äù positives entirely, recall will be **low**, even if overall accuracy is high.  
4. You‚Äôll often tune your model to **balance** these metrics, considering the cost of a false positive vs. a false negative.

In **rare class** problems, always check metrics like **precision** and **recall** (and often **F1 score**, which we saw in the previous chapter), rather than relying on accuracy alone. Next, we‚Äôll learn how to **trade off** between precision and recall to optimize your model‚Äôs performance for skewed datasets.

---

## ***Trading Off Precision and Recall***

In an **ideal scenario**, we want a classifier to have **both** high precision **and** high recall:

- **High Precision**: When the classifier says ‚ÄúYes‚Äù (e.g., the patient **has** the disease), it‚Äôs **very likely** correct.  
- **High Recall**: Among all real ‚ÄúYes‚Äù cases, the classifier **catches** most of them.

However, in practice, there's often a **trade-off** between these two metrics. Let‚Äôs explore how adjusting a **decision threshold** can shift precision and recall, and how to choose a good balance.

---

### ***1. Precision & Recall (Recap)***

Recall that:
- **Precision** $$= \frac{\text{true positives}}{\text{true positives + false positives}}$$  
  Measures how many ‚Äúpositive‚Äù predictions are **actually** correct. $(\text{True positives+False positives})$ is the number of $\text{Total Predicted Positive}$.

- **Recall** $$= \frac{\text{true positives}}{\text{true positives + false negatives}}$$  
  Measures how many of the **actual** positives we **successfully** identify. $(\text{True positives+False negatives})$ is the number of $\text{Total Actual Positive}$.

---

### ***2. Using a Threshold on Model Outputs***

#### ***Logistic Regression Example***
A **logistic regression** model outputs a probability $f_{\vec{\text{w}},b}(x)$ between **0** and **1**. Commonly:
- Predict $y=1$ if $f_{\vec{\text{w}},b}(x) \geq 0.5$  
- Predict $y=0$ otherwise

But you‚Äôre **not** restricted to 0.5. By changing this **threshold**, you can shift precision and recall:

1. **High Threshold** (e.g., 0.7 or 0.9)  
   - You only predict $y=1$ if you‚Äôre **very confident**.  
   - **Consequence**:  
     - **Precision** goes **up** (fewer false positives: you rarely say ‚ÄúYes‚Äù unless you‚Äôre sure).  
     - **Recall** goes **down** (you may miss some real positives because you‚Äôre being ‚Äústrict‚Äù).  

2. **Low Threshold** (e.g., 0.3 or 0.1)  
   - You predict $y=1$ more **easily** (whenever $f_{\vec{\text{w}},b}(x)$ is even slightly above that low cutoff).  
   - **Consequence**:  
     - **Precision** goes **down** (more false positives: you‚Äôre quick to say ‚ÄúYes‚Äù).  
     - **Recall** goes **up** (you catch more true positives, because you say ‚ÄúYes‚Äù more often).

> **Analogy**:  
> - **High threshold**: A teacher who only gives an **A** grade if a student scores **95%** or above on exams (high precision: an ‚ÄúA‚Äù means they‚Äôre truly top-notch, but many good students might miss out).  
> - **Low threshold**: A teacher who gives **A‚Äôs** to anyone scoring **70%** or above (high recall: more students get ‚ÄúA,‚Äù but some are borderline).

---

### ***3. The Precision‚ÄìRecall Curve***

If you **vary** the threshold from **0** to **1**, you trace out a **precision‚Äìrecall** curve. Typically:

- **High threshold** ‚Üí high precision, low recall  
- **Lower threshold** ‚Üí lower precision, higher recall

You can **plot** these (precision on one axis, recall on the other) and observe the **trade-off**. Each **point** on the curve corresponds to a specific threshold setting.

> **Why Not Use Cross-Validation to Pick the Threshold?**  
> While you can **evaluate** different thresholds on a validation set, the final choice often depends on **domain-specific** factors (e.g., cost of false positives vs. false negatives). It‚Äôs usually a **manual** or **business** decision rather than an automated one.

---

### ***4. Choosing a Threshold***

How do you pick the **best** threshold? It depends on your application‚Äôs **priorities**:

- **Cost of False Positives** vs. **Cost of False Negatives**  
  - For a **rare disease** test: If diagnosing incorrectly means expensive but not life-threatening follow-up tests, you might prefer **higher recall** to avoid missing real cases.  
  - If wrongly diagnosing someone leads to a serious, risky surgery, you might want **high precision** so that when you say ‚ÄúYes,‚Äù you‚Äôre more certain.

- **Domain Knowledge**  
  - E.g., in **spam filtering**, a small number of false positives (legitimate emails flagged as spam) might be more annoying than letting a few spam emails in. Or maybe your preference is reversed.  
  - In a **fault detection** system, missing a dangerous fault could be disastrous, so recall is key.

---

### ***5. Beyond Manual Thresholding: The F1 Score***

Sometimes, you‚Äôd like a **single** metric that balances precision and recall. One popular choice:

$$
F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

- $F_1$ is the **harmonic mean** of precision and recall, emphasizing **both**.  

> **Note**:  
> The harmonic mean is a type of average that is calculated by:
> 1. Taking the reciprocal (1/x) of each number
> 2. Finding their arithmetic mean
> 3. Taking the reciprocal of the result
> 
> For the F1 score specifically, the formula is:
> $$F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$
> 
> Here's why harmonic mean is used instead of arithmetic mean:
> 
> 1. **Sensitivity to low values**: The harmonic mean is always lower than the arithmetic mean (except when all numbers are equal). It's particularly sensitive to low values in the data set. This is perfect for F1 score because:
>    - If either precision OR recall is very low, the F1 score will be low
>    - You can't compensate for a terrible precision with a perfect recall (or vice versa)
> 
> 2. **Example**:
>    - If precision = 1.0 and recall = 0.01
>    - Arithmetic mean would be (1.0 + 0.01)/2 = 0.505 (seems okay)
>    - Harmonic mean (F1) would be ‚âà 0.0198 (shows the true poor performance)
> 
> This is why F1 score is a better metric than simple averaging when you need to balance precision and recall in imbalanced datasets. It penalizes extreme imbalances between precision and recall.

- If either precision **or** recall is low, $F_1$ remains **low**.  
- This metric can help **automate** model selection or threshold choice when you want a balanced approach to both.

> **Example**:  
> If you have three different classifiers with (Precision, Recall) pairs:
> - Algorithm A: (0.5, 0.4) ‚Üí $F_1 \approx 0.44$  
> - Algorithm B: (0.7, 0.1) ‚Üí $F_1 \approx 0.18$  
> - Algorithm C: (0.02, 1.0) ‚Üí $F_1 \approx 0.04$  
> 
> Even though B has higher precision than A, it has terrible recall. C has perfect recall but nearly zero precision. **F1** highlights that A‚Äôs balanced performance is best among the three.

---

### ***6. Additional Examples***

1. **Medical Diagnosis**:  
   - **Lower Threshold**: Catch as many diseases as possible (high recall). Accept some healthy patients falsely flagged (lower precision).  
   - **Higher Threshold**: Only label severe cases. Reduces false positives (higher precision), but risk missing actual cases (lower recall).

2. **School Admission**:  
   - If you reduce the threshold for admission, more students get accepted (high recall), but more may be unprepared for the program (lower precision).  
   - If you raise the bar, you get fewer but better-prepared admits (higher precision), possibly excluding good candidates on the edge (lower recall).

---

### ***7. Key Takeaways***

- **Precision‚ÄìRecall Trade-off**: A fundamental consideration in **imbalanced** or **high-stakes** classification tasks.  
- **Threshold Tuning**: Adjusting the probability cutoff changes how strictly you label positives or negatives.  
- **No One-Size-Fits-All**: The ‚Äúoptimal‚Äù threshold depends on **practical** or **business** factors (costs/risks).  
- **F1 Score**: A single metric to combine precision and recall, especially useful if you need **balance**.

In short, whenever you face a **rare** or **important** class, think carefully about how to **balance** finding true positives (recall) against making sure your positives are correct (precision). Setting the right threshold‚Äîand possibly using metrics like F1‚Äîcan help you build more **effective** and **responsible** classifiers.