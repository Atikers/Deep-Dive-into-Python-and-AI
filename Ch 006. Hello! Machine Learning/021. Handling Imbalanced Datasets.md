# Handling Imbalanced Datasets

## Error Metrics for Imbalanced Datasets

When working on **classification** tasks where the **positive** class (e.g., presence of a rare disease) is very **uncommon**, standard metrics like **accuracy** can be **misleading**. Let's explore why‚Äîand how **precision** and **recall** give a more meaningful view.

---

### 1. The Problem with Accuracy on Rare Classes

Imagine you‚Äôre trying to detect a **rare disease**:

- **Labels**:  
  - $y = 1$ if disease is present  
  - $y = 0$ otherwise

- **Apparent Success**:  
  Suppose you achieve **1% error** on your test set (i.e., **99% accuracy**). That might sound great at first.

- **But...**:  
  If only **0.5%** of patients actually have the disease, a naive program that **always predicts $y=0$** (disease absent) would be correct for **99.5%** of patients‚Äîbecause 99.5% don‚Äôt have the disease!  
  - This ‚Äúdumb‚Äù strategy yields **0.5% error**, which **outperforms** your 1% error classifier on sheer accuracy alone.  
  - But it‚Äôs clearly **useless**‚Äîit never catches any true cases.

> **Analogy**:  
> Imagine there‚Äôs a school of 1,000 students, and only 5 of them cheat on tests. If you accuse **no one** of cheating, you‚Äôre ‚Äúcorrect‚Äù 995 times out of 1,000 ‚Üí 99.5% accuracyüòÇ. But you fail to identify **any** actual cheaters.

Hence, with **rare positives**, high accuracy does **not** necessarily mean you‚Äôre good at **finding** those positives.

---

### 2. Confusion Matrix Basics

Instead of relying on raw accuracy, it‚Äôs helpful to examine a **confusion matrix**, which breaks predictions down into:

|               | **Actual: 1** | **Actual: 0** |
|---------------|---------------|---------------|
| **Predicted: 1** | True Positive (TP) | False Positive (FP) |
| **Predicted: 0** | False Negative (FN)| True Negative (TN)  |

- **True Positive (TP)**: The model correctly predicts $y=1$ for a positive example.  
- **False Positive (FP)**: The model incorrectly predicts $y=1$ when it‚Äôs actually 0.  
- **False Negative (FN)**: The model incorrectly predicts $y=0$ when it‚Äôs actually 1.  
- **True Negative (TN)**: The model correctly predicts $y=0$ for a negative example.

For example, if you have 100 test examples and your confusion matrix ends up being:

- TP = 15  
- FP = 5  
- FN = 10  
- TN = 70  

That means:
- 25 examples (15 + 10) were actually $y=1$, and
- 75 examples (5 + 70) were actually $y=0$.

---

### 3. Precision and Recall

To judge performance **beyond** accuracy, we define two key metrics:

1. **Precision**  
   - Measures: ‚ÄúOut of all the positives you **predicted**, how many are **actually** positive?‚Äù  
   - Formula:  
     $$\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$$
   - In the example:  
     $$\text{TP} = 15, \quad \text{FP} = 5 \quad\Longrightarrow\quad\text{Precision} = \frac{15}{15 + 5} = 0.75$$
   - Interpretation: If the model says, ‚ÄúPatient has disease,‚Äù there‚Äôs a **75%** chance they really have it.

2. **Recall**  
   - Measures: ‚ÄúOut of all the **actual** positives, how many did you **correctly** identify?‚Äù  
   - Formula:  
     $$\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$$
   - In the example:  
     $$\text{TP} = 15, \quad \text{FN} = 10 \quad\Longrightarrow\quad\text{Recall} = \frac{15}{15 + 10} = 0.60$$
   - Interpretation: Among patients who **actually** have the disease, the model catches **60%** of them.

> **Analogy**:
> - **Precision**: If you **claim** 20 people are infected, are you correct on most of those claims?  
> - **Recall**: Out of the **whole** infected population, how many did you **find**?

---

### 4. Why Precision & Recall Help

- **Avoids ‚ÄòAll Negative‚Äô Pitfall**  
  - If your model **never** predicts $y=1$, your recall becomes **0%**‚Äîhelping you spot a trivial strategy that ‚Äúachieves‚Äù high accuracy by ignoring positives entirely.  
- **Balance Both**  
  - You want a model that, when it says ‚ÄúYes, disease,‚Äù it‚Äôs **trustworthy** (high precision), **and** it actually **finds** a decent portion of diseased patients (high recall).  
- **Trade-offs**  
  - Often, you‚Äôll tweak thresholds to improve recall (catch more positives) but risk lowering precision (more false alarms). We‚Äôll discuss that further next.

---

### 5. Additional Examples

- **Fraud Detection**  
  - Only 0.1% of transactions are fraudulent. High accuracy can be achieved by labeling almost everything ‚Äúlegitimate,‚Äù but that‚Äôs not **useful**‚Äîyou‚Äôll never catch actual fraudsters. Precision and recall let you measure how well you‚Äôre flagging real fraud.

- **Defect Detection in Manufacturing**  
  - If only a small fraction of items are defective, a naive approach of labeling all as ‚Äúgood‚Äù yields high accuracy. But you want good recall (finding actual defects) **and** good precision (not discarding too many good items).

---

### 6. Key Takeaways

1. **Accuracy** can be **misleading** with skewed classes.  
2. **Precision** (quality of positive predictions) and **Recall** (coverage of actual positives) are crucial.  
3. If your model is ‚Äúmissing‚Äù positives entirely, recall will be **low**, even if overall accuracy is high.  
4. You‚Äôll often tune your model to **balance** these metrics, considering the cost of a false positive vs. a false negative.

In **rare class** problems, always check metrics like **precision** and **recall** (and often **F1 score**, which we saw in the previous chapter), rather than relying on accuracy alone. Next, we‚Äôll learn how to **trade off** between precision and recall to optimize your model‚Äôs performance for skewed datasets.