# Bias and Variance

One of the most important questions in **machine learning** is figuring out **why** your model isn’t performing as well as you’d like. Is it because it doesn’t match the training data well enough (high bias / underfitting)? Or is it because it memorized the training data too closely and doesn’t generalize (high variance / overfitting)? Understanding **bias** and **variance** will help you decide **what to do next** to improve your model.

---

## Diagnosing Bias and Variance

### 1. What Are Bias and Variance?

- **High Bias (Underfitting)**:  
  - The model doesn’t capture the underlying trend well enough.  
  - As a result, it performs **poorly** even on the **training set**.  
  - In other words, the model is **too simple**.  

- **High Variance (Overfitting)**:  
  - The model matches the training data **very closely**, sometimes capturing noise or random fluctuations.  
  - It does great on training data but **fails** when presented with **new** (unseen) data.  
  - In other words, the model is **too complex**.  

Think of **bias** like a **bad pair of glasses** that blurs everything you see—you consistently see things incorrectly (the model is too simple, so it misses important details).  
Think of **variance** like an **overly tight glove** that fits your hand so precisely it can’t fit any hand that’s even slightly different (the model is so specifically tailored to the training data that it doesn’t fit new data well).

---

### 2. Diagnosing Bias vs. Variance

To **systematically** tell if your model has high bias or high variance, we look at **two errors**:

1. **Training Error** ($J_{\text{train}}$): How well the model does on the **same data** it was trained on.  
2. **Cross-Validation Error** ($J_{\text{cv}}$): How well the model does on **new** examples it hasn’t seen during training (the cross-validation set).

#### High Bias Signature
- $J_{\text{train}}$ is **high**.  
- $J_{\text{cv}}$ is also **high** (and usually close to $J_{\text{train}}$).  
- Interpretation: The model can’t even learn the training data well → **underfitting**.

#### High Variance Signature
- $J_{\text{train}}$ is **low**.  
- $J_{\text{cv}} \gg J_{\text{train}}$ (much higher than training error).  
- Interpretation: The model memorizes the training data but fails on new data → **overfitting**.

#### Just Right (Low Bias, Low Variance)
- $J_{\text{train}}$ is **low**.  
- $J_{\text{cv}}$ is also **low** and **close** to $J_{\text{train}}$.  
- Interpretation: The model fits the data well **and** generalizes well.

---

### 3. Visualizing Bias and Variance

Imagine you’re predicting **house prices** based on **size**:
- **Low-Degree Polynomial** (e.g., a **straight line**, $d=1$):  
  - Likely **high bias**. The line is too simple to capture the housing data’s real pattern.  
  - You’ll see both $J_{\text{train}}$ and $J_{\text{cv}}$ are **high**.

- **High-Degree Polynomial** (e.g., $d=4$ or $d=10$):  
  - Likely **high variance**. The curve may wiggle around to fit training points perfectly, but fails on new data.  
  - You’ll see $J_{\text{train}}$ is very **low** but $J_{\text{cv}}$ is **much higher**.

- **Moderate-Degree Polynomial** (e.g., $d=2$):  
  - Often a **good balance** between too simple and too complex.  
  - Both $J_{\text{train}}$ and $J_{\text{cv}}$ can be relatively **low**.

If you plotted **training error** ($J_{\text{train}}$) and **cross-validation error** ($J_{\text{cv}}$) against the **degree of the polynomial** $d$:
- $J_{\text{train}}$ will typically **go down** as $d$ increases (complex model fits training data better).  
- $J_{\text{cv}}$ will **go down** at first (the model is getting more expressive), but after a certain point, it **goes back up** (overfitting).  
- The **ideal** $d$ is often somewhere in the middle, where $J_{\text{cv}}$ is minimized.

---

### 4. Can You Have Both High Bias and High Variance?

In simpler models like linear regression in 1D, you typically see **either** high bias **or** high variance, not both. However, in more complex models (e.g., large neural networks), it’s possible to be **underfitting** some parts of the data and **overfitting** other parts at the same time.  
- You might see:
  - $J_{\text{train}}$ is **high** (some parts underfit)  
  - $J_{\text{cv}} \gg J_{\text{train}}$ (also overfit in other regions).  

This case is less common in basic polynomial regression but more plausible in complicated or very large models.

---

### 5. Why This Matters for Improving Your Model

- **If your model has high bias** (underfits):  
  1. **Add more features** (more information might help it learn better).  
  2. **Increase model complexity** (e.g., try a higher polynomial degree or deeper neural network).  
  3. **Reduce regularization** (so the model can better fit the training data).

- **If your model has high variance** (overfits):  
  1. **Collect more training data** (the model has more examples to generalize from).  
  2. **Add regularization** (e.g., increase $\lambda$ so the model doesn’t rely too heavily on training points).  
  3. **Reduce model complexity** (try a lower polynomial degree or simpler neural network).

By checking **$J_{\text{train}}$** and **$J_{\text{cv}}$**, you can **diagnose** which direction to go.

---

### 6. Key Takeaways

1. **High Bias (Underfitting)**:  
   - $J_{\text{train}}$ ≈ **High**  
   - $J_{\text{cv}}$ ≈ **High** (close to $J_{\text{train}}$)  
   - **Action**: Increase complexity, gather more relevant features, reduce regularization.

2. **High Variance (Overfitting)**:  
   - $J_{\text{train}}$ ≈ **Low**  
   - $J_{\text{cv}} \gg J_{\text{train}}$ (much bigger)  
   - **Action**: More data, more regularization, reduce model complexity.

3. **Neither**:  
   - $J_{\text{train}}$ ≈ **Low**  
   - $J_{\text{cv}}$ ≈ **Low** (similar to $J_{\text{train}}$)  
   - **Action**: Your model might be **“just right”**.

4. **Both**: Rare in simple problems, possible in very complex models.  
   - $J_{\text{train}}$ is **high**  
   - $J_{\text{cv}} \gg J_{\text{train}}$  
   - **Action**: Might need both more capacity **and** more data/regularization, depending on the part of the data you’re not fitting well.

By understanding bias and variance, you gain a **compass** for how to improve your model. In the next sections, we’ll see how **regularization** can shift the balance between these two types of error.

---

## Regularization and Bias/Variance

So far, we’ve seen how the **degree of a polynomial** (model complexity) can impact bias and variance. Another powerful way to control bias and variance is **regularization**, often implemented using a **regularization parameter** $\lambda$.

---

### 1. What Is Regularization?

When you train a model—like a **polynomial regression**—you often minimize a cost function:

$$
J(\mathbf{\vec w}, b) \;=\; \frac{1}{2m}\sum_{i=1}^{m} \bigl(f_{\mathbf{\vec w},b}(x^{(i)}) \;-\; y^{(i)}\bigr)^2
\;+\;
\frac{\lambda}{2m}\sum_{j=1}^{n} w_j^2.
$$

- The **first term** (mean squared error) measures how well your model fits the data.  
- The **second term** (regularization term) **penalizes** large weights $\mathbf{w}$.  
- $\lambda$ is the regularization parameter that **balances** these two objectives.

A **bigger** $\lambda$ puts more emphasis on keeping weights small (less complex curves). A **smaller** $\lambda$ allows the model to fit training data more flexibly (potentially more complex curves).

---

### 2. Extreme Cases of $\lambda$

1. **$\lambda$ is very large** (e.g., $\lambda = 10,000$):  
   - The algorithm **strongly** penalizes large weights.  
   - Most $\mathbf{w}$ become **very close to 0**, leaving the model almost like $f_{\mathbf{\vec w},b}(x) \approx b$, a **constant** function.  
   - This typically leads to **high bias (underfitting)** because the model is too simple to capture important data trends.  
   - You’ll notice:
     - $J_{\text{train}}$ is **large** (it doesn’t even fit the training data well).  
     - $J_{\text{cv}}$ also tends to be **large**.

2. **$\lambda$ is very small** (e.g., $\lambda = 0$):  
   - **No** regularization term is added, so the model can freely adjust weights to fit the training data.  
   - The model can become overly complex—like the **wiggly** polynomial that **overfits**.  
   - You’ll see:
     - $J_{\text{train}}$ is **very small** (fits training data almost perfectly).  
     - $J_{\text{cv}}$ is **much larger** than $J_{\text{train}}$ (poor generalization).

3. **Intermediate $\lambda$**:  
   - A moderate penalty on large weights.  
   - Often finds a **balance** between underfitting and overfitting.  
   - Both $J_{\text{train}}$ and $J_{\text{cv}}$ can be **relatively low**.

---

### 3. Using Cross-Validation to Choose $\lambda$

Just like you **chose** the best polynomial degree $d$ by trying different values and checking the **cross-validation (CV)** error, you can do the **same** for $\lambda$:

1. Pick a **range** of $\lambda$ values (e.g., $0, 0.01, 0.02, 0.04, 0.08, \dots, 10$).  
2. For each $\lambda$, **train** your model → get $(\mathbf{w}^{\langle \lambda \rangle}, b^{\langle \lambda \rangle})$.  
3. Compute the **cross-validation error** $J_{\text{cv}}\bigl(\mathbf{w}^{\langle \lambda \rangle}, b^{\langle \lambda \rangle}\bigr)$.  
4. **Choose** the $\lambda$ that gives you the **lowest** CV error.  
5. **Finally**, confirm by looking at the **test set** to estimate generalization error.

This method ensures you **don’t** overfit the test set by using it repeatedly to pick $\lambda$. Instead, the test set is only used **once** at the end, to validate your final choice.

---

### 4. Bias and Variance as $\lambda$ Changes

If you **plot** how $J_{\text{train}}$ and $J_{\text{cv}}$ change as $\lambda$ increases:

- **When $\lambda$ is small** (near 0), the model is free to become very flexible → **overfits** →  
  - $J_{\text{train}}$ **low**  
  - $J_{\text{cv}}$ **high** (high variance)
- **When $\lambda$ is large**, the model becomes almost too simple → **underfits** →  
  - $J_{\text{train}}$ **high**  
  - $J_{\text{cv}}$ **high** (high bias)
- **Somewhere in the middle**, there’s a **sweet spot** where both $J_{\text{train}}$ and $J_{\text{cv}}$ are **relatively low**.

Visually, you might see a **U-shaped curve** for $J_{\text{cv}}$ across increasing $\lambda$. The **minimum** of that curve is your **best** $\lambda$.

---

### 5. Comparing to Polynomial Degree

This procedure mirrors what we saw with **polynomial degree** $d$:
- On one axis (polynomial degree), going from low $d$ (high bias) to high $d$ (high variance).  
- On the other axis (regularization $\lambda$), going from high $\lambda$ (high bias) to low $\lambda$ (high variance).  
- In **both** cases, there’s a middle ground that balances bias and variance.

---

### 6. Key Takeaways

1. **Regularization** adds a penalty for large weights, helping to **reduce variance** (overfitting).  
2. **Large $\lambda$** → strong regularization → simpler model → higher bias risk.  
3. **Small $\lambda$** → weak regularization → model can overfit → higher variance risk.  
4. Use **Cross-Validation** to systematically find a **good** $\lambda$.  
5. After choosing $\lambda$, check the **test set** for a fair measure of real-world performance.

By tuning $\lambda$, you can often **shift** your model between **high bias** and **high variance** regimes, aiming for the sweet spot where you achieve **low** training **and** cross-validation errors. This gives you another powerful tool (alongside model complexity, features, etc.) for improving your machine learning systems.