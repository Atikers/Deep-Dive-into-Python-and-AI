# ***Making Recommendations***

## ***Collaborative Filtering***

### ***Have You Ever Wondered How Streaming Sites Know Which Movie You’ll Love Next?***

Imagine you have a huge movie library and your friends are constantly rating the movies they watch with stars (from 0 to 5). Now think of a streaming service that wants to guess which new movies each friend might enjoy. How does it figure that out without just randomly suggesting titles?

In this section, we’ll explore an approach often called **recommendation systems** or **collaborative filtering**. Even if you’re new to these ideas, don’t worry! We’ll walk through everything step by step!

---

### ***1. The Basics of Our Setup***

Let's say we have:
- **4 users**
- **5 movies**

These users have rated some of the movies from **0 to 5 stars**. We’ll mark a movie with a “?” if a user hasn’t watched or hasn’t provided a rating. For instance, if User 1 hasn’t watched Movie 3, we’ll mark it as “?”.

A table might look like this:

| Movie    | User 1 | User 2 | User 3 | User 4 |
|----------|--------|--------|--------|--------|
| Movie 1  | 5      | 5      | 0      | 0      |
| Movie 2  | 5      | ?      | ?      | 0      |
| Movie 3  | ?      | 4      | 0      | ?      |
| Movie 4  | 0      | 0      | 5      | 4      |
| Movie 5  | 0      | 0      | 5      | ?      |

- $n_u$ = number of users (4 here).
- $n_m$ = number of movies (5 here).

We also define:
- $r(i,j) = 1$ if user $j$ has rated movie $i$, and $r(i,j) = 0$ otherwise.
- $y(i,j)$ = the actual rating (0 to 5 stars) **only** if user $j$ has rated movie $i$.

For example:
- $r(1,1) = 1$ because User 1 rated Movie 1.
- $r(3,1) = 0$ because User 1 did **not** rate Movie 3.
- $y(3,2) = 4$ means User 2 gave Movie 3 a rating of **4 stars**.

---

### ***2. Why This Is So Valuable***

This setup is the foundation of many recommendation engines. Think about the times you’ve used:
- A streaming service that suggests shows (like Netflix or Max).
- An online store that suggests products (like Amazon).
- A food delivery app that recommends restaurants or dishes.
  
**In all of these cases**, the platform tries to figure out what you might enjoy next based on the ratings (or clicks, watch times, etc.) that **you and other users** have provided.

---

### ***3. Making Predictions for Unrated Movies***

The challenge is: **How do we guess what rating a user would give to a movie they haven’t seen yet?** In the table above, we have question marks (“?”) where a rating is missing. If we can guess (or predict) those values, we can:
1. Sort the predicted ratings from highest to lowest.
2. Show the user the top-rated items first (because we think they’ll like them the most).

In more advanced settings, we might also have “features” that describe each movie (like its genre—romance, action, comedy, etc.). We’ll see that these can help us make better guesses. But **even if** we don’t have those movie features, there are still ways to learn from just the ratings themselves (we’ll explore that soon).

---

### ***4. An Analogy: Classmates and Science Projects***

To make this easier:
- Think of each **movie** as a **science project** in your class.
- Each **user** is a **classmate** who reviews these science projects with “stars.”
- Some classmates review (rate) multiple projects, others might not have the time to review them all (so some ratings are missing).
  
If you want to **recommend** which projects a new student should check out, you’d look at everyone’s past ratings and guess which remaining projects might be top-rated by certain classmates—even though those classmates haven't officially rated them yet.

---

### ***5. Where We’re Headed Next***

We’ll soon talk about:
1. **How to mathematically predict** the missing “?” ratings using the information we have.
2. **What to do if we do (or do not) have extra details** about each movie (like genre).
3. **How large companies** integrate this approach into their websites or apps to keep users interested and drive sales (or views).

**Key idea**: By focusing on user preferences (the ratings they provide), we can make surprisingly accurate guesses about what they’d love to see, buy, or do next—even if they haven’t explicitly told us.

---

### ***6. More Real-World Examples***

Beyond movies, you can use the same approach for:
- **Music suggestions**: Predict a listener’s rating for songs or playlists they haven’t heard yet.
- **Restaurant discovery**: Estimate which restaurants you might rate highly based on others with similar tastes.
- **Clothing or product recommendations**: Help a store guess which clothes or gadgets you’d love, based on your previous purchases and ratings.

The possibilities are **enormous** and form a huge part of how modern apps keep us engaged!

---

### ***7. Recap***

- We have **users** and **items** (movies), each with a **user–item rating table**.
- Many ratings are **missing** (users haven’t rated every item).
- We define $r(i,j)$ and $y(i,j)$ to keep track of which movies are actually rated and their star values.
- **Goal**: Predict the missing ratings to make recommendations.

---

## ***Using Per-Item Features***

Imagine you have a bunch of movies, each with a “personality”—like how **romantic** or how **action-packed** it is. Now picture a handful of people, each with their own tastes in movies. If we know a user’s likes and dislikes so far, **can we guess** what star rating they’ll give to a brand-new movie just by looking at those personality (feature) scores?

In this section, we’ll explore:
1. **How** we represent these “personalities” (features) for each movie.
2. **How** we build a simple linear model for each user to predict ratings.
3. **How** we define the **cost function** (the math that helps us learn good parameters for our model).
4. **Why** regularization helps avoid wacky or overfitted predictions.

Let’s dive in step by step.

---

### ***1. Setting Up Movie Features***

Say we have:
- $n_u$ = number of users (e.g., Alice, Bob, Carol, Dave).
- $n_m$ = number of movies (e.g., 5 different titles).
- $n$ = number of features describing each movie.  

#### Example: Romance and Action
- $x_1(i)$: “romance level” of movie $i$.
- $x_2(i)$: “action level” of movie $i$.

So each movie $i$ is described by a **feature vector** $x^{(i)} = \begin{bmatrix} x_1(i) \\ x_2(i) \end{bmatrix}$

> **Analogy**: Think of each movie like a **soup**. The “romance” level is how sweet and gentle the soup is, while the “action” level is how spicy or punchy it tastes. A high romance score means a very sweet flavor; a high action score means a spicy kick!

---

### ***2. A Different “Mini Linear Model” for Each User***

We want to predict user $j$’s rating for movie $i$. We do that by:  

$$
\text{Predicted rating} = w^{(j)} \cdot x^{(i)} + b^{(j)}
$$

where:
- $w^{(j)}$ is **user $j$’s** parameter vector (same size as $x^{(i)}$).
- $b^{(j)}$ is a **bias** term for user $j$ (like an offset in their rating style).

Each user gets their own $w^{(j)}$ and $b^{(j)}$ because not everyone likes the same mix of romance and action. For example:
- Alice might love romance and hate action, so $w^{(1)}$ might be heavily weighted on the romance feature.
- Bob might like a bit of both, so $w^{(2)}$ might be more balanced.

> **Analogy**: Think of $w^{(j)}$ and $b^{(j)}$ like **taste buds** for user $j$. They interpret the soup’s “ingredients” (the movie’s romance/action levels) differently for each person.

---

### ***3. Cost Function for a Single User***

How do we **learn** $w^{(j)}$ and $b^{(j)}$? We need a **cost function** that says how close our predictions are to the **actual** star ratings, $y^{(i,j)}$.

1. **Focus on movies the user actually rated**: If $r(i,j) = 1$, it means user $j$ **did** rate movie $i$.  
2. **Sum of squared errors**: We compare $\bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)^2$ for each rated movie.  
3. **Regularization**: We add a term to discourage $w^{(j)}$ from growing **too large** (overfitting is problem always). 

Putting it all together, for user $j$:  

$$
J\bigl(w^{(j)}, b^{(j)}\bigr) = \frac{1}{2}\sum_{i : r(i,j)=1} \Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2 + \frac{\lambda}{2} \sum_{k=1}^{n} \Bigl(w_{k}^{(j)}\Bigr)^2.
$$

- The summation $\sum_{i : r(i,j)=1}$ means we **only** sum over those movies $i$ that user $j$ actually rated.  
- $\lambda$ is the **regularization parameter**: a small positive number that helps control the size of $w^{(j)}$.
- original cost function $J\bigl(w^{(j)}, b^{(j)}\bigr)$ is  

$$
\frac{1}{2m^{(j)}}\sum_{i : r(i,j)=1} \Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2 + \frac{\lambda}{2m^{(j)}} \sum_{k=1}^{n} \Bigl(w_{k}^{(j)}\Bigr)^2
$$

but we remove the $m^{(j)}$ from the denominator because:
  - Users have different numbers of rated movies (some might rate 100 movies, others only 10)
  - Not dividing by $m^{(j)}$ allows users with more ratings to have a stronger influence on the cost function, which is desirable since their preferences are more reliable
  - It simplifies the optimization process while maintaining the effectiveness of the model

---

### ***4. Cost Function Across All Users***

We don’t just have one user; we have $n_u$ of them. We do the same cost calculation for each user $j$, then **sum** them all up:  

$$
J\bigl(w^{(1)}, b^{(1)}, \dots, w^{(n_u)}, b^{(n_u)}\bigr)
=\sum_{j=1}^{n_u} \left[\frac{1}{2}\sum_{i : r(i,j)=1} \Bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\Bigr)^2 + \frac{\lambda}{2}\sum_{k=1}^{n} \Bigl(w_{k}^{(j)}\Bigr)^2 \right]
$$

> **Analogy**: Imagine each user has their own personal mini “taste test” to see how well our soup predictions line up with their actual ratings. Then we combine everyone’s taste tests for one giant scoreboard.

---

### ***5. Training: Finding the Best Parameters***

To **train** the system, we:
1. Start with some guess for each $w^{(j)}$ and $b^{(j)}$.
2. Use **gradient descent** (or a similar optimization method) to adjust them **all** and **minimize** the total cost $J$.

#### ***(Recap) Training with Gradient Descent***
- The weight vector $w^{(j)}$
- The bias term $b^{(j)}$

##### ***computing the gradients***
- For each user j, we compute the partial derivatives of the cost function J:
1. **Partial derivative with respect to** $w_k^{(j)}$:  
  
$$
\frac{\partial}{\partial w_k^{(j)}} J = \sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)x_k^{(i)} + \lambda w_k^{(j)}
$$

2. **Partial derivative with respect to** $b^{(j)}$:  
  
$$
\frac{\partial}{\partial b^{(j)}} J = \sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)
$$

##### ***update rules***

Using learning rate $\alpha$, we update our parameters:

1. **Weight update rule**:  

$$
w_k^{(j)} := w_k^{(j)} - \alpha \left[\sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)x_k^{(i)} + \lambda w_k^{(j)}\right]
$$

2. **Bias update rule**:  

$$
b^{(j)} := b^{(j)} - \alpha \sum_{i:r(i,j)=1} \bigl(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)}\bigr)
$$

##### ***Implementation Example***

Here's how we might implement this in code:

```python
def gradient_descent(X, y, r, learning_rate=0.01, lambda_reg=0.1, num_iterations=1000):
    num_users = r.shape[1]
    num_features = X.shape[1]
    
    # initialize parameters
    w = np.zeros((num_users, num_features))  # weight vector for each user
    b = np.zeros(num_users)                  # bias for each user
    
    for iteration in range(num_iterations):
        for j in range(num_users):  # for each user
            # indices of movies rated by the user
            rated_items = np.where(r[:, j] == 1)[0]
            
            if len(rated_items) > 0:
                # update weight
                for k in range(num_features):
                    grad_w = 0
                    for i in rated_items:
                        pred = np.dot(w[j], X[i]) + b[j]
                        grad_w += (pred - y[i,j]) * X[i,k]
                    grad_w += lambda_reg * w[j,k]  # regularization term
                    w[j,k] -= learning_rate * grad_w
                
                # update bias
                grad_b = 0
                for i in rated_items:
                    pred = np.dot(w[j], X[i]) + b[j]
                    grad_b += (pred - y[i,j])
                b[j] -= learning_rate * grad_b
    
    return w, b
```

##### ***Key Considerations***

When implementing gradient descent for recommendation systems, keep in mind:

1. **Batch vs. Mini-batch**
   - The implementation above uses batch gradient descent
   - For large datasets, mini-batch processing might be more efficient
   - Mini-batches can help escape local minima and converge faster

2. **Learning Rate Selection**
   - Too large: may cause divergence
   - Too small: slow convergence
   - Consider using adaptive learning rates or learning rate scheduling

3. **Regularization Strength ($\lambda$)**
   - Controls the balance between fitting the data and preventing overfitting
   - Can be selected using cross-validation
   - Typically starts with small values (0.01 to 0.1)

4. **Convergence Monitoring**
   - Track the cost function $J$ across iterations
   - Stop when changes become sufficiently small
   - Consider implementing early stopping based on validation set performance

---

### ***6. What If We Don’t Have (Good) Features?***

So far, we assumed we know exactly how “romantic” or “action-packed” each movie is. But what if we **don’t**? In many real-world scenarios, you only have user rating data, with no explicit features. The **great news** is there’s a more advanced approach (often called “collaborative filtering with learned features”) that can discover these hidden factors **automatically**—but that’s a story for another time!

> **Teaser**: You can learn these features **directly** from the rating patterns, letting the system figure out how to group movies (and users) in a way that best predicts the missing ratings.

---

### ***7. Key Takeaways***

- **Linear Regression Per User**: We treat each user’s ratings as its own mini linear regression problem.
- **Cost Function**: Sum of squared errors over all rated movies, plus a **regularization** term.
- **Global Cost**: Add up individual user costs to measure the overall performance.
- **Optimization**: We tune $w^{(j)}$ and $b^{(j)}$ for each user to minimize the total cost.
- **Features**: If we have them (like romance level, action level), we can make direct predictions. If we don’t, more advanced methods can learn them from data.

Remember: The magic is that the system learns how each user’s tastes align with a movie’s features—so it can **predict** new ratings, even if the user never rated that particular movie before.

---