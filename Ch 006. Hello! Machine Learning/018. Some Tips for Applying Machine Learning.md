# Some Tips for Applying Machine Learning

## Deciding What to Try Next

When you start building real-world machine learning projects, one of the biggest challenges is knowing **which step to take next**. You may have a powerful toolbox that includes Linear Regression, Logistic Regression, Decision Trees(You'll learn about this in the next chapter), and even Deep Neural Networks, but **choosing how to improve your model** can be tricky. Sometimes, a team might spend many months collecting data or tweaking parameters without seeing significant improvement—where a more systematic approach could yield better results in just a few weeks.

In this section, we’ll look at **how to make good decisions about next steps** in your machine learning projects. We'll also discuss **diagnostics**, which are tests you can run to pinpoint what's holding your model back and how best to fix it.

---

### 1. The Value of Good Decisions

Think of a machine learning project like **cooking a complex dish**: you have several possible ingredients (features and data sources) and several cooking methods (algorithms and hyperparameters). You can spend hours (or even days) in the kitchen, only to end up with a meal that tastes just “okay.” But if you know exactly how each ingredient works—when to add more salt, when to cook on high heat, when to reduce the sauce—you can get **great results faster**.

In a similar way, **making good decisions** in a machine learning project allows you to quickly zero in on what's really needed:

- **Do you need more data?**  
  Sometimes more data helps your model learn better patterns. Other times, it barely moves the performance needle.

- **Do you need more (or fewer) features?**  
  Maybe you’re missing important information in your dataset. Or perhaps you have too many irrelevant features causing “noise” instead of clarity.

- **Do you need different transformations of existing features?**  
  For example, adding polynomial features $x_1^2, x_2^2, x_1 \times x_2$ might capture complex relationships that a plain linear model can’t.

- **Do you need to adjust regularization?**  
  If $\lambda$ (the regularization parameter) is too large, the model might be too simple (underfitting). If $\lambda$ is too small, it might overfit.

Investing your energy wisely in **only** the changes that significantly boost performance is what differentiates a quick, successful project from a slow, frustrating one.

---

### 2. The Role of Diagnostics

A **diagnostic** is like a **medical test** for your model: it checks the “health” of your model and pinpoints areas that need attention. Instead of guessing which medicine (improvement) might work, a diagnostic test offers **evidence** that can guide you more reliably.

For instance:

- **Learning Curve Diagnostics**: Tells you whether more data is likely to help.  
- **Bias/Variance Tests**: Indicates if your model is underfitting (high bias) or overfitting (high variance).  
- **Error Analysis**: Shows exactly which examples your model is struggling with, so you can design better features or gather more targeted data.

Sometimes, running these diagnostics costs time upfront—just like going to a doctor for tests before deciding on a treatment. But this approach can **save you months** of wasted effort because you get a clearer idea of what’s wrong and what might actually fix it.

---

### 3. Example: Improving House Price Predictions

Imagine you have a **regularized linear regression** model to predict house prices. Your cost function might look like this:

$$
J(\vec W, b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{\vec W, b}(\vec x^{(i)}) - y^{(i)})^2 
\;+\; \frac{\lambda}{2m} \sum_{j=1}^{n} \ {W_j}^2
$$

You’ve trained your model, but the predictions are **not accurate enough**. What can you do?

1. **Collect More Training Examples**  
   - **Pros**: More data can reduce overfitting and help your model generalize better.  
   - **Cons**: Gathering data can be expensive and time-consuming. A diagnostic test (like a learning curve) can help you see if this is really worth doing.

2. **Add or Remove Features**  
   - **Pros**: Adding relevant features (like proximity to schools, average neighborhood income, or recent renovations) might give your model more insight.  
   - **Cons**: Irrelevant or redundant features can add noise. Sometimes, fewer well-chosen features can help the model focus.  
   - **Diagnostic Tip**: Error analysis on a small set of predictions might reveal that your model struggles specifically with older homes, suggesting you track building age more precisely.

3. **Feature Engineering (e.g., Polynomial Features)**  
   - **Pros**: Captures non-linear relationships that simple linear models miss.  
   - **Cons**: Might lead to overfitting if you overdo it with too many polynomial terms.  
   - **Diagnostic Tip**: Check train vs. test performance to see if your new features truly help.

4. **Tweak Regularization Parameter ($\lambda$)**  
   - If $\lambda$ is too high, you may be underfitting (predictions too simplistic).  
   - If $\lambda$ is too low, you may be overfitting (model too tailored to the training data).  
   - **Diagnostic Tip**: Plot different values of $\lambda$ against your cost function or error metrics (like RMSE) to see the “sweet spot.”

---

### 4. Why Systematic Diagnostics Matter

Picture a **treasure hunt**: you can try digging holes randomly across the field, hoping to find the treasure by chance. Or you can use a map (diagnostics), which shows likely spots to dig based on clues. The second approach is obviously faster and more effective.

**Diagnostics are your map.** They give you clues about:

- **Whether a certain action (like collecting data) is even worth it**  
- **Which parts of your model or dataset need the most attention**  
- **How to adjust hyperparameters and features systematically**

---

### 5. Looking Ahead

In the following sections, we’ll explore **specific diagnostics** you can use for your projects. These tools will help you decide:

- Should I **collect more data**?  
- Do I need **new features** or better **feature transformations**?  
- Is my model suffering from **bias** (underfitting) or **variance** (overfitting)?  
- How do I **tune hyperparameters** like learning rate or regularization?

By applying these diagnostics, you can **prioritize the right tasks** and avoid the time sink of trial-and-error. Whether you’re building a simple linear predictor or a state-of-the-art neural network, the **ability to pinpoint next steps** with confidence will dramatically speed up your progress.

---

### Key Takeaways

1. **Identify bottlenecks** in your model’s performance before making changes.  
2. **Use diagnostics** to guide your improvements—don’t guess blindly.  
3. **Focus on changes** that actually address your model’s weaknesses, rather than doing everything at once.  
4. **Iterate quickly**, checking progress with each new tweak or data addition.

With these principles in mind, you’re ready to explore the **practical steps** for evaluating and improving your machine learning models—so let’s keep going!

----

## Evaluating a Model

So, you've trained a machine learning model—how do you **evaluate** that model’s performance? Having a systematic way to evaluate performance not only tells you how good your model is right now, but also clarifies how to **improve** it.

Let's walk through an example using **housing price prediction**. You might start with a polynomial function of the **size** of the house $x$. Perhaps you fit a **fourth-order polynomial** (i.e., features $x, x^2, x^3, x^4$) to just **five data points**. This could give you a model that **perfectly** fits those five points—meaning it does great on the **training data**. But in reality, it might be **too wiggly** and won’t generalize well to new houses.

### 1. Why Overfitting Is a Problem
When you see a model that **matches the training data** suspiciously well—like a perfectly curved line that hits every point—this can be a **red flag for overfitting**. Overfitting means your model isn’t capturing the true underlying pattern; instead, it’s just memorizing your training data.

In a **single-feature** scenario (e.g., just house size), you can **plot** the function and visually see if it looks “too wiggly.” But in reality, most problems use **multiple features**—the size of the house ($x_1$), number of bedrooms ($x_2$), number of floors ($x_3$), and age of the home ($x_4$), for example. Plotting a **four-dimensional** function is nearly impossible to visualize. 

Hence, you need a **systematic, numerical** way to tell if your model is doing well.

---

### 2. Train/Test Split

One common technique is to **split** your dataset into:
- A **Training Set** (e.g., 70% or 80% of data)
- A **Test Set** (the remaining 30% or 20%)

You **train** your model (find the parameters $\mathbf{w}, b$) **only** on the training set. Then you **evaluate** its performance on the test set, which the model hasn’t seen before.

Suppose we have a small dataset of 10 house-price examples. We might use:
- 7 examples ($m_{\text{train}} = 7$) for training
- 3 examples ($m_{\text{test}} = 3$) for testing

Concretely, you’ll have:

- Training Examples:  
  $$
    (x_{\text{train}}^{(1)}, y_{\text{train}}^{(1)}), \dots, (x_{\text{train}}^{(m_{\text{train}})}, y_{\text{train}}^{(m_{\text{train}})})
  $$
- Test Examples:  
  $$
    (x_{\text{test}}^{(1)}, y_{\text{test}}^{(1)}), \dots, (x_{\text{test}}^{(m_{\text{test}})}, y_{\text{test}}^{(m_{\text{test}})})
  $$

Where:
- $m_{\text{train}}$ = number of training examples
- $m_{\text{test}}$ = number of test examples

---

### 3. Linear Regression Example (Squared Error Cost)

If your task is **regression**, you might fit parameters $\mathbf{w}, b$ to **minimize** the following cost function:
$$
J(\mathbf{\vec w}, b) 
= \frac{1}{2m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} \bigl(f_{\mathbf{\vec w},b}(x_{\text{train}}^{(i)}) - y_{\text{train}}^{(i)}\bigr)^2
\;+\;
\frac{\lambda}{2m_{\text{train}}}\sum_{j=1}^{n} \vec w_j^2,
$$
where
- $\lambda$ is the **regularization** parameter,  
- $f_{\mathbf{\vec w},b}(x)$ is your model’s prediction (e.g., a polynomial function of $x$),  
- The first sum is the **mean squared error** term.

After training, you evaluate on the **test set**:
$$
J_{\text{test}}(\mathbf{w}, b)
= \frac{1}{2m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} \Bigl(f_{\mathbf{w},b}\bigl(x_{\text{test}}^{(i)}\bigr) - y_{\text{test}}^{(i)}\Bigr)^2.
$$
**Note:** The test error formula **does not include** the regularization term. We’re just checking how well the model predicts new data.

You can also compute the **training error** (without the regularization term) to see how well the model does on its own training data:
$$
J_{\text{train}}(\mathbf{\vec w}, b)
= \frac{1}{2m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} \Bigl(f_{\mathbf{\vec w},b}\bigl(x_{\text{train}}^{(i)}\bigr) - y_{\text{train}}^{(i)}\Bigr)^2.
$$

- If $J_{\text{train}}$ is **very low** (close to 0) but $J_{\text{test}}$ is **high**, this suggests **overfitting**.
- If both $J_{\text{train}}$ and $J_{\text{test}}$ are **high**, your model might be **underfitting**.

---

### 4. Classification Example (0/1 Classification)

If your task is **classification**, such as identifying handwritten digits (0 vs. 1):
1. You **minimize** a cost function like **logistic loss** (cross-entropy):

$$
   J(\mathbf{\vec w}, b) 
   = - \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} 
     \Bigl[y_{\text{train}}^{(i)} \log\bigl(f_{\mathbf{\vec w},b}(x_{\text{train}}^{(i)})\bigr) 
     + (1 - y_{\text{train}}^{(i)}) \log\bigl(1 - f_{\mathbf{\vec w},b}(x_{\text{train}}^{(i)})\bigr)\Bigr] 
   \;+\;
   \frac{\lambda}{2m_{\text{train}}}\sum_{j=1}^{n} \vec w_j^2.
$$

2. You **evaluate** on the test set with either:
   - **Logistic loss** again (like above, but computed on test examples), or  
   - The **fraction of misclassified** examples (0/1 loss), which is often more intuitive:
     $$
     J_{\text{test}} = \frac{\text{Number of misclassified test examples}}{m_{\text{test}}}.
     $$
     Similarly, $J_{\text{train}}$ can be the fraction misclassified in the training set.

Misclassifications happen when your predicted label $\hat{y}$ (e.g., 0 or 1) differs from the true label $y$. This fraction-of-misclassifications metric is sometimes easier to interpret because it directly tells you the **percentage of mistakes** your model makes.

---

### 5. Why This Matters

This **train/test** split approach is crucial for **any** machine learning algorithm:
- It shows you if you’re **overfitting** or **underfitting**.
- It gives you an **unbiased** estimate of performance on data **not** seen during training.
- By comparing $J_{\text{train}}$ and $J_{\text{test}}$, you can **pinpoint** whether to simplify your model (avoid overfitting) or add complexity (combat underfitting).

Sometimes, you’ll see **80/20 splits**, **70/30 splits**, or even **90/10 splits**. The key idea is to **keep your test set separate** so you get a **realistic** check of how your model performs on data it hasn’t seen before.

---

### 6. Looking Ahead

Once you have a way to measure how good your model is (e.g., $J_{\text{test}}$), you can do things like:
- Compare **different polynomial degrees** (1st order, 2nd order, 3rd order, etc.) for regression.
- Try **alternative models** (like Logistic Regression vs. Neural Networks) and pick the best one.
- **Tune hyperparameters** (like regularization strength $\lambda$) or choose how many features to use.

In the next steps, we’ll explore **automatic model selection** using **validation sets** or **cross-validation**, which refine the train/test methodology so your algorithm can **choose** the best complexity or architecture **by itself**.

---

### Key Takeaways

1. **Splitting** your data into **train** and **test** sets helps prevent **overfitting** and gives you a realistic performance estimate.  
2. **Compute separate errors**:
   - $J_{\text{train}}$: performance on training data  
   - $J_{\text{test}}$: performance on unseen test data  
3. **Overfitting signs**: low $J_{\text{train}}$, high $J_{\text{test}}$.  
4. **Underfitting signs**: high $J_{\text{train}}$, high $J_{\text{test}}$.  
5. For **classification**, it’s often helpful to measure **accuracy** (or error) as the fraction of misclassified examples.  

With this procedure, you can now **evaluate** your model systematically and decide **how to improve** it for better predictive performance!

---