# Some Tips for Applying Machine Learning

![alt text](images/image-18.png)

## ***Deciding What to Try Next***

When you start building real-world machine learning projects, one of the biggest challenges is knowing **which step to take next**. You may have a powerful toolbox that includes Linear Regression, Logistic Regression, Decision Trees(You'll learn about this in the next chapter), and even Deep Neural Networks, but **choosing how to improve your model** can be tricky. Sometimes, a team might spend many months collecting data or tweaking parameters without seeing significant improvement—where a more systematic approach could yield better results in just a few weeks.

In this section, we’ll look at **how to make good decisions about next steps** in your machine learning projects. We'll also discuss **diagnostics**, which are tests you can run to pinpoint what's holding your model back and how best to fix it.

---

### ***1. The Value of Good Decisions***

Think of a machine learning project like **cooking a complex dish**: you have several possible ingredients (features and data sources) and several cooking methods (algorithms and hyperparameters). You can spend hours (or even days) in the kitchen, only to end up with a meal that tastes just “okay.” But if you know exactly how each ingredient works—when to add more salt, when to cook on high heat, when to reduce the sauce—you can get **great results faster**.

In a similar way, **making good decisions** in a machine learning project allows you to quickly zero in on what's really needed:

- **Do you need more data?**  
  Sometimes more data helps your model learn better patterns. Other times, it barely moves the performance needle.

- **Do you need more (or fewer) features?**  
  Maybe you’re missing important information in your dataset. Or perhaps you have too many irrelevant features causing “noise” instead of clarity.

- **Do you need different transformations of existing features?**  
  For example, adding polynomial features $x_1^2, x_2^2, x_1 \times x_2$ might capture complex relationships that a plain linear model can’t.

- **Do you need to adjust regularization?**  
  If $\lambda$ (the regularization parameter) is too large, the model might be too simple (underfitting). If $\lambda$ is too small, it might overfit.

Investing your energy wisely in **only** the changes that significantly boost performance is what differentiates a quick, successful project from a slow, frustrating one.

---

### ***2. The Role of Diagnostics***

A **diagnostic** is like a **medical test** for your model: it checks the “health” of your model and pinpoints areas that need attention. Instead of guessing which medicine (improvement) might work, a diagnostic test offers **evidence** that can guide you more reliably.

For instance:

- **Learning Curve Diagnostics**: Tells you whether more data is likely to help.  
- **Bias/Variance Tests**: Indicates if your model is underfitting (high bias) or overfitting (high variance).  
- **Error Analysis**: Shows exactly which examples your model is struggling with, so you can design better features or gather more targeted data.

Sometimes, running these diagnostics costs time upfront—just like going to a doctor for tests before deciding on a treatment. But this approach can **save you months** of wasted effort because you get a clearer idea of what’s wrong and what might actually fix it.

---

### ***3. Example: Improving House Price Predictions***

Imagine you have a **regularized linear regression** model to predict house prices. Your cost function might look like this:

$$
J(\vec W, b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{\vec W, b}(\vec x^{(i)}) - y^{(i)})^2 
+\frac{\lambda}{2m} \sum_{j=1}^{n} \ {W_j}^2
$$

You’ve trained your model, but the predictions are **not accurate enough**. What can you do?

1. **Collect More Training Examples**  
   - **Pros**: More data can reduce overfitting and help your model generalize better.  
   - **Cons**: Gathering data can be expensive and time-consuming. A diagnostic test (like a learning curve) can help you see if this is really worth doing.

2. **Add or Remove Features**  
   - **Pros**: Adding relevant features (like proximity to schools, average neighborhood income, or recent renovations) might give your model more insight.  
   - **Cons**: Irrelevant or redundant features can add noise. Sometimes, fewer well-chosen features can help the model focus.  
   - **Diagnostic Tip**: Error analysis on a small set of predictions might reveal that your model struggles specifically with older homes, suggesting you track building age more precisely.

3. **Feature Engineering (e.g., Polynomial Features)**  
   - **Pros**: Captures non-linear relationships that simple linear models miss.  
   - **Cons**: Might lead to overfitting if you overdo it with too many polynomial terms.  
   - **Diagnostic Tip**: Check train vs. test performance to see if your new features truly help.

4. **Tweak Regularization Parameter ($\lambda$)**  
   - If $\lambda$ is too high, you may be underfitting (predictions too simplistic).  
   - If $\lambda$ is too low, you may be overfitting (model too tailored to the training data).  
   - **Diagnostic Tip**: Plot different values of $\lambda$ against your cost function or error metrics (like RMSE) to see the “sweet spot.”

---

### ***4. Why Systematic Diagnostics Matter***

Picture a **treasure hunt**: you can try digging holes randomly across the field, hoping to find the treasure by chance. Or you can use a map (diagnostics), which shows likely spots to dig based on clues. The second approach is obviously faster and more effective.

**Diagnostics are your map.** They give you clues about:

- **Whether a certain action (like collecting data) is even worth it**  
- **Which parts of your model or dataset need the most attention**  
- **How to adjust hyperparameters and features systematically**

---

### ***5. Looking Ahead***

In the following sections, we’ll explore **specific diagnostics** you can use for your projects. These tools will help you decide:

- Should I **collect more data**?  
- Do I need **new features** or better **feature transformations**?  
- Is my model suffering from **bias** (underfitting) or **variance** (overfitting)?  
- How do I **tune hyperparameters** like learning rate or regularization?

By applying these diagnostics, you can **prioritize the right tasks** and avoid the time sink of trial-and-error. Whether you’re building a simple linear predictor or a state-of-the-art neural network, the **ability to pinpoint next steps** with confidence will dramatically speed up your progress.

---

### ***Key Takeaways***

1. **Identify bottlenecks** in your model’s performance before making changes.  
2. **Use diagnostics** to guide your improvements—don’t guess blindly.  
3. **Focus on changes** that actually address your model’s weaknesses, rather than doing everything at once.  
4. **Iterate quickly**, checking progress with each new tweak or data addition.

With these principles in mind, you’re ready to explore the **practical steps** for evaluating and improving your machine learning models—so let’s keep going!

----

## ***Evaluating a Model***

So, you've trained a machine learning model—how do you **evaluate** that model’s performance? Having a systematic way to evaluate performance not only tells you how good your model is right now, but also clarifies how to **improve** it.

Let's walk through an example using **housing price prediction**. You might start with a polynomial function of the **size** of the house $x$. Perhaps you fit a **fourth-order polynomial** (i.e., features $x, x^2, x^3, x^4$) to just **five data points**. This could give you a model that **perfectly** fits those five points—meaning it does great on the **training data**. But in reality, it might be **too wiggly** and won’t generalize well to new houses.

### ***1. Why Overfitting Is a Problem***
When you see a model that **matches the training data** suspiciously well—like a perfectly curved line that hits every point—this can be a **red flag for overfitting**. Overfitting means your model isn’t capturing the true underlying pattern; instead, it’s just memorizing your training data.

In a **single-feature** scenario (e.g., just house size), you can **plot** the function and visually see if it looks “too wiggly.” But in reality, most problems use **multiple features**—the size of the house ($x_1$), number of bedrooms ($x_2$), number of floors ($x_3$), and age of the home ($x_4$), for example. Plotting a **four-dimensional** function is nearly impossible to visualize. 

Hence, you need a **systematic, numerical** way to tell if your model is doing well.

---

### ***2. Train/Test Split***

One common technique is to **split** your dataset into:
- A **Training Set** (e.g., 70% or 80% of data)
- A **Test Set** (the remaining 30% or 20%)

You **train** your model (find the parameters $\mathbf{w}, b$) **only** on the training set. Then you **evaluate** its performance on the test set, which the model hasn’t seen before.

Suppose we have a small dataset of 10 house-price examples. We might use:
- 7 examples ($m_{\text{train}} = 7$) for training
- 3 examples ($m_{\text{test}} = 3$) for testing

Concretely, you’ll have:

- Training Examples:  
  $$
    (x_{\text{train}}^{(1)}, y_{\text{train}}^{(1)}), \dots, (x_{\text{train}}^{(m_{\text{train}})}, y_{\text{train}}^{(m_{\text{train}})})
  $$
- Test Examples:  
  $$
    (x_{\text{test}}^{(1)}, y_{\text{test}}^{(1)}), \dots, (x_{\text{test}}^{(m_{\text{test}})}, y_{\text{test}}^{(m_{\text{test}})})
  $$

Where:
- $m_{\text{train}}$ = number of training examples
- $m_{\text{test}}$ = number of test examples

---

### ***3. Linear Regression Example (Squared Error ***Cost)

If your task is **regression**, you might fit parameters $\mathbf{w}, b$ to **minimize** the following cost function:
$$
J(\mathbf{\vec w}, b) 
= \frac{1}{2m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} \bigl(f_{\mathbf{\vec w},b}(x_{\text{train}}^{(i)}) - y_{\text{train}}^{(i)}\bigr)^2
\;+\;
\frac{\lambda}{2m_{\text{train}}}\sum_{j=1}^{n} \vec w_j^2,
$$
where
- $\lambda$ is the **regularization** parameter,  
- $f_{\mathbf{\vec w},b}(x)$ is your model’s prediction (e.g., a polynomial function of $x$),  
- The first sum is the **mean squared error** term.

After training, you evaluate on the **test set**:
$$
J_{\text{test}}(\mathbf{w}, b)
= \frac{1}{2m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} \Bigl(f_{\mathbf{w},b}\bigl(x_{\text{test}}^{(i)}\bigr) - y_{\text{test}}^{(i)}\Bigr)^2.
$$
**Note:** The test error formula **does not include** the regularization term. We’re just checking how well the model predicts new data.

You can also compute the **training error** (without the regularization term) to see how well the model does on its own training data:
$$
J_{\text{train}}(\mathbf{\vec w}, b)
= \frac{1}{2m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} \Bigl(f_{\mathbf{\vec w},b}\bigl(x_{\text{train}}^{(i)}\bigr) - y_{\text{train}}^{(i)}\Bigr)^2.
$$

- If $J_{\text{train}}$ is **very low** (close to 0) but $J_{\text{test}}$ is **high**, this suggests **overfitting**.
- If both $J_{\text{train}}$ and $J_{\text{test}}$ are **high**, your model might be **underfitting**.

---

### ***4. Classification Example (0/1 Classification)***

If your task is **classification**, such as identifying handwritten digits (0 vs. 1):
1. You **minimize** a cost function like **logistic loss** (cross-entropy):

$$
   J(\mathbf{\vec w}, b) 
   = - \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} 
     \Bigl[y_{\text{train}}^{(i)} \log\bigl(f_{\mathbf{\vec w},b}(x_{\text{train}}^{(i)})\bigr) 
     + (1 - y_{\text{train}}^{(i)}) \log\bigl(1 - f_{\mathbf{\vec w},b}(x_{\text{train}}^{(i)})\bigr)\Bigr] 
   \;+\;
   \frac{\lambda}{2m_{\text{train}}}\sum_{j=1}^{n} \vec w_j^2.
$$

2. You **evaluate** on the test set with either:
   - **Logistic loss** again (like above, but computed on test examples), or  
   - The **fraction of misclassified** examples (0/1 loss), which is often more intuitive:
     $$
     J_{\text{test}} = \frac{\text{Number of misclassified test examples}}{m_{\text{test}}}.
     $$
     Similarly, $J_{\text{train}}$ can be the fraction misclassified in the training set.

Misclassifications happen when your predicted label $\hat{y}$ (e.g., 0 or 1) differs from the true label $y$. This fraction-of-misclassifications metric is sometimes easier to interpret because it directly tells you the **percentage of mistakes** your model makes.

---

### ***5. Why This Matters***

This **train/test** split approach is crucial for **any** machine learning algorithm:
- It shows you if you’re **overfitting** or **underfitting**.
- It gives you an **unbiased** estimate of performance on data **not** seen during training.
- By comparing $J_{\text{train}}$ and $J_{\text{test}}$, you can **pinpoint** whether to simplify your model (avoid overfitting) or add complexity (combat underfitting).

Sometimes, you’ll see **80/20 splits**, **70/30 splits**, or even **90/10 splits**. The key idea is to **keep your test set separate** so you get a **realistic** check of how your model performs on data it hasn’t seen before.

---

### ***6. Looking Ahead***

Once you have a way to measure how good your model is (e.g., $J_{\text{test}}$), you can do things like:
- Compare **different polynomial degrees** (1st order, 2nd order, 3rd order, etc.) for regression.
- Try **alternative models** (like Logistic Regression vs. Neural Networks) and pick the best one.
- **Tune hyperparameters** (like regularization strength $\lambda$) or choose how many features to use.

In the next steps, we’ll explore **automatic model selection** using **validation sets** or **cross-validation**, which refine the train/test methodology so your algorithm can **choose** the best complexity or architecture **by itself**.

---

### ***Key Takeaways***

1. **Splitting** your data into **train** and **test** sets helps prevent **overfitting** and gives you a realistic performance estimate.  
2. **Compute separate errors**:
   - $J_{\text{train}}$: performance on training data  
   - $J_{\text{test}}$: performance on unseen test data  
3. **Overfitting signs**: low $J_{\text{train}}$, high $J_{\text{test}}$.  
4. **Underfitting signs**: high $J_{\text{train}}$, high $J_{\text{test}}$.  
5. For **classification**, it’s often helpful to measure **accuracy** (or error) as the fraction of misclassified examples.  

With this procedure, you can now **evaluate** your model systematically and decide **how to improve** it for better predictive performance!

---

## ***Model Selection and Training/Validation/Test Sets***

In the previous sections, we saw how splitting data into **training** and **test** sets gives us a better understanding of how well a model might generalize. However, there’s still a **pitfall**: if we use the **test set** to **choose** among multiple models (for example, deciding which polynomial degree or which neural network architecture to use), we risk **overfitting** to the test set. That means our test set error could become an **overly optimistic** estimate of how well the model truly performs on brand-new data.

To tackle this issue, we introduce a **third** dataset: the **cross-validation** set (often called the **validation set** or **dev set**). This leads us to a **3-way split**:

1. **Training Set** – used to **fit** the parameters \(\mathbf{w}, b\).  
2. **Cross-Validation Set (CV / Dev Set)** – used to **select** among different models or hyperparameters (e.g., polynomial degree \(d\), regularization parameter \(\lambda\), number of hidden units in a neural network, etc.).  
3. **Test Set** – used **only** at the end, to **estimate** the model’s performance on completely new data.

---

### ***1. Why Do We Need a Cross-Validation Set?***

Imagine you’re **studying for a big test** in school:

- You might have a **textbook** with practice problems. (This is your **training set**.)  
- You might have a **sample quiz** that you use to check which areas you still need to study. (This is your **cross-validation set**—you use it to figure out how well you’re doing and what approach works best.)  
- Finally, you take the **actual exam**. (This is your **test set**—the final measure of your performance.)

If you keep using the **actual exam** (test set) to decide how you study, you’ll end up memorizing those exact questions! Your score on that exam might look great—but only because you’ve effectively **overfit** to the exam questions. In real life (or in real machine learning applications), that approach won’t generalize to new problems. 

---

### ***2. How to Split the Data***

A common approach is something like:
- **60%** of your data goes to the **Training Set**.
- **20%** goes to the **Cross-Validation Set** (CV).
- **20%** goes to the **Test Set**.

These percentages can vary depending on the total size of your dataset. Sometimes you’ll see **70/15/15** or **80/10/10** splits. The key idea is to:

1. **Train** on the training set.  
2. **Pick** the best model (degree of polynomial, neural network architecture, etc.) by looking at **cross-validation** performance.  
3. **Estimate** final generalization performance using the **test set**—which you haven’t used for any decisions.

---

### ***3. Model Selection Workflow***

Suppose you’re doing **polynomial regression** to predict house prices. You consider **10 different models**:  
- \(d = 1\) : \(f_{\mathbf{\vec w},b}(x) = w_1 x + b\) (linear)  
- \(d = 2\) : \(f_{\mathbf{\vec w},b}(x) = w_1 x + w_2 x^2 + b\)  
- \(\dots\)  
- \(d = 10\): \(f_{\mathbf{\vec w},b}(x) = w_1 x + \dots + w_{10} x^{10} + b\)

**Step-by-step**:

1. **Train each polynomial degree** \(d\) on the **training set**, resulting in parameters \(\mathbf{w}^{\langle d \rangle}, b^{\langle d \rangle}\).  
2. For each model, **evaluate** the **cross-validation** error:
   \[
   J_{\text{cv}}\bigl(\mathbf{w}^{\langle d \rangle}, b^{\langle d \rangle}\bigr).
   \]
   - For regression, you might use **mean squared error** on the CV set.
   - For classification, it might be **fraction of misclassified** CV examples.  
3. **Choose** the model (the polynomial degree \(d\)) that yields the **lowest** cross-validation error.  
4. **Finally**, take that chosen model (say, \(d=4\) or \(d=5\)) and compute its error on the **test set**:
   \[
   J_{\text{test}}\bigl(\mathbf{w}^{\langle d \rangle}, b^{\langle d \rangle}\bigr).
   \]
   This \(\, J_{\text{test}}\) is your **unbiased** estimate of how well your final model generalizes to brand-new data.

Because the test set **wasn’t** used to pick any parameters or hyperparameters, it offers a **fair** measure of performance.

---

### ***4. Other Examples of Model Selection***

- **Choosing a Neural Network Architecture**:  
  You might try a small network with fewer layers versus a deeper network with more layers and units. After training them on the **training set**, evaluate each architecture on the **CV set**. The architecture with the **lowest CV error** is chosen. You then test that final architecture on the **test set** to confirm performance.

- **Choosing Regularization Parameters**:  
  For example, you might try \(\lambda = 0\) (no regularization), \(\lambda = 0.1\), \(\lambda = 1\), etc., see which gives the best cross-validation score, then confirm on the test set.

In **every** case, the principle is the same: **Use only the training set to fit parameters; use the cross-validation set to pick among different model variations; and hold out the test set for final evaluation.**

---

### ***5. Why Not Use Only Train/Test Splits?***

If you use the **test set** directly to pick your best model, you’re effectively **tuning** your model to also perform well on the test set (even if unintentionally). This leads to an **optimistic** (and unrealistic) estimate of generalization performance. The **cross-validation** set prevents this by acting as a “quiz” or “dress rehearsal” for the final performance check.

---

### ***6. Key Takeaways***

1. **3-Way Split**: Training, Cross-Validation (Validation/Dev), Test.  
2. **Train** with the training set.  
3. **Compare/Select** models using the cross-validation set.  
4. **Final Evaluation** on the test set, which is not used in any decision-making steps.  
5. This approach helps ensure an **unbiased** estimate of your model’s ability to generalize.

In summary, **model selection** is the process of picking the best version of your learning algorithm—whether that’s the polynomial degree, neural network architecture, or any other design choice. By using **training**, **cross-validation**, and **test** sets appropriately, you can **automatically** find the best model **and** get a reliable estimate of its real-world performance.
