# ***Beyond Supervised Learning (2) - Anomaly Detection***

## ***Finding Unusual Events***

### ***Have you ever wondered how systems automatically catch “strange” or “rare” or "Fraudulent" behaviors?***

Meet **anomaly detection**, our second unsupervised learning algorithm. It learns what “normal” looks like from unlabeled data—so when something **unusual** shows up, it can raise a red flag. Whether it’s monitoring aircraft engines or spotting credit card fraud, anomaly detection is a **powerful tool** for safeguarding against unexpected or dangerous events.

---

### ***1. What Is Anomaly Detection?***

**Anomaly detection** looks at a dataset of **mostly “normal”** examples and tries to figure out **when a new example doesn’t fit** the usual pattern. It doesn’t need labels for “bad” or “good”; it just knows what **typical** behavior looks like.

- **Analogy**: Imagine you watch a flock of birds daily. You learn to recognize how they usually move and sound. If a bird suddenly **flies very differently** or makes an **odd** sound, your instinct says something’s off—maybe it’s injured or a different species. That’s the essence of anomaly detection: **spotting the odd one out**.

---

### ***2. Key Idea: Probability Modeling***

A common way to do anomaly detection is by modeling a **probability distribution** $p(x)$ over your data’s features. For each data point:
1. **Estimate** how likely (probable) that point is, based on past “normal” data.
2. If a **new** point $x_{\text{test}}$ has a **very low** probability (below some small $\epsilon$), it’s flagged as an **anomaly**.

```text
if   p(x_test) < ε  →  "Unusual/Anomaly"  
else                →  "Looks Normal"
```

- **Example**  
  - For an aircraft engine, features could be $x_1$ = heat generated, $x_2$ = vibration intensity, etc.  
  - We look at **all** the normal engines we have and learn a model $p(x)$ of how an engine typically behaves.  
  - When a **new** engine arrives, we measure ($x_1, x_2$). If $p(x_{\text{test}})$ is extremely small, the engine might be flawed or **risky**, so we **inspect** it further.

---

### ***3. Where It’s Used***

1. **Manufacturing**  
   - Aircraft engines, circuit boards, smartphones… you want to catch **defective** products before shipping them to customers.
   - By learning what “normal” products look like, anomaly detection can help **flag** suspicious items for **extra** testing.

2. **Fraud Detection**  
   - Monitor user activity (e.g., login frequency, transaction counts, typing speed).  
   - If a user’s behavior is **unusually** high or low compared to the general population, it might indicate **fraud**—warranting an extra check.

3. **System Monitoring**  
   - Track server or computer health (CPU usage, memory usage, network traffic).  
   - If one machine’s behavior **deviates** far from the “norm,” you might have a **hardware failure** or **security breach**.

> **Note**: Typically, we **don’t** immediately label a flagged item as 100% fraudulent or broken; we **investigate further** to confirm whether it’s really an issue or just an unusual but harmless event.

---

### ***4. Why It Works***

- **Most data is normal**  
  You have plenty of examples showing how a “good” engine or a “legitimate” user usually behaves.
- **Learn what’s typical**  
  Model $p(x)$ so it’s **large** in the dense “normal” region and **small** on the fringes.
- **Detect the rare**  
  If a new point falls **way outside** the normal region, $p(x_{\text{test}})$ becomes **tiny**, and we can **sound the alarm**.

---

### ***5. Key Takeaways***

1. **Unsupervised Approach**  
   - No labeled “bad” examples needed. We rely on the assumption that **most** data is normal.

2. **Probability Threshold**  
   - If $p(x_{\text{test}}) < \epsilon$, mark it as **anomaly**. Choosing $\epsilon$ carefully balances missing real anomalies vs. raising too many false alarms.

3. **Broad Applications**  
   - From **fraud** and **manufacturing** to **network** and **system monitoring**, anomaly detection **keeps watch** for out-of-the-ordinary events.

4. **Next Steps**  
   - Often, we use **Gaussian distributions** (or other advanced density estimations) to model $p(x)$.  
   - After flagging anomalies, **human** or **automated** checks can confirm if the item is truly problematic.

Anomaly detection may not be as talked about as other AI techniques, but it’s a **silent hero** in many industries. By learning what “normal” looks like, it shines a spotlight on potential **failures, frauds, or faults**, protecting everything from airplane engines to entire data centers. And you can apply it to **your own life** to become more **aware** of unusual patterns or success. That's the true power of anomaly detection. Let's dive in and see how it works!

---

## ***Gaussian (Normal) Distribution***

### ***Have you ever wondered why so many things in nature follow a “bell-shaped” curve?***

In many real-world situations—like measuring people’s heights or looking at how temperatures vary—data often clusters around a **center** value and then gently tapers off on either side. This shape is called the **Gaussian** (or **Normal**) distribution. It’s also known as a **bell curve** because, well…it looks like a bell!

---

### ***1. What is the Gaussian Distribution?***

Suppose you have a single feature $x$—for example, the height of a person. If $x$ follows a **Gaussian distribution** with:
- Mean (average) $ \mu $  
- Variance $ \sigma^2 $ (where $ \sigma $ is called the **standard deviation**)

then the **probability** that $x$ takes a particular value is given by:

$$
p(x) = \frac{1}{\sqrt{2\pi}\,\sigma} \exp\!\biggl(-\frac{(x - \mu)^2}{2\sigma^2}\biggr).
$$

- **$\mu$ (mu)** is where the **peak** of the bell is (the center).
- **$\sigma$ (sigma)** is how “spread out” the bell is. A **large** $\sigma$ means a wider curve, while a **small** $\sigma$ means a narrower, taller curve.

> **Analogy**: Think of **$\mu$** like the **average** height in your class. If most people cluster around that height, you’ll see a **peak** near $\mu$. The **standard deviation** $\sigma$ tells you how **much** people’s heights vary from that average. If $\sigma$ is large, some people are **much taller** or **much shorter** than $\mu$.

---

### ***2. Visualizing the Bell Curve***

If you plotted many measurements of $x$—like drawing a **histogram** of a thousand heights—you might get a shape that **resembles** the smooth Gaussian curve. The more data you have, and the more it truly follows a “centered + spread” pattern, the more it looks like that **bell**.

- **Center at $\mu$**: That’s the highest point of the curve.
- **Spread of $\sigma$**: How wide or narrow the bell appears.

| **Parameter** | **Effect**          |
|---------------|---------------------|
| $\mu$ (mean)  | Shifts the bell left or right.  |
| $\sigma$ (standard deviation)      | Stretches or compresses the bell’s width. |

---

### ***3. Changing $\mu$ and $\sigma$: Examples***

1. **$ \mu = 0, \sigma = 1$**  
   - Centered at 0, with a moderate spread.
2. **$ \mu = 0, \sigma = 0.5$**  
   - Still centered at 0, but more **narrow** and **tall**.
3. **$ \mu = 0, \sigma = 2$**  
   - Still centered at 0, but very **wide** and **short**.
4. **$ \mu = 3, \sigma = 0.5$**  
   - Shifted to **3** on the x-axis, narrower spread.

The **area under** each curve is **1**, which is how probabilities stay consistent (the total probability must be 1).

---

### ***4. Estimating $\mu$ and $\sigma^2$ from Data***

When you have a dataset $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$ of $m$ points, you can **estimate** the Gaussian parameters like this:

$$
\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)},
$$

$$
\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} \bigl(x^{(i)} - \mu\bigr)^2.
$$

- $\mu$ is just the **average** of all your data points.
- $\sigma^2$ is the **average** of the squared differences from that mean (i.e., how “spread out” the data is).

> **Fun Fact**: In statistics, some formulas use $1/(m-1)$ instead of $1/m$ for $\sigma^2$(If you want to know more, search for "Unbiased Estimator", "Degrees of Freedom", "Bessel's Correction"), but for many practical cases, $1/m$ is perfectly fine.

---

### ***5. Gaussian Distribution for Anomaly Detection***

Once you’ve **modeled** your data as a Gaussian, you get a **probability** for each new value of $x$:
- If $p(x)$ is **high**, $x$ seems “normal” (close to the center).
- If $p(x)$ is **low**, $x$ might be “anomalous” (far from the center), suggesting a **possible anomaly**.

This idea **extends** easily to more features (e.g., 2, 3, or 100 features) in higher-dimensional versions of the Gaussian. But the core concept remains:  
**Model** your normal data with a “bell” shape, then **flag** anything that’s unusually far from that bell as a potential anomaly.

---

### ***6. Key Takeaways***

1. **A Bell-Shaped World**  
   Many **natural** or **human** measurements (height, test scores, etc.) approximately follow this “bell” pattern.

2. **Parameters ($\mu$, $\sigma$) Define the Curve**  
   - $\mu$ says **where** it’s centered.  
   - $\sigma$ says **how wide** it spreads.

3. **Estimation from Data**  
   Simple **averages** give you a decent first guess at $\mu$ and $\sigma^2$.

4. **Foundation for Anomaly Detection**  
   Use the Gaussian model to see which points are **common** vs. **rare**.  
   Rare points may indicate **problems** or **interesting outliers**.

Understanding **Gaussian distributions** equips you with a **powerful** tool, not only in **anomaly detection** but in countless other areas (testing hypotheses, analyzing measurement errors, financial modeling and risk assessment, or just figuring out how random things distribute!). It's one of the **cornerstones** of statistics and machine learning.
