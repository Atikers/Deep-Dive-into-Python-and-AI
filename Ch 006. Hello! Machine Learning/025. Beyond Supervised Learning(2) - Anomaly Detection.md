# ***Beyond Supervised Learning (2) - Anomaly Detection***

## ***The Curious Chicken and the Mystery Eggs***

![alt text](images/image-25.jpg)

Once upon a time, in a cozy barnyard, Farmer Lily spent her days watching over her chickens. Most of them laid perfectly normal eggs—smooth, white or brown, and just the right size. But every now and then, Farmer Lily would stumble upon something peculiar: an egg that was oddly large, strangely colored, or dotted with mysterious spots. She wondered if it meant that chicken was sick, or if something else was wrong. She wanted a way to quickly tell the normal eggs from the strange ones, before anything bad happened to her flock.

One morning, an enchanted owl flew into the barnyard and offered Farmer Lily a magical measuring stick. This stick could measure many details about each egg: how big it was, how heavy it was, whether it had unusual markings, and even its shell temperature. Farmer Lily gathered all the normal eggs she could find and used the stick to learn what an average, healthy egg looked like. With each measurement, the stick built up a picture of “normal.”

From then on, whenever a new egg came along, the measuring stick would compare that egg’s numbers—like its size or color—to the numbers of all the normal eggs. If the new egg fit neatly in the normal range, the stick stayed silent. But if an egg’s measurements were way off—too large, too heavy, an odd color—the stick beeped loudly, signaling that something unusual was going on. Farmer Lily called these “strange” eggs anomalies because they didn’t match what she’d learned to be normal.

At first, Farmer Lily wasn’t sure how to choose the beep level for her magic stick. She tried setting it really low, so the stick beeped even if an egg was just a bit heavier than normal. Then she had too many false alarms—lots of normal eggs getting flagged. She raised the beep level really high, but then the stick missed some genuinely strange eggs. Eventually, by testing a few real odd eggs she already knew about, she found the right balance: a beep level that caught the weird eggs without scolding all the normal ones.

Farmer Lily also discovered that adding more details helped. If she only measured egg size, maybe an egg seemed normal when it was actually a strange shape. But once she added color or shell strength to her measurements, the differences became clearer. If she spotted a suspicious example that the stick missed, she’d figure out a new feature—maybe how bumpy the shell was—and teach the stick that this feature mattered. Over time, her method got better at telling normal eggs from truly odd ones.

In the end, Farmer Lily’s magical system protected her chickens by detecting potential problems early. If an egg showed signs that were far from normal—blue spots, weird lumps, super-heavy weight—she could investigate immediately and make sure her chickens were healthy. Meanwhile, all the perfectly ordinary eggs stayed under the radar, so she wouldn’t waste time on false alarms. And that is how anomaly detection saved the day in Farmer Lily’s barnyard: by using clever measurements, a bit of magical know-how, and just enough real-world knowledge to separate the truly strange eggs from the harmless ones.

---

## ***Finding Unusual Events***

### ***Have you ever wondered how systems automatically catch “strange” or “rare” or "Fraudulent" behaviors?***

Meet **anomaly detection**, our second unsupervised learning algorithm. It learns what “normal” looks like from unlabeled data—so when something **unusual** shows up, it can raise a red flag. Whether it’s monitoring aircraft engines or spotting credit card fraud, anomaly detection is a **powerful tool** for safeguarding against unexpected or dangerous events.

---

### ***1. What Is Anomaly Detection?***

**Anomaly detection** looks at a dataset of **mostly “normal”** examples and tries to figure out **when a new example doesn’t fit** the usual pattern. It doesn’t need labels for “bad” or “good”; it just knows what **typical** behavior looks like.

- **Analogy**: Imagine you watch a flock of birds daily. You learn to recognize how they usually move and sound. If a bird suddenly **flies very differently** or makes an **odd** sound, your instinct says something’s off—maybe it’s injured or a different species. That’s the essence of anomaly detection: **spotting the odd one out**.

---

### ***2. Key Idea: Probability Modeling***

A common way to do anomaly detection is by modeling a **probability distribution** $p(x)$ over your data’s features. For each data point:
1. **Estimate** how likely (probable) that point is, based on past “normal” data.
2. If a **new** point $x_{\text{test}}$ has a **very low** probability (below some small $\epsilon$), it’s flagged as an **anomaly**.

```text
if   p(x_test) < ε  →  "Unusual/Anomaly"  
else                →  "Looks Normal"
```

- **Example**  
  - For an aircraft engine, features could be $x_1$ = heat generated, $x_2$ = vibration intensity, etc.  
  - We look at **all** the normal engines we have and learn a model $p(x)$ of how an engine typically behaves.  
  - When a **new** engine arrives, we measure ($x_1, x_2$). If $p(x_{\text{test}})$ is extremely small, the engine might be flawed or **risky**, so we **inspect** it further.

---

### ***3. Where It’s Used***

1. **Manufacturing**  
   - Aircraft engines, circuit boards, smartphones… you want to catch **defective** products before shipping them to customers.
   - By learning what “normal” products look like, anomaly detection can help **flag** suspicious items for **extra** testing.

2. **Fraud Detection**  
   - Monitor user activity (e.g., login frequency, transaction counts, typing speed).  
   - If a user’s behavior is **unusually** high or low compared to the general population, it might indicate **fraud**—warranting an extra check.

3. **System Monitoring**  
   - Track server or computer health (CPU usage, memory usage, network traffic).  
   - If one machine’s behavior **deviates** far from the “norm,” you might have a **hardware failure** or **security breach**.

> **Note**: Typically, we **don’t** immediately label a flagged item as 100% fraudulent or broken; we **investigate further** to confirm whether it’s really an issue or just an unusual but harmless event.

---

### ***4. Why It Works***

- **Most data is normal**  
  You have plenty of examples showing how a “good” engine or a “legitimate” user usually behaves.
- **Learn what’s typical**  
  Model $p(x)$ so it’s **large** in the dense “normal” region and **small** on the fringes.
- **Detect the rare**  
  If a new point falls **way outside** the normal region, $p(x_{\text{test}})$ becomes **tiny**, and we can **sound the alarm**.

---

### ***5. Key Takeaways***

1. **Unsupervised Approach**  
   - No labeled “bad” examples needed. We rely on the assumption that **most** data is normal.

2. **Probability Threshold**  
   - If $p(x_{\text{test}}) < \epsilon$, mark it as **anomaly**. Choosing $\epsilon$ carefully balances missing real anomalies vs. raising too many false alarms.

3. **Broad Applications**  
   - From **fraud** and **manufacturing** to **network** and **system monitoring**, anomaly detection **keeps watch** for out-of-the-ordinary events.

4. **Next Steps**  
   - Often, we use **Gaussian distributions** (or other advanced density estimations) to model $p(x)$.  
   - After flagging anomalies, **human** or **automated** checks can confirm if the item is truly problematic.

Anomaly detection may not be as talked about as other AI techniques, but it’s a **silent hero** in many industries. By learning what “normal” looks like, it shines a spotlight on potential **failures, frauds, or faults**, protecting everything from airplane engines to entire data centers. And you can apply it to **your own life** to become more **aware** of unusual patterns or success. That's the true power of anomaly detection. Let's dive in and see how it works!

---

## ***Gaussian (Normal) Distribution***

### ***Have you ever wondered why so many things in nature follow a “bell-shaped” curve?***

In many real-world situations—like measuring people’s heights or looking at how temperatures vary—data often clusters around a **center** value and then gently tapers off on either side. This shape is called the **Gaussian** (or **Normal**) distribution. It’s also known as a **bell curve** because, well…it looks like a bell!

---

### ***1. What is the Gaussian Distribution?***

Suppose you have a single feature $x$—for example, the height of a person. If $x$ follows a **Gaussian distribution** with:
- Mean (average) $\mu$  
- Variance $\sigma^2$ (where $\sigma$ is called the **standard deviation**)

then the **probability** that $x$ takes a particular value is given by:

$$
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-(x - \mu)^2}{2\sigma^2}}
$$

- **$\mu$ (mu)** is where the **peak** of the bell is (the center).
- **$\sigma$ (sigma)** is how “spread out” the bell is. A **large** $\sigma$ means a wider curve, while a **small** $\sigma$ means a narrower, taller curve.

> **Analogy**: Think of **$\mu$** like the **average** height in your class. If most people cluster around that height, you’ll see a **peak** near $\mu$. The **standard deviation** $\sigma$ tells you how **much** people’s heights vary from that average. If $\sigma$ is large, some people are **much taller** or **much shorter** than $\mu$.

---

### ***2. Visualizing the Bell Curve***

If you plotted many measurements of $x$—like drawing a **histogram** of a thousand heights—you might get a shape that **resembles** the smooth Gaussian curve. The more data you have, and the more it truly follows a “centered + spread” pattern, the more it looks like that **bell**.

- **Center at $\mu$**: That’s the highest point of the curve.
- **Spread of $\sigma$**: How wide or narrow the bell appears.

| **Parameter** | **Effect**          |
|---------------|---------------------|
| $\mu$ (mean)  | Shifts the bell left or right.  |
| $\sigma$ (standard deviation)      | Stretches or compresses the bell’s width. |

---

### 3. Changing $\mu$ and $\sigma$: Examples

1. **$\mu = 0, \sigma = 1$**  
   - Centered at 0, with a moderate spread.
2. **$\mu = 0, \sigma = 0.5$**  
   - Still centered at 0, but more **narrow** and **tall**.
3. **$\mu = 0, \sigma = 2$**  
   - Still centered at 0, but very **wide** and **short**.
4. **$\mu = 3, \sigma = 0.5$**  
   - Shifted to **3** on the x-axis, narrower spread.

The **area under** each curve is **1**, which is how probabilities stay consistent (the total probability must be 1).

---

### 4. Estimating $\mu$ and $\sigma^2$ from Data

When you have a dataset $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$ of $m$ points, you can **estimate** the Gaussian parameters like this:

$$
\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)},
$$

$$
\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} \bigl(x^{(i)} - \mu\bigr)^2.
$$

- $\mu$ is just the **average** of all your data points.
- $\sigma^2$ is the **average** of the squared differences from that mean (i.e., how “spread out” the data is).

> **Fun Fact**: In statistics, some formulas use $1/(m-1)$ instead of $1/m$ for $\sigma^2$(If you want to know more, search for "Unbiased Estimator", "Degrees of Freedom", "Bessel's Correction"), but for many practical cases, $1/m$ is perfectly fine.

---

### ***5. Gaussian Distribution for Anomaly Detection***

Once you’ve **modeled** your data as a Gaussian, you get a **probability** for each new value of $x$:
- If $p(x)$ is **high**, $x$ seems “normal” (close to the center).
- If $p(x)$ is **low**, $x$ might be “anomalous” (far from the center), suggesting a **possible anomaly**.

This idea **extends** easily to more features (e.g., 2, 3, or 100 features) in higher-dimensional versions of the Gaussian. But the core concept remains:  
**Model** your normal data with a “bell” shape, then **flag** anything that’s unusually far from that bell as a potential anomaly.

---

### ***6. Key Takeaways***

1. **A Bell-Shaped World**  
   Many **natural** or **human** measurements (height, test scores, etc.) approximately follow this “bell” pattern.

2. **Parameters ($\mu$, $\sigma$) Define the Curve**  
   - $\mu$ says **where** it’s centered.  
   - $\sigma$ says **how wide** it spreads.

3. **Estimation from Data**  
   Simple **averages** give you a decent first guess at $\mu$ and $\sigma^2$.

4. **Foundation for Anomaly Detection**  
   Use the Gaussian model to see which points are **common** vs. **rare**.  
   Rare points may indicate **problems** or **interesting outliers**.

Understanding **Gaussian distributions** equips you with a **powerful** tool, not only in **anomaly detection** but in countless other areas (testing hypotheses, analyzing measurement errors, financial modeling and risk assessment, or just figuring out how random things distribute!). It's one of the **cornerstones** of statistics and machine learning.

---

## ***Anomaly Detection Algorithm***

### ***Have you ever wondered how to detect anomalies when you have multiple features at once?***

Previously, we looked at how a single-feature Gaussian (Normal) distribution can help us identify unusual data points. But in **real-world** scenarios, we often have **more than one** feature—like temperature, vibration, and pressure all at once. How do we extend the idea of a **bell curve** into multiple dimensions? That’s where **density estimation** with multiple features comes in.

---

### ***1. Modeling Multiple Features***

Let’s say each example $\vec{x}^{(i)}$ has **$n$ features**:

$$
\vec{x}^{(i)} =
\begin{pmatrix}
x_1^{(i)} \\
x_2^{(i)} \\
\vdots \\
x_n^{(i)}
\end{pmatrix}.
$$

For anomaly detection, we want to model the **probability** of observing each feature combination:  

$$
p(\vec{x}) = p(x_1, x_2, \dots, x_n).
$$

**Assumption** (for simplicity):  
We often assume the features are **independently** distributed (although this can be an approximation). In that case,

$$
p(\vec{x}) = p(x_1)\times p(x_2)\times \cdots \times p(x_n).
$$

Then we can **fit** each feature with its own 1D Gaussian, characterized by $(\mu_j, \sigma_j^2)$ for $j = 1, 2, \dots, n.$

> **Analogy**: Think of each feature as a separate **bell curve**. The combined probability is like **multiplying** these bell-curve probabilities together, giving a single measure of how “normal” your entire set of features is at once.

---

### ***2. Fitting the Gaussian Parameters***

For each feature $j$, we estimate its:
- **Mean**:  

$$
\mu_j = \frac{1}{m} \sum_{i=1}^{m} x_j^{(i)}.
$$
- **Variance**:  

$$
\sigma_j^2 = \frac{1}{m} \sum_{i=1}^{m} \bigl(x_j^{(i)} - \mu_j\bigr)^2.
$$

Here,
- $x_j^{(i)}$ is the value of feature $j$ in the $i$ th example,
- $m$ is the number of training examples (all assumed “normal” data).

Once we have **$\mu_j$** and **$\sigma_j^2$** for **each** feature $j$, we can write the probability density for the entire feature vector $\vec{x}$:  

$$
p(\vec{x}) = \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-(x - \mu)^2}{2\sigma^2}}
$$

---

### ***3. Computing the Anomaly Score***

1. **Calculate $p(\vec{x}^{\text{test}})$**  
   - Plug your new test point’s features into the formula for $p(\vec{x})$.  
   - If the result (probability) is **very small**, it indicates $\vec{x}^{\text{test}}$ is **unlikely** under the model of “normal” data.

2. **Threshold $\varepsilon$ (epsilon)**  
   - Choose a **small** cutoff $\varepsilon$.  
   - If $p(\vec{x}^{\text{test}}) < \varepsilon$, **flag** it as an anomaly.

> **Why a threshold?**  
> Because you need a **practical** way to say: “When is a probability *so small* that we consider it suspicious?”

---

### ***4. Putting It All Together: Steps of the Algorithm***

1. **Choose $n$ Features**  
   - Pick features you believe might show **anomalous** behavior if something goes wrong (e.g., temperature, vibration, memory usage, etc.).

2. **Estimate Parameters**  
   - For each feature $j$:  

$$
\mu_j = \frac{1}{m} \sum_{i=1}^{m} x_j^{(i)}, 
\quad
\sigma_j^2 = \frac{1}{m} \sum_{i=1}^{m} (x_j^{(i)} - \mu_j)^2.
$$

3. **Compute Probability for a New Example**  
   - For a new $\vec{x}^{\text{test}}$, calculate:  

$$
p(\vec{x}^{\text{test}}) = \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-(x - \mu)^2}{2\sigma^2}}
$$

4. **Compare with Threshold $\varepsilon$**  
   - If $p(\vec{x}^{\text{test}}) < \varepsilon$, **label** it as **anomaly**; otherwise, it’s considered **normal**.

---

### ***5. Why This Works***

- **Captures Each Feature’s Behavior**  
  By modeling each feature’s distribution separately, you account for what’s “normal” for that specific attribute (e.g., typical engine temperature range).
- **Combines Them Together**  
  The **product** of probabilities ensures that a **very low** probability on **any** key feature can make the overall probability small—hinting that something’s amiss.
- **Threshold Tuning**  
  Adjusting $\varepsilon$ helps control the **trade-off** between catching real anomalies (true positives) and avoiding too many **false alarms**.

---

### ***6. Key Takeaways***

1. **Multiple Features, One Probability**  
   - We treat each feature with a **Gaussian** and multiply them to get the final $p(\vec{x})$.
2. **Simple Parameter Estimation**  
   - Just compute the **mean** and **variance** for each feature from your (normal) training data.
3. **Flexible & Extendable**  
   - Add or remove features depending on your problem. The approach remains the same.
4. **Threshold for Anomalies**  
   - Decide a **cutoff** that flags data points with extremely **low** probability as “suspicious.”

Whether you’re dealing with **aircraft engines**, **web server logs**, or **credit card transactions**, this multi-feature Gaussian approach is a straightforward and effective way to spot things that just **don’t** look like they belong. It’s the core principle behind many industrial-scale anomaly detection systems, quietly ensuring **safety** and **security** across countless applications.

---

## ***Developing and Evaluating an Anomaly Detection System***  

### ***Have you ever wondered how we know our anomaly detector is “working” if there are almost no anomalies?***

One big challenge in **anomaly detection** is that most data is normal, while anomalies (the weird or faulty cases) are **rare**. So how do we **evaluate** whether our system can catch those few anomalies without crying wolf too often on normal items? Let’s explore how **labeled** data—though small—can help us tune and test an anomaly detector.

---

### ***1. Why Having (Even a Few) Labeled Anomalies Helps***

Imagine you’re a **treasure hunter** who mostly digs up normal rocks (label = 0), but occasionally finds precious gems (label = 1). The number of gems (anomalies) is tiny compared to all the rocks. Even a **handful** of those labeled gems can help you:

- **Check if your “treasure detector” is accurate**: Are you correctly finding the gems?
- **Avoid false alarms**: Are you flagging too many normal rocks as precious?

If you had **no** labeled anomalies, you’d be flying blind—relying purely on unsupervised signals. But **one or two labeled gems** in your validation set can guide adjustments (like changing your detector’s threshold).

---

### ***2. Splitting Data: Training, Validation, and Test***

#### ***Scenario: Aircraft Engines***
Suppose you’ve built 10,000 **good** engines ($y=0$) over the years but only **20** flawed ones ($y=1$). That’s a stark imbalance: 20 anomalies vs. 10,000 normal.

**Possible Split:**
- **Training Set**: 6,000 normal engines. (Even if 1–2 anomalies slip in, it typically won’t ruin the training.)
- **Cross-Validation (CV) Set**: 2,000 normal + 10 anomalies.
- **Test Set**: 2,000 normal + 10 anomalies.

1. **Train** a probability model $p(x)$ on the 6,000 normal engines (largely unsupervised).
2. **Tune** your threshold $\varepsilon$ using the CV set. Check how many of the 10 anomalies are **correctly detected** and how many normal ones get **incorrectly flagged**.
3. **Evaluate** on the test set to see if your final settings still do well on “new” data.

> **Analogy**:  
> Think of your training set as the **main cooking recipe**—all normal ingredients. Your CV set is where you **taste-test** new experimental flavors, including a few “strange” ones. Once you’re satisfied, you show it to a **taste panel** (the test set) to confirm the dish is truly good and doesn’t miss unusual flavors.

---

#### ***What If You Have Very Few Anomalies?***
Sometimes you might have only **2** or **3** anomalies total. You could:
- Use **no separate test set** and just have a **training set** + **CV set**.  
- Put **all** known anomalies into the CV set so you can at least tune $\varepsilon$ in a meaningful way.

**Caution**: This means you won’t have a separate test set to confirm final performance. You risk **overfitting** to the CV set. But if anomalies are extremely scarce, it’s still often the **best** approach.

---

### ***3. Evaluating the Algorithm***

After training $p(x)$ on normal data:

1. **Predict** $y = 1$ (“anomaly”) if $p(x) < \varepsilon$; else predict $y=0$ (“normal”).
2. **Compare** predictions against the **labeled** anomalies (and normal examples) in the CV or test set.

Because you may have **10 anomalies** vs. **2,000 normal** in a validation set (very skewed), you can consider:
- **True Positives (TP)**: flagged anomalies that are **truly** anomalies.
- **False Positives (FP)**: flagged anomalies that are actually **normal** items.
- **False Negatives (FN)**: missed anomalies your detector thinks are normal.
- **True Negatives (TN)**: normal items flagged as normal (most of them).

From these, you can compute **Precision**, **Recall**, and the **$F_1$ score**. These metrics are often more helpful than just accuracy when **$y=1$** is rare(you would remember the concept of **Precision, Recall**, and **F1 score** in the previous chapter(*021. Handling Imbalanced Data*)).

> **Analogy**:  
> - **Precision** is like “If you pick up a rock claiming it’s a gem, how often are you right?”  
> - **Recall** is “Of all the actual gems out there, how many did you find?”

---

### ***4. Key Takeaways***

1. **Small Labeled Anomalies = Big Help**  
   - Even a few known “bad” examples let you **tune** your system more effectively.
   
2. **Data Splits**  
   - Train on mostly normal data, put your **few anomalies** into cross-validation (and test) for **evaluation**.

3. **Skewed Metrics**  
   - In highly imbalanced scenarios (lots of normal, few anomalies), rely on **Precision, Recall, F1**—not just accuracy.

4. **When Data Is Extremely Scarce**  
   - You might **merge** CV and test sets, but beware of possible **overfitting**.

By combining **unsupervised** modeling of normal data with **a pinch** of labeled anomalies, you can **fine-tune** your anomaly detection approach. Whether you’re monitoring **aircraft engines**, searching for **fraudulent transactions**, or looking for **strange signals** in sensor data, having **at least a few** known anomalies to guide your threshold selection is like having a **compass**—it helps you navigate more confidently toward a system that catches the real outliers without causing too many false alarms.

---

## ***Anomaly Detection vs. Supervised Learning***

### ***Have you ever wondered why some rare problems need a special approach(anomaly detection), while others are tackled by regular classification?***

In the world of **machine learning**, you'll often see two methods mentioned: **anomaly detection** and **supervised learning**. Both can spot "positive" examples like *fraudulent transactions* or *scratched smartphones*. (Note: Here "positive" doesn't mean "good" - it simply means the label is 1, while "negative" means the label is 0) These methods work differently depending on **how many** examples you have and whether you expect *brand-new* or *already-familiar* problems to arise.

---

### ***1. When Anomaly Detection Shines***

1. **Very Few "Positive" Examples**  
   - You might have only 0–20 “anomalies” ($y=1$) but lots of normal data ($y=0$).  
   - **Analogy**: Think of searching for *unusual gemstones* in a *giant pile of rocks*. You only have a couple gem samples to know what “strange” might look like.

2. **Many Different Ways to Fail**  
   - Future anomalies can appear in forms you’ve **never** seen before.  
   - **Analogy**: If a *car engine* can fail in a *thousand* unexpected ways, your few known examples won’t cover all those possibilities.

3. **Goal: Model “Normal,” Flag “Weird”**  
   - Anomaly detection **learns** from the **normal data** to figure out what’s typical, and then anything that *deviates strongly* is flagged as anomalous.  
   - **Analogy**: If you’re used to seeing *smooth apples* all day, the first time you see an *apple with neon polka dots*, you’d immediately suspect it’s *unusual*.

**Common Uses**  
- **Fraud detection** (new scam types pop up).  
- **Manufacturing**: new, unseen defects (e.g., brand-new ways for engines to fail).  
- **Cybersecurity**: hackers invent fresh exploits.

---

### ***2. When Supervised Learning Works Better***

1. **Enough Positive Examples**  
   - You have **plenty** of “positive” cases ($y=1$) and also many normal ($y=0$).  
   - **Analogy**: If you have 20 *clear pictures* of how a “dangerous insect” looks, your classifier can *memorize* those patterns and easily spot them again.

2. **Future Positives Look Similar**  
   - Future anomalies (positives) are likely to resemble the ones you’ve **already** seen in training.  
   - **Analogy**: *Spam emails* - new spam is usually just a variation of old spam (“Lose weight fast!”, “Win a prize!”, etc.).

3. **Goal: Distinguish Labeled Groups**  
   - Supervised learning is trained on both positive and negative examples. It *learns directly* what “positive” looks like and can classify new examples accordingly.

**Common Uses**  
- **Spam detection** (repetitive patterns).  
- **Known manufacturing defects** (e.g., repeated scratches).  
- **Weather prediction** (sunny, rainy—labels seen many times before).  
- **Disease classification** (looking for the same illness patterns).

---

### ***3. Comparing Both Approaches***

| **Criteria**                  | **Anomaly Detection**                                                         | **Supervised Learning**                                           |
|------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------|
| **Positive Examples**        | Very **few** (0–20 typical).                                                 | **Many** positive & negative examples.                            |
| **Types of Anomalies**       | Could be **brand-new** forms we never saw before.                             | Often **similar** to past positives in the training set.          |
| **Training Focus**           | Model **normal** data; label unusual points as outliers.                      | Model **both** normal ($y=0$) and positive ($y=1$) examples.       |
| **Use Cases**                | Fraud detection, new manufacturing defects, novel security threats, etc.      | Spam detection, repeated known defects, weather/disease classes.  |

---

### ***4. Key Takeaways***

- **Choose Anomaly Detection** if:  
  - You have **extremely few** known positives.  
  - You expect **many “unknown” ways** something can go wrong.  
- **Choose Supervised Learning** if:  
  - You have **enough** positive examples to learn from.  
  - Future positives will likely **match** previously seen patterns.

> **Analogy Summary**:  
> - **Anomaly detection** is like trying to spot *brand-new*, *unexpected creatures* in a forest when you’ve only seen a handful before.  
> - **Supervised learning** is like identifying *known species* of animals you’ve already encountered many times.

No matter which path you pick, the right choice depends on **how many** positives you have and **whether** you expect new anomalies to look *nothing like* the old ones. By thinking about these factors, you’ll choose the **most effective** approach for detecting that next *unusual* glitch—or that next *suspicious* email in your inbox!

---

## ***Choosing What Features to Use***

### ***Have you ever wondered why some anomalies slip through the cracks or why your detector keeps flagging perfectly normal data?***

In **anomaly detection**, the **features** you choose (and how you transform them) can dramatically impact performance. Unlike supervised learning—where labels guide the algorithm to ignore or emphasize certain features—anomaly detection typically relies on **unlabeled** data. As a result, **poorly chosen or poorly scaled** features can lead your model to **miss** true anomalies or **false alarm** on normal points. Let’s explore **why** feature selection matters and how to pick the best transformations.

---

### ***1. Aim for (Approximate) Gaussian Features***

When you use a **Gaussian (Normal)** model for each feature, it helps if that feature’s distribution is *not too skewed*. If a feature $x$ looks **highly non-Gaussian**, you can try **transforming** it to become more bell-shaped.

#### **Common Transformations**
- $x \leftarrow \log(x)$ or $x \leftarrow \log(x + C)$  
  - Good for data that **grows quickly** and covers many orders of magnitude.
- $x \leftarrow \sqrt{x}$ or $x \leftarrow x^{1/3}$  
  - Helps when large values distort the distribution too much.
- $x \leftarrow x^p$ (for some power $p < 1$)  
  - Tweaking $p$ can **squash** big values or **stretch** small ones to produce a more balanced shape.

> **Analogy**:  
> Imagine you’re recording **loudness** of various sounds. Most normal sounds cluster at moderate decibels, but a jet engine is *way louder* than everyday noises. Taking a **log** of decibels can level the playing field, so extremely large values don’t overshadow everything else.

#### **Practical Tip**  
- Plot a **histogram** of your feature’s values to see if it looks roughly like a bell curve. If not, **experiment** with different transformations until it’s closer to normal.  
- **Apply** the **same** transformation to your training, validation, and test sets.

---

### ***2. Use Error Analysis to Improve Features***

After training your anomaly detector, look at **which anomalies it fails to catch** or **which normal points it flags incorrectly** in your cross-validation set. These examples often provide **clues** about better features to add or transform.

#### **Example: Typing Speed**
- Suppose $x_1$ is the number of transactions a user makes. A certain user’s $x_1$ matches normal behavior, so $p(x)$ remains large (i.e., not flagged).  
- But if you examine that user manually, you realize their **typing speed** is *unusually fast*—a suspicious sign for a *robotic script* or a *stolen account*.  
- **Solution**: Add a new feature $x_2$ = “typing speed.” Now the user might appear *far from* normal on $x_2$, making $p(x_1, x_2)$ **smaller** and easier to flag.

> **Analogy**:  
> If you’re missing an anomaly, it’s like having a puzzle piece that doesn’t quite fit. You investigate, realize you need a **new dimension** (feature) that captures the culprit’s unique trait—like adding an extra puzzle piece so the overall picture becomes clear.

---

### ***3. Combining Existing Features***

Sometimes, neither CPU load ($x_3$) nor network traffic ($x_4$) alone is an **anomaly**, but their **ratio** might be unusual. For instance, a server with **high CPU** usage but **low network** traffic could indicate a hidden malfunction.

#### **Create Ratios or Other Derived Features**
- $x_5 = \dfrac{\text{CPU load}}{\text{network traffic}}$
- $x_6 = \dfrac{(\text{CPU load})^2}{\text{network traffic}}$
- And so on…

This is especially helpful in **data center monitoring**, **manufacturing**, or any domain where *combinations* of raw features expose anomalies more clearly than each feature by itself.

---

### ***4. Practical Workflow***

1. **Plot Distributions**  
   - For each feature, check if it’s *roughly* Gaussian. If skewed, experiment with transformations.
2. **Train Initial Model**  
   - Fit your anomaly detection algorithm using the (potentially transformed) features.
3. **Error Analysis**  
   - Identify **missed anomalies** (false negatives) and **falsely flagged** normal examples (false positives).
   - Ask: *What extra data point or combination of data points would make this outlier truly stand out?*
4. **Add/Transform Features**  
   - Try new forms ($x \leftarrow \log(x + C)$, $x \leftarrow x^p$, or create ratio features).  
   - Retrain and see if $p(x)$ **better separates** normal from anomalous.

> **Analogy**:  
> Think of anomaly detection as **spotting suspicious characters** in a crowd. Initially, you only measure people’s heights, so a sneaky individual of average height goes unnoticed. By also measuring **unusually big body** (a new feature), the same individual no longer blends in.

---

### ***5. Key Takeaways***

- **Feature Quality Matters More Here**  
  In anomaly detection, you *don't* have labeled data telling the algorithm what's important; it's up to **you** to pick or **transform** features that reveal anomalies.
- **Transforming Non-Gaussian Features**  
  - Makes them *fit* the Gaussian distribution assumption *better*—so $p(x)$ more reliably identifies outliers.
- **Look at Errors, Then Iterate**  
  - Each missed anomaly or false alarm is a potential **hint** for a better feature or transformation.
- **Combine Features**  
  - Ratios or polynomial combinations can expose anomalies that single raw features can’t.

By carefully **crafting** and **refining** features—whether by taking logs, creating ratios, or adding entirely new metrics—you’ll empower your anomaly detection algorithm to separate the *truly normal* from the *truly odd*. This **focus on feature engineering** is often the secret ingredient that makes unsupervised anomaly detection systems truly **effective** in practice.