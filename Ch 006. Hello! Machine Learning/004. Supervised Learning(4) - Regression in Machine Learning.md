# Supervised Learning(4) - Regression in Machine Learning

## ***Introduction***

Imagine you want to predict the **price** of a house based on its **size**. This is a common problem in machine learning, and it can be solved using a type of **supervised learning** called **linear regression**. But what exactly is **supervised learning**?

In **supervised learning**, the computer learns by looking at **examples** where it knows both the **input** and the **output** (the correct answer). 

For example, if you show the computer a house's **size** (input) and its **price** (output), the computer can learn from these examples and make predictions for new houses that it hasn’t seen before.

## ***Example: Predicting House Prices***

Let’s say you’re a **real estate agent** and want to help your client predict how much their house could sell for. You have a dataset with the **sizes** and **prices** of houses in New York.

### ***Step 1: Plotting the Data***

You can plot the data on a **graph**:
- The **horizontal axis** represents the size of the house in **square feet**.
- The **vertical axis** represents the **price** of the house in **thousands of dollars**.

Each data point (represented as a small cross) shows the size and price of a house. For example, a house that is 2,104 square feet and sold for $400,000 will be one of the points on this graph.

### ***Step 2: Fitting a Line (Linear Regression)***

Now, let’s say you want to predict the price of a house that is **1,250 square feet**. You can use **linear regression**, which means you fit a **straight line** to the data. This line helps you make predictions based on the size of the house.

Once you have the line, you can check where the house with 1,250 square feet lands on the line. Let’s say the line suggests that this house might sell for around **$220,000**. That’s your prediction!

## ***Regression vs. Classification***

Linear regression is a type of **regression model**. In **regression**, the goal is to predict a **number**. For example, in this case, you are predicting a **house price** like $220,000 or $300,000. 

Regression models always predict a **continuous** range of numbers, meaning there are many possible outcomes.

On the other hand, there is another type of supervised learning called **classification**. In classification, instead of predicting a number, the computer predicts **categories**. For example:
- A **cat or dog** in a picture (two categories).
- Whether a patient has a specific **disease** or not (two categories).
- Recognizing one of **10 medical conditions** (10 categories).

In classification, there are only a small number of possible outputs. In contrast, in **regression**, there are **infinitely many possible numbers** that the model could predict.

## ***Understanding the Data Table***

In addition to plotting the data on a graph, you can also organize it into a **data table**:
- One column shows the **size** of the house (input).
- The other column shows the **price** of the house (output).

For example, if you have 47 houses in your dataset, your table will have 47 rows. Each row represents a house with its size and price. This table helps the computer learn how size relates to price.

## ***Standard Notation in Machine Learning***

In machine learning, there’s a common way to write inputs and outputs:
- **x**: The input variable (also called a **feature**). In this case, **x** is the size of the house.
- **y**: The output variable (also called a **target**). Here, **y** is the price of the house.

For example:
- If the house is 2,104 square feet, then **x = 2104**.
- If the price is $400,000, then **y = 400** (since prices are in thousands of dollars).

If you have **m** training examples, like **47 houses**, you can say that **m = 47**.

To represent a single training example, you can write **(x, y)**. For example:
- The first house is **(2104, 400)**, meaning 2,104 square feet and $400,000.
- The second house might be **(1416, 232)**, meaning 1,416 square feet and $232,000.

We use $x^{(i)}$ and $y^{(i)}$ to refer to the **i-th** training example, where **i** is the row number in the table. For example:
- $x^{(1)}$ = 2104 and $y^{(1)}$ = 400 for the first house.
- $x^{(2)}$ = 1416 and $y^{(2)}$ = 232 for the second house.

## ***Example: Predicting House Prices***

Let’s say you're trying to predict the **price** of a house based on its **size**. You have a dataset that includes the sizes of several houses and the prices they sold for. This dataset is called your **training set**. 

### ***Step 1: Input Features and Output Targets***

- The **input feature** is the **size of the house** (measured in square feet). We call this **x**.
- The **output target** is the **price** of the house (in dollars). We call this **y**.

So, for each house, you have both **x** (the size) and **y** (the price). These are the examples you give the computer to help it learn.

### ***Step 2: Building the Model (Function)***

Once you feed this data to the computer, it creates a **function**. This function is like a recipe that takes **$x$** (the size of a house) and predicts **$\hat{y}$** (the estimated price of the house). We call this function **$f$**.

- **$f(x)$ = $\hat{y}$** means "the function **$f$** takes an input **$x$** and gives a prediction **$\hat{y}$**.
- **$\hat{y}$** is the computer’s **guess** for the price of a house. It might not be perfect, but it’s an estimate based on the data it has seen.

## ***Linear Regression: Drawing a Straight Line***

In this example, we’re going to use a **linear regression** model. This means the computer will try to fit a **straight line** through the data to make predictions.

### ***Step 3: The Linear Function***

The **linear function** we use is written as:

$$f(x) = w \cdot x + b$$


- **$w$** is the **slope** of the line (how steep it is).
- **$b$** is the **intercept** (where the line crosses the vertical axis).
- **$x$** is the size of the house (input).
- **$\hat{y}$** is the predicted price (output).

The goal of linear regression is to find the best values of **$w$** and **$b$** so that the line fits the data as closely as possible. The line should go through or close to most of the data points.

## ***Why Use a Straight Line?***

You might ask, "Why do we use a **straight line**? Why not a curve or some other shape?" Well, sometimes more complex shapes are useful, but we start with a straight line because it’s **simple** and **easy** to work with. Once you understand how to fit a straight line, you can later learn how to fit more complex models like **curves**.

## ***Univariate Linear Regression***

In this example, we are only using **one input** (the size of the house), so this model is called **univariate linear regression**. 

- **Uni** means "one" (from Latin), so univariate just means "one variable."
- We are only using the size of the house as the input, but later, we can add more inputs like the number of bedrooms or the location.

## ***Step 4: Making Predictions***

Once the computer has created a line that fits the data, you can use the model to make **predictions**. For example:
- If a house is 1,250 square feet, the model might predict that the price will be around $220,000.
- If a house is 2,000 square feet, the model might predict a different price.

The model takes the input (size of the house) and uses the function to predict the output (price).