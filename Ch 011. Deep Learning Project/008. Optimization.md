# Optimization

> ***Have you ever wondered why some neural networks converge beautifully while others oscillate wildly, never quite reaching their destination? What if the difference between a model that trains in hours versus days lies not in its architecture, but in how it learns to learn?***

In our theoretical exploration, we discovered how different optimization algorithms work conceptually. Now, it's time to implement them ourselves and witness their behavior firsthand. Through this hands-on journey, you'll transform from someone who knows about optimization to someone who truly understands it.d

## Setting the Stage: Understanding Our Tools

Before we dive into implementation, let's establish a clear mental model. Think of training a neural network as teaching someone to navigate through a complex landscape blindfolded, using only the slope beneath their feet as guidance.

In programming terms, we work with several key objects:
- **Parameters**: The current position in our landscape
- **Gradients**: The slope at our current position
- **Learning Rate**: How big our steps are
- **Optimizer**: The strategy for choosing our path

Python provides us with a simple yet powerful syntax for mathematical operations:

```python
# Basic operations that power deep learning
a / b           # Division - as natural as writing math
beta**t         # Exponentiation - beta to the power of t
np.sqrt(x)      # Square root - for algorithms like Adam
```

These operations might seem trivial, but they're the building blocks of every optimization algorithm. Just as legal documents are built from precise terminology, optimization algorithms are built from these fundamental mathematical operations.

## Exercise 1: Implementing Basic Gradient Descent

Let's start with the foundation - implementing the gradient descent update rule. This is where our Parameters object receives its instructions to move:

```python
def update_parameters_with_gd(parameters, grads, learning_rate):
    """
    Update parameters using gradient descent.
    
    Think of this as our Navigator object taking a single step downhill.
    """
    L = len(parameters) // 2  # number of layers
    
    for l in range(1, L + 1):
        # The update rule: new position = current position - step_size Ã— slope
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)]
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * grads["db" + str(l)]
    
    return parameters
```

This simple update rule embodies the entire philosophy of gradient descent: move in the opposite direction of the gradient by a distance proportional to the learning rate. It's like a hiker who always steps in the steepest downward direction.

## Understanding Batch Processing: From One to Many

Now, here's where things get interesting. The implementation above assumes we've already computed gradients, but how we compute them makes a dramatic difference. Let's examine three approaches:

### Batch Gradient Descent: The Careful Planner

```python
# Process ALL training examples at once
a, caches = forward_propagation(X, parameters)  # X contains all m examples
cost_total = compute_cost(a, Y)  # Total cost for all data
grads = backward_propagation(a, caches, parameters)  # Average gradient
parameters = update_parameters(parameters, grads)  # One update per epoch
```

Imagine a **Cartographer_Bot** that surveys the entire landscape before taking a single, well-informed step. It sees all training examples as a collective, computes their average gradient, and moves accordingly.

### Stochastic Gradient Descent: The Reactive Explorer

```python
for j in range(0, m):  # This loop is the key difference!
    # Process ONE example at a time
    a, caches = forward_propagation(X[:,j], parameters)
    cost_total += compute_cost(a, Y[:,j])
    grads = backward_propagation(a, caches, parameters)
    parameters = update_parameters(parameters, grads)  # Update after each example
```

This is like an impulsive explorer who reacts immediately to each piece of information. The crucial `for j in range(0, m)` loop means we update our parameters $m$ times per epoch instead of just once. Each training example gets its own voice, immediately influencing the model's direction.

### Mini-Batch Gradient Descent: The Balanced Strategist

This approach finds the sweet spot between the two extremes. Instead of processing all data or just one example, we process manageable chunks. But first, we need to prepare our data properly.

## Exercise 2: Creating Mini-Batches - The Art of Data Organization

Creating mini-batches involves two crucial steps that work together like a **Data Organizer** object with two methods:

### Step 1: Shuffle - Mixing the Deck

```python
# Why shuffle? Imagine your data is ordered: [dog1, dog2, ..., dog1000, cat1, cat2, ..., cat1000]
# Without shuffling, early batches would only see dogs!

permutation = list(np.random.permutation(m))
shuffled_X = X[:, permutation]  # Apply same shuffle to inputs
shuffled_Y = Y[:, permutation]  # and labels to maintain pairing
```

Shuffling prevents the catastrophic scenario where your model first learns "everything is a dog" and then suddenly switches to "everything is a cat". It ensures each mini-batch is a representative sample of your entire dataset.

### Step 2: Partition - Slicing the Cake

Now comes the partitioning logic. Understanding array slicing is crucial here:

```python
def random_mini_batches(X, Y, mini_batch_size=64, seed=0):
    """
    Creates mini-batches from shuffled data.
    
    Think of this as a Batch Manager organizing data into digestible portions.
    """
    m = X.shape[1]  # number of examples
    mini_batches = []
    
    # After shuffling (already shown above)...
    
    # Calculate how many complete mini-batches we can make
    num_complete_minibatches = math.floor(m / mini_batch_size)
    
    # Create complete mini-batches
    for k in range(0, num_complete_minibatches):
        # The key insight: use k to calculate start and end indices
        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handle the last mini-batch (might be smaller)
    if m % mini_batch_size != 0:
        # From the last complete batch to the end
        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]
        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches
```

The indexing pattern `k * mini_batch_size : (k + 1) * mini_batch_size` creates non-overlapping slices. When $k=0$, we get indices $0:64$. When $k=1$, we get $64:128$, and so on. It's like cutting a long ribbon into equal pieces, with possibly a shorter piece at the end.

## Exercise 3 & 4: Momentum - Adding Memory to Our Steps

### Initializing Velocity: Preparing the Memory Banks

```python
def initialize_velocity(parameters):
    """
    Initialize velocity - think of this as creating a Memory Bank object
    that will store our movement history.
    """
    L = len(parameters) // 2
    v = {}
    
    for l in range(1, L + 1):
        # Start with zero velocity - like a ball at rest
        v["dW" + str(l)] = np.zeros(parameters["W" + str(l)].shape)
        v["db" + str(l)] = np.zeros(parameters["b" + str(l)].shape)
        
    return v
```

Why initialize to zero? Because initially, we have no movement history. It's like a ball sitting still before we give it the first push. The velocity must match the shape of our parameters because we'll be adding them together later.

### Updating with Momentum: Building Up Speed

```python
def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Update parameters using momentum.
    
    Imagine a Physical Ball rolling downhill, accumulating speed.
    """
    L = len(parameters) // 2
    
    for l in range(1, L + 1):
        # Update velocity: combine previous momentum with current gradient
        v["dW" + str(l)] = beta * v["dW" + str(l)] + (1 - beta) * grads["dW" + str(l)]
        v["db" + str(l)] = beta * v["db" + str(l)] + (1 - beta) * grads["db" + str(l)]
        
        # Update parameters using the velocity (not gradient directly!)
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * v["dW" + str(l)]
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * v["db" + str(l)]
    
    return parameters, v
```

A common mistake is to multiply by beta again when updating parameters (like `learning_rate * beta * v["dW"]`). 

> ***Remember: velocity already incorporates the momentum factor. Adding beta again would artificially dampen the movement, like trying to push a rolling ball backward.***

## Exercise 5 & 6: Adam - The Adaptive Navigator

Adam is like having two specialized advisors: one tracking direction (first moment) and another tracking certainty (second moment).

### Initializing Adam: Preparing Two Memory Banks

```python
def initialize_adam(parameters):
    """
    Initialize both v and s - like creating a Navigation System with
    both a Direction Tracker and a Certainty Analyzer.
    """
    L = len(parameters) // 2
    v = {}  # First moment - average direction
    s = {}  # Second moment - average squared magnitude
    
    for l in range(1, L + 1):
        # Both start at zero - no history yet
        v["dW" + str(l)] = np.zeros(parameters["W" + str(l)].shape)
        v["db" + str(l)] = np.zeros(parameters["b" + str(l)].shape)
        s["dW" + str(l)] = np.zeros(parameters["W" + str(l)].shape)
        s["db" + str(l)] = np.zeros(parameters["b" + str(l)].shape)
        
    return v, s
```

As one learner aptly put it: "preparing the rice bowls" - we need two bowls because Adam tracks two different types of information.

### Updating with Adam: Orchestrating Adaptive Steps

```python
def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,
                                beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    The full Adam update - a sophisticated Navigation Orchestrator at work.
    """
    L = len(parameters) // 2
    v_corrected = {}
    s_corrected = {}
    
    for l in range(1, L + 1):
        # Update biased first moment estimate (direction)
        v["dW" + str(l)] = beta1 * v["dW" + str(l)] + (1 - beta1) * grads["dW" + str(l)]
        v["db" + str(l)] = beta1 * v["db" + str(l)] + (1 - beta1) * grads["db" + str(l)]
        
        # Correct bias in first moment
        v_corrected["dW" + str(l)] = v["dW" + str(l)] / (1 - beta1**t)
        v_corrected["db" + str(l)] = v["db" + str(l)] / (1 - beta1**t)
        
        # Update biased second moment estimate (squared gradients)
        # Note the **2 - this tracks variance!
        s["dW" + str(l)] = beta2 * s["dW" + str(l)] + (1 - beta2) * (grads["dW" + str(l)]**2)
        s["db" + str(l)] = beta2 * s["db" + str(l)] + (1 - beta2) * (grads["db" + str(l)]**2)
        
        # Correct bias in second moment
        s_corrected["dW" + str(l)] = s["dW" + str(l)] / (1 - beta2**t)
        s_corrected["db" + str(l)] = s["db" + str(l)] / (1 - beta2**t)
        
        # Update parameters: adaptive learning rate for each parameter
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * v_corrected["dW" + str(l)] / (np.sqrt(s_corrected["dW" + str(l)]) + epsilon)
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * v_corrected["db" + str(l)] / (np.sqrt(s_corrected["db" + str(l)]) + epsilon)
    
    return parameters, v, s, v_corrected, s_corrected
```

The magic of Adam lies in the final update step: dividing by `sqrt(s_corrected)`. Parameters with high variance (uncertain gradients) get smaller updates, while parameters with low variance (consistent gradients) get larger updates. It's like having an intelligent navigator that takes confident steps in reliable directions but treads carefully in uncertain terrain.

## Exercise 7 & 8: Learning Rate Decay - The Art of Slowing Down

### Simple Decay: Gradual Deceleration

```python
def update_lr(learning_rate0, epoch_num, decay_rate):
    """
    Implements inverse time decay - like a Timekeeper Bot that gradually
    reduces step size as time passes.
    """
    learning_rate = 1 / (1 + decay_rate * epoch_num) * learning_rate0
    return learning_rate
```

This creates a smooth decay curve. Early in training (small epoch_num), the denominator stays close to 1. As training progresses, the learning rate gradually decreases, never reaching zero but approaching it asymptotically.

### Scheduled Decay: Strategic Deceleration

```python
def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):
    """
    Implements scheduled decay - like a Strategic Scheduler that maintains
    constant speed within intervals, then drops to a new level.
    """
    learning_rate = 1 / (1 + decay_rate * np.floor(epoch_num / time_interval)) * learning_rate0
    return learning_rate
```

The key insight is `np.floor(epoch_num / time_interval)`. This creates a step function:
- Epochs 0-999: `floor(epoch/1000) = 0` â†’ original learning rate
- Epochs 1000-1999: `floor(epoch/1000) = 1` â†’ first reduction
- Epochs 2000-2999: `floor(epoch/1000) = 2` â†’ second reduction

Think of it as a marathon runner who maintains a steady pace for each 5km segment, then adjusts to a slightly slower pace for the next segment, conserving energy for the long haul.

## Putting It All Together: The Complete Training Loop

```python
def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, 
          beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=5000, 
          decay=None, decay_rate=1):
    """
    The complete Training Orchestrator that brings together all our optimization components.
    """
    # Initialize our system
    parameters = initialize_parameters(layers_dims)
    
    # Initialize optimizer-specific components
    if optimizer == "momentum":
        v = initialize_velocity(parameters)  # One memory bank
    elif optimizer == "adam":
        v, s = initialize_adam(parameters)   # Two memory banks
        t = 0  # Adam's time counter
    
    # The main training loop
    for i in range(num_epochs):
        # Create mini-batches for this epoch
        mini_batches = random_mini_batches(X, Y, mini_batch_size, seed=i)
        
        for minibatch in mini_batches:
            # Standard forward/backward propagation
            minibatch_X, minibatch_Y = minibatch
            AL, caches = forward_propagation(minibatch_X, parameters)
            grads = backward_propagation(minibatch_X, minibatch_Y, caches)
            
            # Update parameters based on chosen optimizer
            if optimizer == "gd":
                parameters = update_parameters_with_gd(parameters, grads, learning_rate)
            elif optimizer == "momentum":
                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)
            elif optimizer == "adam":
                t += 1  # Increment Adam counter
                parameters, v, s, _, _ = update_parameters_with_adam(
                    parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon
                )
        
        # Apply learning rate decay if specified
        if decay:
            learning_rate = decay(learning_rate0, i, decay_rate)
    
    return parameters
```

## Key Insights from Practice

Through implementing these optimizers, several crucial insights emerge:

1. **The Power of Vectorization**: Mini-batch gradient descent leverages matrix operations, making it dramatically faster than processing examples one by one. This is why we use batches of 64 or 128 - they fit efficiently in modern hardware.

2. **Memory vs. Reactivity Trade-off**: Momentum and Adam sacrifice memory (storing v and s) for better convergence. It's like choosing between a reactive but erratic explorer and a thoughtful navigator with a map.

3. **Adaptive Learning**: Adam's per-parameter learning rates solve a fundamental problem - not all parameters need the same step size. Dense layers might need different treatment than convolutional layers.

4. **The Importance of Decay**: Without learning rate decay, optimizers can oscillate around minima forever. Decay ensures we eventually settle into a good solution.

## From Theory to Mastery

Remember our initial question about why some networks converge beautifully while others oscillate wildly? Through these implementations, we've discovered it's not magic - it's the careful orchestration of several key ideas:

- **Data organization** (shuffling and mini-batches)
- **Movement strategy** (gradient descent, momentum, or adaptive methods)
- **Speed control** (learning rate and its decay)

Each optimizer we've implemented represents a different philosophy of navigation. Gradient descent is the purist - always stepping directly downhill. Momentum adds physics - remembering where we've been going. Adam adds intelligence - adapting to the terrain under our feet.

As you experiment with these implementations on real problems, you'll develop an intuition for which optimizer suits which situation. Sometimes the simple approach wins. Sometimes you need the full sophistication of Adam. The key is understanding not just how to implement them, but why each design choice matters.

## Summary

Armed with these implementations, you're ready to tackle real optimization challenges. Try different optimizers on the same problem. Watch how they behave. Notice when momentum helps and when it doesn't. Observe how Adam's adaptivity shines on complex problems but might be overkill on simple ones.

> ***Remember: these aren't just algorithms - they're different ways of thinking about the fundamental problem of learning from data. Learn them, and you learn one of the core skills of deep learning.***