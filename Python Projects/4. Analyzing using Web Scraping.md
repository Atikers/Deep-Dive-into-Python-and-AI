# Analyzing Stock Performance and Building a Dashboard, Visualizing using a Web Scraping
## 1) Scenario
This time, I will use web scraping to gather financial data, analyze stock performance, and build a dashboard.    
The focus will be on collecting historical share prices and revenue data for ***Microsoft***.    
By leveraging Python's web scraping libraries, I aim to visualize this data interactively to identify trends and provide insights for investment strategies.

## 2) Setup
```python
!pip install yfinance
!pip install pandas            # For data manipulation and analysis
!pip install requests          # For making HTTP requests to fetch web pages
!pip install nbformat          # For Jupyter Notebook file handling (if needed)
!pip install bs4               # For parsing HTML and XML documents
!pip install lxml              # For parsing XML and HTML (used by BeautifulSoup)
!pip install plotly            # For interactive data visualization
!pip install selenium          # For web scraping, especially for dynamically loaded content
!pip install webdriver-manager # For automatically downloading and managing WebDriver binaries for Selenium
```
```python
import os                                                    # For operating system interactions, such as file and directory operations
import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import plotly.graph_objects as go                            # For creating interactive plots and visualizations
from plotly.subplots import make_subplots                    # For creating subplots in Plotly
from selenium import webdriver                               # For automating web browser interactions
from selenium.webdriver.chrome.service import Service        # For managing the ChromeDriver service
from webdriver_manager.chrome import ChromeDriverManager     # For automatically downloading and managing WebDriver binaries for Selenium
import time                                                  # For adding delays in the script, useful for waiting for page loads
```

I will Set up **ChromeDriver** service using webdriver-manager
```python
service = Service(ChromeDriverManager().install())                # For Automatically downloads and sets up the ChromeDriver binary
```
```python
options = webdriver.ChromeOptions()            # For Configuring Chrome options for the WebDriver
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(service=service, options=options)        # For Initializing the Chrome WebDriver with the specified service and options
```
In this project, I define the function `make_graph`. It takes a dataframe with **stock data** (**Date** and **Close** columns), a dataframe with **revenue data** (**Date** and **Revenue** columns), and the name of the stock.
```python
def make_graph(stock_data, revenue_data, stock):
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=("Historical Share Price", "Historical Revenue"), vertical_spacing = .3)

    stock_data['Date'] = pd.to_datetime(stock_data['Date'])        # For Converting the 'Date' columns to datetime
    revenue_data['Date'] = pd.to_datetime(revenue_data['Date'])
    
    stock_data_specific = stock_data[stock_data['Date'] <= '2024-03-31']        # For Filtering data up to the specified dates
    revenue_data_specific = revenue_data[revenue_data['Date'] <= '2024-03-31']
    
    # For Adding traces for stock prices and revenue
    fig.add_trace(go.Scatter(x=stock_data_specific['Date'], y=stock_data_specific['Close'].astype("float"), name="Share Price"), row=1, col=1)
    fig.add_trace(go.Scatter(x=revenue_data_specific['Date'], y=revenue_data_specific['Revenue'].astype("float"), name="Revenue"), row=2, col=1)
    
    fig.update_xaxes(title_text="Date", row=1, col=1)        # For Updating x-axes and y-axes titles
    fig.update_xaxes(title_text="Date", row=2, col=1)
    fig.update_yaxes(title_text="Price ($US)", row=1, col=1)
    fig.update_yaxes(title_text="Revenue ($US Millions)", row=2, col=1)
    
    fig.update_layout(showlegend=True,        # For Updating layout
                      height=900,
                      title=stock,
                      xaxis_rangeslider_visible=True)
    
    fig.show()        # For Showing the figure
```

## 3) Steps
### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) Step 1 : Using yfinance to Extract Stock Data    
```python
Microsoft = yf.Ticker("MSFT")        # For Using yfinance to Extract Stock Data

Microsoft_data = Microsoft.history(period="max")        # For Extracting stock information and save it in a dataframe named Microsoft_data   

Microsoft_data.reset_index(inplace=True)        # For Resetting the index
print(Microsoft_data.head())
```
Then, I get the following :    
![image1](https://github.com/Atikers/Images/blob/main/Project%20%234%20-%20image(1).jpg)


### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) Step 2 : Using Web Scraping to Extract Microsoft Revenue Data
```python
url = "https://www.macrotrends.net/stocks/charts/MSFT/microsoft/revenue"        # For Fetching the web page
driver.get(url)
```
```python
time.sleep(5)        # For Waiting for the page to fully load
```
```python
html = driver.page_source        # For Extracting page source and parse with BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')
```
```python
driver.quit()        # For Closing the WebDriver
```
```python
current_directory = os.getcwd()        # For Saving the HTML content to a file
file_path = os.path.join(current_directory, "saved_page.html")
with open(file_path, 'w', encoding='utf-8') as file:
    file.write(str(soup))            # For Writing the string representation of the BeautifulSoup object to the file
```
```python
with open(file_path, 'r', encoding='utf-8') as file:        # For Reading the saved HTML file
    html_content = file.read()
```
```python
soup = BeautifulSoup(html_content, 'html.parser')        # For Parsing the HTML using BeautifulSoup
```
```python
table = soup.find('table', {'class': 'historical_data_table'})        # For Finding the table containing the financial data
```
```python
data = []        # For Initializing a list to store revenue data

```
```python
# For Looping through each row in the table
for row in table.find_all('tr')[1:]:  # For Skipping the header row
    cols = row.find_all('td')
    if len(cols) > 1:
        date = cols[0].text.strip()
        revenue = cols[1].text.strip().replace('$', '').replace(',', '')
        data.append([date, revenue])
```
```python
columns = ['Date', 'Revenue']        # For Converting the data into a pandas DataFrame
Microsoft_revenue = pd.DataFrame(data, columns=columns)
```
```python
Microsoft_revenue['Date'] = pd.to_datetime(Microsoft_revenue['Date'])        # For Converting 'Date' column to datetime format
```
```python
Microsoft_revenue['Revenue'] = Microsoft_revenue['Revenue'].astype(float)        # For Converting 'Revenue' column to float
```
```python
print(Microsoft_revenue.head())        # For Printing the DataFrame to verify the data
```


### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) Step 3 : Plotting Microsoft Stock Graph
To graph the Microsoft Stock Data, Using the `make_graph` function(Graph will only show data upto March 2024).

```python
make_graph(Microsoft_data, Microsoft_revenue, 'Microsoft')
```
Then, I get the following :
![image2](https://github.com/Atikers/Images/blob/main/Project%20%234%20-%20image(2).jpg)


## 4) Conclusion
In this project, I successfully utilized Python's web scraping libraries to extract financial data for ***Microsoft***.    

By combining `yfinance` for historical stock prices and `BeautifulSoup` with `selenium` for revenue data, I was able to gather and visualize key financial metrics.    

The `make_graph` function provided a clear, interactive dashboard to identify trends and make informed investment decisions.    
This approach can be extended to other stocks and financial metrics, offering a versatile tool for financial analysis.    

By leveraging these techniques, I can automate the process of data collection and visualization, enabling more efficient and effective financial analysis.  

This project demonstrates the power of combining various Python libraries to achieve comprehensive data analysis and visualization.
