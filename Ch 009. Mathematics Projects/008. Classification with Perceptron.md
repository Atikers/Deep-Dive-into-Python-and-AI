# **Classification with a Single-Layer Perceptron**

In **binary classification**, you have two classes (often labeled 0 or 1). A **single-layer perceptron** with a **sigmoid** activation can handle such tasks by producing probabilities between 0 and 1.

## **Setup and Data**

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
from sklearn.datasets import make_blobs

%matplotlib inline
np.random.seed(3)
```

### **Example Dataset**
We can create or load data $(X, Y)$. Each **column** of `X` is one training example. For instance:

```python
m = 30  # number of samples
X = np.random.randint(0, 2, (2, m))  # shape (2,m)
Y = np.logical_and(X[0]==0, X[1]==1).astype(int).reshape((1,m))

print("X shape:", X.shape)  # (2, m)
print("Y shape:", Y.shape)  # (1, m)
```

**Why columns?** Because matrix calculus often treats each sample as a column vector.

## **Single Perceptron Model**

### **Forward Propagation (Matrix Form)**
A single-layer perceptron does:
1. **Linear part**:  

$$
Z = WX + b
$$

   - `W` has shape `(1, n_x)` for binary classification output.  
   - `X` has shape `(n_x, m)`.  
   - `b` is broadcast to shape `(1, m)`.  
   - So `Z` ends up `(1, m)`, each column is $z^{(i)}$.

2. **Sigmoid**:  

$$
A = \sigma(Z) = \frac{1}{1+e^{-Z}}
$$

- Producing a `(1, m)` matrix $A$, each column a probability $\hat{y}^{(i)}$.

### **Log Loss (Binary Cross-Entropy)**

For labels `Y` (shape `(1,m)`) and predictions `A`, the average cost is:  

$$
\text{cost} = \frac{1}{m}\sum_{i=1}^m [ - y^{(i)}\ln(a^{(i)}) - (1-y^{(i)})\ln(1-a^{(i)}) ]
$$

### **Backward Propagation (Matrix Form)**
We differentiate w.r.t. `W` and `b`. Define:  

$$
dZ = A - Y \quad \text{(shape: (1,m))}
$$

Then:

1. Gradient w.r.t. $W$:  

$$
dW = \frac{1}{m} dZ X^T \quad \text{(result shape: (1, n))}
$$

- Why? Each sample contributes $(a^{(i)} - y^{(i)}) \cdot x^{(i)}$. The matrix multiplication $(1,m)\times(m,n_x)$ does the summation over samples in one step.

2. Gradient w.r.t. $b$:  

$$
db = \frac{1}{m}\sum_{i=1}^m (a^{(i)} - y^{(i)})
$$

- In matrix form, thatâ€™s often $(1,m)$ multiplied by a column of ones shape $(m,1)$.  

### **Parameter Update**  

$$
W \leftarrow W - \alpha dW, \quad b \leftarrow b - \alpha db.
$$

## **Implementation**

```python
def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))

def forward_propagation(X, params):
    W, b = params["W"], params["b"]
    Z = W @ X + b  # shape (1,m)
    A = sigmoid(Z) # shape (1,m)
    return A

def compute_cost(A, Y):
    m = Y.shape[1]
    # -1/m SUM [y log(a) + (1-y) log(1-a)]
    logprobs = -np.multiply(Y, np.log(A)) - np.multiply(1-Y, np.log(1-A))
    cost = (1/m)*np.sum(logprobs)
    return cost

def backward_propagation(A, X, Y):
    m = X.shape[1]
    dZ = A - Y               # shape (1,m)
    dW = (1/m)*(dZ @ X.T)    # shape (1,n_x)
    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)  # shape (1,1)
    return {"dW": dW, "db": db}

def update_parameters(params, grads, lr):
    params["W"] -= lr * grads["dW"]
    params["b"] -= lr * grads["db"]
    return params

def nn_model(X, Y, num_iterations=50, learning_rate=1.2, print_cost=False):
    # Initialize
    n_x = X.shape[0]
    params = {"W": np.random.randn(1,n_x)*0.01, "b": np.zeros((1,1))}
    
    for i in range(num_iterations):
        # Forward
        A = forward_propagation(X, params)
        cost = compute_cost(A, Y)
        # Backward
        grads = backward_propagation(A, X, Y)
        # Update
        params = update_parameters(params, grads, learning_rate)
        
        if print_cost:
            print(f"Cost after iteration {i}: {cost:.6f}")
    return params
```

### **Predictions**

After training, compute $\hat{y}=\sigma(z)$. We pick **class=1** if $\hat{y}>0.5$, else **0**.

```python
def predict(X, params):
    A = forward_propagation(X, params)
    return (A>0.5).astype(int)
```

## **Quick Demo**

```python
# Train on small random dataset
m = 30
X = np.random.randint(0,2,(2,m))
Y = np.logical_and(X[0]==0, X[1]==1).astype(int).reshape((1,m))

params = nn_model(X, Y, num_iterations=40, learning_rate=1.2, print_cost=True)
print("Trained W:", params["W"])
print("Trained b:", params["b"])

# Test predictions
X_test = np.array([[0,1],[1,0]]) # shape(2,2)
print("Predictions:", predict(X_test, params))
```

## **Larger, More Complex Data**

```python
# Generate 2D blobs of points
n_samples = 1000
samples, labels = make_blobs(n_samples=n_samples,
                             centers=([2.5,3],[6.7,7.9]),
                             cluster_std=1.4,
                             random_state=0)
X_large = samples.T       # shape(2,1000)
Y_large = labels.reshape((1,n_samples))

params_large = nn_model(X_large, Y_large, num_iterations=100, learning_rate=1.2)
```

**Optionally** visualize the boundary or compute accuracy metrics.

# **Key Takeaways**

1. **Matrix Form**: By storing data samples in columns, we do all sample updates in **one** matrix multiplication step. 
2. **Forward Prop**:  

$$
Z = W X + b, \quad A = \sigma(Z)
$$

3. **Log Loss**: Encourages correct classification with probabilistic interpretation.  
4. **Backward Prop**:  

$$
dW = \tfrac{1}{m}(A-Y)X^T,\quad db=\tfrac{1}{m}\sum(A-Y)
$$

5. **Gradient Descent**: Repeats forward + backward + update until convergence.

This single perceptron approach demonstrates the fundamentals used in **deeper** networks, just with more layers and parameters. Experiment with learning rate or iteration count to see how it affects convergence.