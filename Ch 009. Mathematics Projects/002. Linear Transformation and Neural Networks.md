# **Linear Transformations and Neural Networks**

This project aims to show you how fundamental concepts in linear algebra underpin both geometric transformations in the plane and key computations in a basic neural network. By exploring matrix-based transformations and building a single-layer neural network, we see firsthand why linear algebra is at the heart of many areas in AI and beyond.

## **1. Foundations of Linear Transformations**

### **1.1 What Is a Transformation?**

In geometry or linear algebra, a **transformation** is a function that maps one vector space to another while preserving the underlying structure. For instance, if we have a transformation $T$ that takes vectors in $\mathbb{R}^2$ and produces vectors in $\mathbb{R}^3$, we might write:  

$$
T : \mathbb{R}^2 \to \mathbb{R}^3
$$

If $v \in \mathbb{R}^2$ is an input vector, $T(v) \in \mathbb{R}^3$ is its image under $T$.

Below is an example function in Python that implements a transformation $T : \mathbb{R}^2 \to \mathbb{R}^3$. It maps:  

$$
v = \begin{bmatrix} v_1 \\
v_2 \end{bmatrix} \quad\mapsto\quad
\begin{bmatrix} 3v_1 \\
0 \\
-2v_2 \end{bmatrix}
$$

```python
import numpy as np

def T(v):
    w = np.zeros((3,1))
    w[0,0] = 3*v[0,0]
    w[2,0] = -2*v[1,0]
    return w

v = np.array([[3], [5]])
w = T(v)
print("Original vector:\n", v, "\n\nResult of the transformation:\n", w)
```

### **1.2 When Is a Transformation Linear?**

A transformation $T$ is called **linear** if it satisfies two properties for any scalar $k$ and any vectors $u, v$:

1. $T(kv) = k T(v)$  
2. $T(u + v) = T(u) + T(v)$

Using the earlier example  

$$
T(v) = \begin{bmatrix}3v_1 \\
0 \\
-2v_2 \end{bmatrix}
$$

you can verify linearity by substituting and seeing that scalar multiples and vector sums behave consistently.

```python
u = np.array([[1], [-2]])
v = np.array([[2], [4]])

k = 7

print("T(k*v):\n", T(k*v), "\n k*T(v):\n", k*T(v), "\n\n")
print("T(u+v):\n", T(u+v), "\n\n T(u)+T(v):\n", T(u)+T(v))
```

### **1.3 Matrix Multiplication as a Transformation**

Often, a linear transformation from $\mathbb{R}^m$ to $\mathbb{R}^n$ can be expressed as matrix multiplication $Av$ for some $n \times m$ matrix $A$ and $m \times 1$ vector $v$. For instance, if  

$$
L(v) = \begin{bmatrix} 3v_1 \\
0 \\
-2v_2 \end{bmatrix}
$$

we can discover $A$ by matching it to $Av$. In that specific case,  

$$
A = \begin{bmatrix} 3 & 0 \\
0 & 0 \\
0 & -2 \end{bmatrix}
$$

Thus $L(v) = A v$ yields the same result.

```python
def L(v):
    A = np.array([[3,0], [0,0], [0,-2]])
    print("Transformation matrix:\n", A, "\n")
    w = A @ v
    
    return w

v = np.array([[3], [5]])
w = L(v)

print("Original vector:\n", v, "\n\n Result of the transformation:\n", w)
```

## **2. Standard 2D Transformations**

In the plane ($\mathbb{R}^2$), some of the most common linear transformations are scaling, shearing, reflection, and rotation. They each have a characteristic $2 \times 2$ matrix that reveals how they act on every point in the plane.

For visual exploration, it’s often helpful to see how each transformation affects:
1. The **standard basis vectors**  

$$
e_1 = \begin{bmatrix}1 \\
0\end{bmatrix}
$$

and  

$$
e_2 = \begin{bmatrix}0 \\
1\end{bmatrix}
$$

2. An image or set of points.

Below is an example “leaf” image stored as a $2 \times 329076$ array, where each column is a point $(x,y)$.

```python
import matplotlib.pyplot as plt

img = np.loadtxt('data/image.txt')
plt.scatter(img[0], img[1], s=0.001, color='black')
plt.show()
```

### **2.1 Horizontal Scaling (Dilation)**

Suppose we want to stretch the $x$-coordinates by a factor of 2 but keep the $y$-coordinates the same. One way to define this is:
- $e_1$ (the unit vector along $x$) maps to $(2,0)$,
- $e_2$ (the unit vector along $y$) remains $(0,1)$.

Hence, the matrix is  

$$
A = \begin{bmatrix} 2 & 0 \\
0 & 1 \end{bmatrix}
$$

```python
def T_hscaling(v):
    A = np.array([[2,0],[0,1]])
    return A @ v

def transform_vectors(T, v1, v2):
    V = np.hstack((v1, v2))
    W = T(V)
    return W

e1 = np.array([[1],[0]])
e2 = np.array([[0],[1]])
res = transform_vectors(T_hscaling, e1, e2)
print("Transformed standard basis:\n", res)
```

To visualize:

```python
# Suppose we have a helper function plot_transformation(T, e1, e2)
import utils
utils.plot_transformation(T_hscaling, e1, e2)
```

And applying this to the “leaf”:

```python
plt.scatter(img[0], img[1], s=0.001, color='black')
scaled_img = T_hscaling(img)
plt.scatter(scaled_img[0], scaled_img[1], s=0.001, color='grey')
plt.show()
```

### **2.2 Reflection About the y-Axis**

Reflecting over the vertical axis sends each $(x,y)$ to $(-x,y)$. The matrix is:  

$$
A = \begin{bmatrix} -1 & 0 \\
0 & 1 \end{bmatrix}
$$

```python
def T_reflection_yaxis(v):
    A = np.array([[-1,0],[0,1]])
    return A @ v

utils.plot_transformation(T_reflection_yaxis, e1, e2)

plt.scatter(img[0], img[1], s=0.001, color='black')
reflected = T_reflection_yaxis(img)
plt.scatter(reflected[0], reflected[1], s=0.001, color='grey')
plt.show()
```

### **2.3 Stretching by a Scalar**

A simpler transformation is scaling everything by a nonzero scalar $a$. Formally:  

$$
(x,y) \mapsto (ax, ay)
$$

**Implementation**:

```python
def T_stretch(a, v):
    """
    Scales every coordinate in v by the factor a.
    """
    T = np.array([[a,0],[0,a]])
    return T @ v
```

Visual check:

```python
plt.scatter(img[0], img[1], s=0.001, color='black')
stretched = T_stretch(2, img)
plt.scatter(stretched[0], stretched[1], s=0.001, color='grey')
plt.show()
```

### **2.4 Horizontal Shearing**

A horizontal shear by factor $m$ sends $(x,y)$ to $(x + my, y)$. The matrix is:  

$$
\begin{bmatrix} 1 & m \\
0 & 1 \end{bmatrix}
$$

```python
def T_hshear(m, v):
    T = np.array([[1,m],[0,1]])
    return T @ v
```

### **2.5 Rotation**

To rotate by an angle $\theta$ counterclockwise:  

$$
\begin{bmatrix} \cos\theta & -\sin\theta \\
\sin\theta & \cos\theta \end{bmatrix}
$$

```python
def T_rotation(theta, v):
    T = np.array([
        [np.cos(theta), -np.sin(theta)],
        [np.sin(theta),  np.cos(theta)]
    ])
    return T @ v
```

You can combine transformations by **matrix multiplication** of their respective matrices. For instance, rotating by $\theta$ and then stretching by $a$ might be:  

$$
v \mapsto (R \cdot S)v
$$

where $R$ is the rotation matrix and $S$ is the stretching matrix.

## **3. Neural Networks With Two Inputs**

So far, we’ve seen how matrices transform geometrical data. But matrix operations also underpin computations in neural networks.

### **3.1 Linear Regression Context**

Consider a **linear regression** setup with two independent variables $x_1, x_2$ and a single scalar output $y$. A typical linear model is:  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

or in vector form:  

$$
\hat{y} = W x + b
$$

where  

$$
W = \begin{bmatrix} w_1 \\
w_2 \end{bmatrix} \quad\quad\quad x = \begin{bmatrix} x_1 \\
x_2 \end{bmatrix}
$$

### **3.2 Network Model With a Single Perceptron**

A single perceptron with two inputs can represent exactly this model:

- **Weights**: $w_1, w_2$  
- **Bias**: $b$  
- **Output**: $z = w_1 x_1 + w_2 x_2 + b$

If you organize multiple training examples $X$ as a $(2 \times m)$ matrix (each column is one $(x_1,x_2)$ pair), and $W$ as a $(1 \times 2)$ array, then multiplying $W \cdot X$ yields a $(1 \times m)$ row of predictions, plus $b$ broadcast across all columns.

### **3.3 Network Parameters**

Let’s store $W$ and $b$ in a small Python dictionary `parameters`, for example:

```python
parameters = {
    "W": np.array([[some_random_initialization, another_random_value]]),
    "b": np.array([[0.]])
}
```

### **3.4 Forward Propagation**

In neural-network terminology, the forward pass computes:  

$$
Z = W X + b, \quad\quad \hat{Y} = Z
$$

Below is a sample function to perform forward propagation:

```python
def forward_propagation(X, parameters):
    W = parameters["W"]
    b = parameters["b"]
    Z = np.dot(W, X) + b
    Y_hat = Z
    return Y_hat
```

### **3.5 Cost Function Overview**

One common cost for linear regression is:  

$$
\mathcal{L}(w,b) = \frac{1}{2m} \sum_{i=1}^m \bigl(\hat{y}^{(i)} - y^{(i)}\bigr)^2
$$

Minimizing this cost helps find the best $w_1, w_2, b$. In practice, we iteratively adjust $w_1, w_2, b$ via gradient-based methods.

### **3.6 Training a Simple Neural Network**

Here’s a skeletal outline for training:
1. **Initialize** $W$ and $b$ (randomly or with small values).
2. For each iteration:
   - **Forward Prop**: Calculate $\hat{Y} = W X + b$.
   - **Compute Cost**: Measure how close $\hat{Y}$ is to $Y$.
   - **Update Parameters**: Adjust $W$ and $b$ to reduce the cost (the “backward propagation” step conceptually involves derivatives, but here you might see it encapsulated in a helper function).

Below is a code snippet showing how you might run multiple iterations (assuming we have a helper function to update parameters):

```python
def nn_model(X, Y, num_iterations=1000, print_cost=False):
    # X: shape (2, m)
    # Y: shape (1, m)
    
    # Initialize W, b
    parameters = {
        "W": np.random.randn(1,2)*0.01,
        "b": np.zeros((1,1))
    }
    
    for i in range(num_iterations):
        Y_hat = forward_propagation(X, parameters)
        cost = np.sum((Y_hat - Y)**2)/(2*X.shape[1])  # example cost
        
        # Suppose we have a helper that updates parameters
        parameters = update_parameters(parameters, Y_hat, X, Y, learning_rate=0.001)
        
        if print_cost and i % 100 == 0:
            print(f"Cost after iteration {i}: {cost:.6f}")
    
    return parameters
```

## **4. Make Predictions**

After training, the final $W$ and $b$ can be used to predict new outputs:  

$$
\hat{y} = W x + b
$$

A simple predictor function might look like:

```python
def predict(X, parameters):
    W = parameters["W"]
    b = parameters["b"]
    Z = np.dot(W, X) + b
    return Z
```

If $X$ has shape $(2, m)$, then the result $\hat{Y}$ will have shape $(1, m)$, giving a predicted value for each of the $m$ examples.

### **Example Usage**

Suppose you have a dataset of $(x_1, x_2, y)$ in a CSV:

```python
import pandas as pd

df = pd.read_csv("data/toy_dataset.csv")
X = df[["x1","x2"]].to_numpy().T  # shape (2, m)
Y = df["y"].to_numpy().reshape(1, -1)  # shape (1, m)

parameters = nn_model(X, Y, num_iterations=5000, print_cost=True)
Y_hat = predict(X, parameters)

# Print some comparison
for i in range(10):
    print(f"(x1, x2) = ({X[0,i]:.2f}, {X[1,i]:.2f}) => Actual: {Y[0,i]:.2f}, Predicted: {Y_hat[0,i]:.2f}")
```

You’ll see the predictions align with your data if the network converged. Congratulations—you’ve integrated 2D transformations and a basic neural net, all powered by matrix operations!

## **Summary**

- **Linear transformations** in 2D can be understood via $2 \times 2$ matrices that scale, reflect, shear, or rotate.
- **Matrix multiplication** is the unifying theme: the same operations that move points around in space also drive the forward pass in a neural network.
- A **single-layer neural network** with two inputs and one perceptron can mirror a linear regression model, computing $Wx + b$.
- **Forward propagation** boils down to matrix multiplication, while the cost function guides parameter updates.

By bridging geometry and neural networks, we see how linear algebra concepts are essential both for visually transforming shapes in the plane and for “transforming” raw inputs into predictions in a learning model!