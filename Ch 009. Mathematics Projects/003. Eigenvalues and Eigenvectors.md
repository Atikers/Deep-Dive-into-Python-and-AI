# **Eigenvalues and Eigenvectors: Two Real-World Examples**

Below are two scenarios illustrating the power of eigenvalues and eigenvectors:

1. **Discrete Dynamical Systems (Webpage Navigation)**
2. **Principal Component Analysis (PCA) for Image Compression**

You’ll see how linear transformations and eigen-decompositions simplify or accelerate tasks like identifying the “most visited” web pages in the long run or compressing a large set of images.

---

## **1. Discrete Dynamical Systems: Navigating Webpages**

### **Transition Matrix**

Consider a small set of $n = 5$ web pages. We define a transition matrix $P \in \mathbb{R}^{5 \times 5}$ whose entries $p_{ij}$ give the probability of navigating to page $j$ from page $i$. For example:

```python
import numpy as np

P = np.array([
    [0,    0.75, 0.35, 0.25, 0.85],
    [0.15, 0,    0.35, 0.25, 0.05],
    [0.15, 0.15, 0,    0.25, 0.05],
    [0.15, 0.05, 0.05, 0,    0.05],
    [0.55, 0.05, 0.25, 0.25, 0   ]
])

# Check columns sum to 1
print("Sum of columns of P:", np.sum(P, axis=0))
```

A **state vector** $X_t$ captures the probability of being on each page at time $t$. If we start on page 4 with certainty:

```python
X0 = np.array([[0],
               [0],
               [0],
               [1],
               [0]])

X1 = P @ X0  # Probability distribution after one step
print("X1 = \n", X1)
```

### **Iterating Over Many Steps**

By applying the matrix repeatedly, we can see where the user ends up after many navigations:

```python
X = X0.copy()
m = 20  # number of steps
for _ in range(m):
    X = P @ X

print("Probability distribution after 20 steps:\n", X)
```

### **Eigenvalues and Long-Run Behavior**

Instead of applying $P$ many times, we note that **Markov matrices** (like $P$, with nonnegative entries and columns summing to 1) have an **eigenvalue** $\lambda = 1$. The corresponding eigenvector (once normalized to sum to 1) gives the *steady-state* or *long-run distribution*.

```python
# Compute eigenvalues/eigenvectors
eigenvals, eigenvecs = np.linalg.eig(P)

print("Eigenvalues of P:\n", eigenvals)
print("\nEigenvectors of P:\n", eigenvecs)
```

Identify which eigenvalue is 1 (within floating-point tolerance), then extract that eigenvector:

```python
idx = np.where(np.isclose(eigenvals, 1, rtol=1e-7))[0][0]
X_inf = eigenvecs[:, idx]
# Normalize so entries sum to 1
X_inf = X_inf / np.sum(X_inf)

print("Long-run probabilities:\n", X_inf)
```

Those probabilities match the distribution you get by multiplying $P$ a large number of times. PageRank and similar algorithms rely on precisely this method to rank pages by “popularity.”

---

## **2. Principal Component Analysis (PCA) for Dimensionality Reduction**

### **2.1 Loading and Flattening an Image Dataset**

Assume we have 55 black-and-white cat images, each $64 \times 64$ pixels. We flatten each image (turn it into one row of 4096 pixels). The combined dataset is thus a $55 \times 4096$ matrix:

```python
import matplotlib.pyplot as plt
import scipy.sparse.linalg

# Suppose we have a function load_images(path) -> list of 2D arrays
import utils  # hypothetical utility with load_images, etc.

imgs = utils.load_images('./data/')  # e.g. 55 cat images
height, width = imgs[0].shape
print(f"Loaded {len(imgs)} images of size {height}x{width}.")

# Flatten each image into one row
imgs_flatten = np.array([img.reshape(-1) for img in imgs])
print("Flattened shape:", imgs_flatten.shape)
```

### **2.2 Centering Data & Covariance Matrix**

**Center** each column (pixel) by subtracting its mean:

```python
def center_data(Y):
    mean_vector = np.mean(Y, axis=0)  # average over each column
    mean_matrix = np.repeat(mean_vector, Y.shape[0])
    mean_matrix = mean_matrix.reshape(Y.shape[0], Y.shape[1], order='F')
    return Y - mean_matrix

X = center_data(imgs_flatten)
```

Then, compute the **covariance matrix**:  

$$
\Sigma = \frac{1}{n - 1} X^\top X
$$

```python
def get_cov_matrix(X):
    n_obs = X.shape[0]
    return (X.T @ X) / (n_obs - 1)

cov_matrix = get_cov_matrix(X)
print("Covariance matrix shape:", cov_matrix.shape)
```

### **2.3 Eigenvalues and Eigenvectors**

We find the principal components by extracting the largest eigenvalues and associated eigenvectors of $\Sigma$. Since $\Sigma$ is large, we may use `scipy.sparse.linalg.eigsh` to compute only the top 55 eigenpairs:

```python
eigenvals, eigenvecs = scipy.sparse.linalg.eigsh(cov_matrix, k=55)
# Sort them in descending order
eigenvals = eigenvals[::-1]
eigenvecs = eigenvecs[:, ::-1]

print("Ten largest eigenvalues:\n", eigenvals[:10])
```

Each eigenvector is a principal component (PC). The eigenvector with the largest eigenvalue captures the direction of greatest variance.

### **2.4 Projecting onto k Principal Components**

To reduce from 4096 pixels to $k$ principal components, pick the top $k$ eigenvectors and multiply:

```python
def perform_PCA(X, eigenvecs, k):
    # columns 0..(k-1) are the k principal components
    V = eigenvecs[:, :k]
    return X @ V

Xred2 = perform_PCA(X, eigenvecs, 2)  # e.g. 2D
print("Xred2 shape:", Xred2.shape)
```

You can then scatter-plot the 2D coordinates and see which images are visually similar (they tend to cluster).

### **2.5 Reconstructing Images**

To reconstruct from $k$ components, multiply back by the top $k$ eigenvectors:

```python
def reconstruct_image(Xred, eigenvecs):
    # The shape of Xred is (n_observations, k)
    # We multiply by eigenvecs[:, :k].T to recover the original dimension
    return Xred @ eigenvecs[:, :Xred.shape[1]].T

Xred10 = perform_PCA(X, eigenvecs, 10)
Xrec10 = reconstruct_image(Xred10, eigenvecs)

# Plot an original vs. reconstructed image
plt.subplot(1,2,1); plt.imshow(imgs[21], cmap='gray'); plt.title("Original")
plt.subplot(1,2,2); plt.imshow(Xrec10[21].reshape(height,width), cmap='gray')
plt.title("Reconstructed (10 PCs)")
```

### **2.6 Explained Variance**

If $\lambda_1, \lambda_2, \dots, \lambda_{55}$ are the eigenvalues, the **explained variance** for the $i$-th principal component is:  

$$
\text{ExplainedVar}(i) = \frac{\lambda_i}{\sum_j \lambda_j}
$$

Often, one picks $k$ so that the **cumulative sum** of explained variance $\sum_{i=1}^k \lambda_i / \sum_j \lambda_j$ exceeds a threshold (e.g., 95%).

---

## **Conclusion**

- **Web Navigation**: Markov transition matrices converge to a steady-state eigenvector, revealing long-run page popularity.  
- **PCA**: Covariance matrices let us find directions of maximal variance (principal components). Projecting onto the top $k$ eigenvectors compresses data, often with minimal loss.

Eigenvalues and eigenvectors thus offer two powerful glimpses into real-world problems—underpinning iterative processes (like repeated clicks on web pages) and compressing high-dimensional data (like sets of images).