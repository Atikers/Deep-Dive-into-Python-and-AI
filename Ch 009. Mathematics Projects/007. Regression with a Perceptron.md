# **Regression with a Perceptron**

In previous assignments, we built a **gradient descent** solution for linear regression (TV vs. Sales). Here, we’ll show how it directly maps to a **single-layer perceptron**—the fundamental building block of neural networks. We’ll first do **one feature**, then expand to **two features** with minimal code changes.

## **Simple Linear Regression (One Feature)**

### **Conceptual Overview**

- **Goal**: Predict Sales (output) from TV marketing budget (input).
- **Model**: $\hat{y} = w x + b$  
  - $w$ = weight, controlling how strongly TV spending affects Sales.  
  - $b$ = bias (an offset).

We treat $\hat{y} = w x + b$ as a **perceptron** with:
- 1 input node - $x$,
- 1 output node - $\hat{y}$,
- 2 parameters - $w,b$.

### **Dataset and Normalization**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load data from CSV (TV vs Sales).
path = "data/tvmarketing.csv"
adv = pd.read_csv(path)

# For stable gradient descent, normalize both columns:
adv_norm = (adv - adv.mean()) / adv.std()

# Convert to NumPy arrays, shape (1,m) for convenience:
X_norm = adv_norm['TV'].values.reshape(1, -1)    # shape: (1, m)
Y_norm = adv_norm['Sales'].values.reshape(1, -1) # shape: (1, m)
m = X_norm.shape[1]  # number of samples
```

**Why normalize?**  
- Different scales (TV in hundreds vs. Sales in tens) can make gradient descent steps inconsistent or too large/small. Normalization speeds convergence.

### **Parameters and Forward Propagation**

```python
# Initialize w and b (small random or zeros).
w = np.random.randn(1,1)*0.01
b = np.zeros((1,1))

def forward_propagation(X, w, b):
    """
    X: (1,m) input data
    w: (1,1) weight
    b: (1,1) bias
    Returns predicted values (1,m) from single perceptron: y_hat = w*X + b
    """
    return w @ X + b  # matrix multiply & broadcast
```

**Key Idea**: The perceptron’s output is just `w * X + b` for each data point.

### **Cost Function (Mean Squared Error)**

```python
def compute_cost(Y_hat, Y):
    """
    Y_hat: (1,m) predicted
    Y    : (1,m) actual
    Returns MSE cost: sum((Y_hat - Y)^2)/(2*m)
    """
    return np.sum((Y_hat - Y)**2) / (2*m)
```

**Why divide by $2m$?**  
- $\frac{1}{2}$ helps derivatives, and $\frac{1}{m}$ averages across all samples.

### **Backward Propagation**

```python
def backward_propagation(Y_hat, X, Y):
    """
    Compute partial derivatives (gradients) wrt w,b.
    Y_hat: (1,m) predicted
    X    : (1,m) inputs
    Y    : (1,m) actual

    dZ = (Y_hat - Y) is the error vector for each sample.
    """
    dZ = Y_hat - Y   # shape (1,m)
    dw = (dZ @ X.T) / m  # scalar gradient wrt w
    db = np.sum(dZ, axis=1, keepdims=True) / m
    return dw, db
```

**Chain rule**: $\frac{\partial L}{\partial w} = \frac{1}{m}\sum (y_{hat}^{(i)} - y^{(i)}) x^{(i)}$.

### **Gradient Descent Update**

```python
def update_parameters(w, b, dw, db, lr=1.2):
    """
    w <- w - lr * dw
    b <- b - lr * db
    """
    w = w - lr*dw
    b = b - lr*db
    return w, b
```

**Learning Rate**: `lr=1.2` controls step size. 
- Too large? Might overshoot.
- Too small? Takes many iterations to converge.

### **Training Loop**

```python
for i in range(30):  # e.g. 30 epochs
    Y_hat = forward_propagation(X_norm, w, b)
    cost = compute_cost(Y_hat, Y_norm)
    dw, db = backward_propagation(Y_hat, X_norm, Y_norm)
    w, b = update_parameters(w, b, dw, db, lr=1.2)
    # print(f"Iter {i}, Cost={cost:.4f}")
```

As iterations progress, `(w, b)` adjust to reduce cost.

### **Making Predictions and Plotting**

```python
X_pred = np.array([50, 120, 280])
# Normalize these new X values with the same stats from adv
X_pred_norm = (X_pred - adv['TV'].mean()) / adv['TV'].std()

# Forward pass on X_pred_norm
Y_pred_norm = w @ X_pred_norm.reshape(1, -1) + b

# Denormalize predictions
Y_pred = Y_pred_norm * adv['Sales'].std() + adv['Sales'].mean()
print("Predicted Sales (for TV budgets 50,120,280):", Y_pred.flatten())

# Plot line + data
plt.scatter(adv["TV"], adv["Sales"], color="black")
plt.xlabel("TV Budget")
plt.ylabel("Sales")
X_line = np.linspace(adv["TV"].min(), adv["TV"].max()*1.1, 200)
X_line_norm = (X_line - adv["TV"].mean()) / adv["TV"].std()
Y_line_norm = w @ X_line_norm.reshape(1, -1) + b
Y_line = Y_line_norm * adv["Sales"].std() + adv["Sales"].mean()
plt.plot(X_line, Y_line[0], color="red")
plt.show()
```

## **Multiple Linear Regression (Two Features)**

### **Extending the Perceptron**

Now we want to predict house prices from, say:
- $x_1$: House size (sq ft),
- $x_2$: Overall quality rating.

A single perceptron has 2 inputs. The formula:  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

In vector form, `W` is `(1,2)`, `X` is `(2,m)`. The code is nearly identical—**only shapes** change.

### **Data Setup**

```python
df = pd.read_csv("data/house_prices_train.csv")
X_multi = df[['GrLivArea','OverallQual']]
Y_multi = df['SalePrice']

# Normalizing columns
X_multi_norm = (X_multi - X_multi.mean()) / X_multi.std()
Y_multi_norm = (Y_multi - Y_multi.mean()) / Y_multi.std()

# Convert to arrays with shapes: X->(2,m), Y->(1,m)
X_multi_norm = X_multi_norm.values.T
Y_multi_norm = Y_multi_norm.values.reshape(1, -1)
m2 = X_multi_norm.shape[1]
```

### **Same Forward/Backward/Update**

```python
# Initialize W=(1,2) and b=(1,1)
W = np.random.randn(1,2)*0.01
b = np.zeros((1,1))

for i in range(100):  # more iterations
    # forward
    Y_hat_multi = W @ X_multi_norm + b
    cost = np.sum((Y_hat_multi - Y_multi_norm)**2)/(2*m2)
    # backward
    dZ_multi = Y_hat_multi - Y_multi_norm  # shape: (1,m2)
    dW_multi = (dZ_multi @ X_multi_norm.T)/m2  # (1,2)
    db_multi = np.sum(dZ_multi, axis=1, keepdims=True)/m2
    # update
    W -= 1.0*dW_multi
    b -= 1.0*db_multi
    # print(f"Iter {i}, cost={cost:.4f}")
```

### **Predictions**

```python
X_test = np.array([[1710,7],[1200,6],[2200,8]]).T
# Normalize new data
X_test_norm = (X_test - X_multi.mean(axis=1,keepdims=True))/X_multi.std(axis=1,keepdims=True)
Y_test_hat_norm = W @ X_test_norm + b
Y_test_hat = Y_test_hat_norm*Y_multi.std() + Y_multi.mean()

print("Predicted house prices:", Y_test_hat.flatten())
```

**Notice**: We did **not** change the forward/backward logic—just shapes. That’s the power of matrix-based perceptrons.

## **Key Takeaways**

1. **Perceptrons for Regression**  
   - For 1 feature: $\hat{y} = w x + b$.  
   - For 2 features: $\hat{y} = w_1 x_1 + w_2 x_2 + b$.  
     - Or in vector form: $\hat{y} = W X + b$.

2. **Forward + Backward = Training**  
   - **Forward** computes predictions.  
   - **Backward** uses partial derivatives to see how changes in $w,b$ reduce errors.  
   - **Update** with gradient descent.

3. **Minimal Code Changes** for More Inputs  
   - Only `W` shape and `X` shape differ.  
   - The principle—“compute a linear combination and adjust with gradients”—stays the same.

4. **Normalization**  
   - Often crucial for stable training.  
   - Means and std dev for each column keep gradient steps consistent.