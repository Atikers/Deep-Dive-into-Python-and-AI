# Building a Two-Layer Neural Network from Scratch

## Introduction

In this project, you will implement a complete neural network with two layers to solve a non-linear classification problem. Think of this neural network as a sophisticated decision-maker, similar to how your brain processes information - taking in raw data, passing it through various processing stages, and producing a final decision.

By the end of this project, you will be able to:
- Understand the architecture of a neural network with two layers
- Implement forward propagation using matrix multiplication
- Perform backward propagation to compute gradients
- Train your neural network model on real data
- Visualize the decision boundary of your trained model

## Understanding the Problem

Imagine you're trying to separate blue and red marbles on a table, but they're arranged in such a way that no single straight line can separate them perfectly. This is exactly the situation with many real classification problems - they require more complex boundaries than a simple straight line.

Consider this example:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
from sklearn.datasets import make_blobs

# Set a seed for consistent results
np.random.seed(3)

# Create a simple visualization of non-linearly separable data
fig, ax = plt.subplots()
xmin, xmax = -0.2, 1.4
x_line = np.arange(xmin, xmax, 0.1)

# Data points from two classes
ax.scatter(0, 0, color="r")
ax.scatter(0, 1, color="b")
ax.scatter(1, 0, color="b")
ax.scatter(1, 1, color="r")
ax.set_xlim([xmin, xmax])
ax.set_ylim([-0.1, 1.1])
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')

# Example of lines that could be used as decision boundaries
ax.plot(x_line, -1 * x_line + 1.5, color="black")
ax.plot(x_line, -1 * x_line + 0.5, color="black")
plt.plot()
```

This pattern is similar to an "XOR" (exclusive OR) problem - where you want one result if both inputs are the same (both 0 or both 1) and a different result if inputs are different.

Think of the house-buying example: you might want a big old house (charm and space) or a small new house (modern but compact), but not a small old house (too cramped and outdated) or a big new house (too expensive). No single rule about size or age would work - you need multiple rules working together.

## Neural Network with Two Layers: Theory

### Architecture Overview

Imagine a neural network as a chain of command in a company:

- **Input Layer**: Like the front-desk employees who receive initial information (your data features)
- **Hidden Layer**: Middle managers who process the information, combining it in useful ways
- **Output Layer**: The CEO who makes the final decision

In our implementation:
- Input layer has 2 nodes (two features, like coordinates on a map)
- Hidden layer has 2 nodes (like having two different specialists analyzing the data)
- Output layer has 1 node (making the final yes/no decision)

### Mathematical Formulation

Let's break down what happens as information flows through our network:

**Hidden Layer:**
The information passes from the input to the hidden layer, where it's transformed. Think of this as each middle manager (hidden node) weighing different aspects of the information differently and then deciding how strongly to react.  

$$
z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]}
$$  

$$
a^{[1](i)} = \sigma(z^{[1](i)})
$$

**Output Layer:**
The processed information from the middle managers then goes to the CEO for a final decision.  

$$
z^{[2](i)} = W^{[2]} a^{[1](i)} + b^{[2]}
$$  

$$
a^{[2](i)} = \sigma(z^{[2](i)})
$$

The weights - $W$ - are like the importance each manager places on different pieces of information. The biases - $b$ - are like their default tendencies or predispositions. The sigmoid function - $\sigma$ - acts like a gatekeeper, determining how strongly to "activate" based on the weighted input.

## Implementation

Let's build our neural network step by step, like constructing a machine with various components.

### Preparing the Dataset

First, we need to gather our training examples - the patterns our network will learn from:

```python
# Generate a dataset with 2000 samples and 4 centers
m = 2000
samples, labels = make_blobs(n_samples=m, 
                             centers=([2.5, 3], [6.7, 7.9], [2.1, 7.9], [7.4, 2.8]), 
                             cluster_std=1.1,
                             random_state=0)
labels[(labels == 0) | (labels == 1)] = 1
labels[(labels == 2) | (labels == 3)] = 0
X = np.transpose(samples)
Y = labels.reshape((1, m))

# Visualize the dataset
plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']))
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Classification Dataset')
plt.show()

print('The shape of X is: ' + str(X.shape))
print('The shape of Y is: ' + str(Y.shape))
print('I have m = %d training examples!' % (m))
```

### Defining the Sigmoid Activation Function

Think of the sigmoid function as a switch that gradually turns on when input exceeds a threshold. Unlike a light switch (which is either on or off), sigmoid is like a dimmer switch that can be partially activated:

```python
def sigmoid(z):
    """
    Implements the sigmoid activation function
    
    Arguments:
    z -- scalar or numpy array
    
    Returns:
    sigmoid(z)
    """
    return 1 / (1 + np.exp(-z))
```

For very negative inputs, sigmoid outputs values close to 0. For very positive inputs, it outputs values close to 1. Around zero, it transitions smoothly between these extremes.

### Neural Network Structure Setup

Let's define the dimensions of our network - how many "employees" we need at each level:

```python
def layer_sizes(X, Y):
    """
    Determines the sizes of the layers in our network
    
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    """
    n_x = X.shape[0]  # Size of input layer - the number of features
    n_h = 2           # Size of hidden layer - our "middle managers"
    n_y = Y.shape[0]  # Size of output layer - the final decision maker
    
    return (n_x, n_h, n_y)
```

### Initializing Parameters

Like a new employee who hasn't formed strong opinions yet, we'll start our network with small random weights. This is like giving them a "blank slate" but with slight tendencies to break symmetry:

```python
def initialize_parameters(n_x, n_h, n_y):
    """
    Initializes the neural network parameters
    
    Arguments:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    parameters -- python dictionary containing the parameters
    """
    # Small random values break symmetry, like giving each employee
    # slightly different starting perspectives
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))  # Biases start at zero, like having no initial preference
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters
```

### Forward Propagation

Forward propagation is like information flowing down a river through multiple processing stations. Each station (layer) transforms the information based on its current configuration (weights and biases):

```python
def forward_propagation(X, parameters):
    """
    Implements the forward propagation algorithm
    
    Arguments:
    X -- input data of shape (n_x, m)
    parameters -- python dictionary containing the parameters
    
    Returns:
    A2 -- the sigmoid output of the second activation
    cache -- dictionary containing Z1, A1, Z2, A2
    """
    # Retrieve parameters
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Forward propagation calculations
    # First transformation - like each middle manager forming their initial opinion
    Z1 = np.dot(W1, X) + b1      # Shape: (n_h, m)
    # First activation - how strongly each manager reacts to their weighted input
    A1 = sigmoid(Z1)             # Shape: (n_h, m)
    # Second transformation - the CEO weighing the managers' opinions
    Z2 = np.dot(W2, A1) + b2     # Shape: (n_y, m)
    # Final activation - the CEO's final decision (a probability between 0 and 1)
    A2 = sigmoid(Z2)             # Shape: (n_y, m)
    
    # Store all intermediate values for backward propagation
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache
```

### Computing the Cost

The cost function measures how wrong our network is, like a performance metric for the entire team. Lower cost means better predictions:

```python
def compute_cost(A2, Y):
    """
    Computes the cross-entropy cost
    
    Arguments:
    A2 -- The sigmoid output of the second activation, shape (1, m)
    Y -- "true" labels vector, shape (1, m)
    
    Returns:
    cost -- cross-entropy cost
    """
    m = Y.shape[1]  # Number of examples
    
    # Compute the cross-entropy cost
    # This is like measuring the penalty for wrong predictions
    # If Y=1 and A2≈0, or Y=0 and A2≈1, the penalty is very high
    logprobs = Y * np.log(A2) + (1 - Y) * np.log(1 - A2)
    cost = -1/m * np.sum(logprobs)
    
    return cost
```

### Backward Propagation

Backward propagation is like a performance review that works backward through the organization. It identifies which parts of the system need adjustment to improve the overall performance:

```python
def backward_propagation(parameters, cache, X, Y):
    """
    Implements the backward propagation algorithm
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- python dictionary containing Z1, A1, Z2, A2
    X -- input data of shape (n_x, m)
    Y -- "true" labels vector of shape (n_y, m)
    
    Returns:
    grads -- python dictionary containing gradients with respect to parameters
    """
    m = X.shape[1]  # Number of examples
    
    # Retrieve parameters
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    
    # Retrieve cached values
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    # Backward propagation: calculate gradients
    # First, calculate error at the output layer
    # This is like measuring how wrong the CEO's decisions were
    dZ2 = A2 - Y                                        # Shape: (n_y, m)
    # Calculate how much to adjust the CEO's weights
    dW2 = 1/m * np.dot(dZ2, A1.T)                       # Shape: (n_y, n_h)
    # Calculate how much to adjust the CEO's bias
    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)      # Shape: (n_y, 1)
    
    # Using the chain rule to find error at the hidden layer
    # This is like determining how each middle manager contributed to the overall error
    dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)             # Shape: (n_h, m)
    # Calculate how much to adjust the middle managers' weights
    dW1 = 1/m * np.dot(dZ1, X.T)                        # Shape: (n_h, n_x)
    # Calculate how much to adjust the middle managers' biases
    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)      # Shape: (n_h, 1)
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads
```

### Updating Parameters

The update step is like implementing the feedback from the performance review - adjusting each employee's approach to do better next time:

```python
def update_parameters(parameters, grads, learning_rate=1.2):
    """
    Updates parameters using gradient descent
    
    Arguments:
    parameters -- python dictionary containing parameters
    grads -- python dictionary containing gradients
    learning_rate -- learning rate for gradient descent
    
    Returns:
    parameters -- python dictionary containing updated parameters
    """
    # Retrieve parameters
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Retrieve gradients
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]
    
    # Update rule for each parameter
    # This is like each employee adjusting their approach based on feedback
    # The learning rate determines how big of an adjustment they make
    W1 = W1 - learning_rate * dW1  # Adjust middle managers' weights
    b1 = b1 - learning_rate * db1  # Adjust middle managers' biases
    W2 = W2 - learning_rate * dW2  # Adjust CEO's weights
    b2 = b2 - learning_rate * db2  # Adjust CEO's bias
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters
```

### Integration: The Neural Network Model

Now we put all the pieces together, like assembling our complete organizational system and training it through many iterations:

```python
def nn_model(X, Y, n_h, num_iterations=3000, learning_rate=1.2, print_cost=False):
    """
    Builds the complete neural network model
    
    Arguments:
    X -- dataset of shape (n_x, number of examples)
    Y -- labels of shape (n_y, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- number of iterations in gradient descent
    learning_rate -- learning rate parameter for gradient descent
    print_cost -- if True, print the cost every 100 iterations
    
    Returns:
    parameters -- parameters learnt by the model
    """
    # Get input and output dimensions
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters
    parameters = initialize_parameters(n_x, n_h, n_y)
    
    # Loop for num_iterations - like training the team through repeated experiences
    for i in range(0, num_iterations):
        # Forward propagation - the team makes predictions
        A2, cache = forward_propagation(X, parameters)
        
        # Compute cost - measure how well the team is doing
        cost = compute_cost(A2, Y)
        
        # Backward propagation - figure out how to improve
        grads = backward_propagation(parameters, cache, X, Y)
        
        # Update parameters - implement the improvements
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the cost every 100 iterations - like periodic performance reviews
        if print_cost and i % 100 == 0:
            print("Cost after iteration %i: %f" % (i, cost))
    
    return parameters
```

This training process is similar to how a chef learns to cook - they try a recipe (forward pass), taste the result (compute cost), figure out what to adjust (backward pass), and then adjust their recipe for next time (update parameters). Through many iterations, they get better and better.

### Making Predictions

After training, we can use our model to make predictions on new data:

```python
def predict(X, parameters):
    """
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    X -- input data of shape (n_x, m)
    parameters -- python dictionary containing your parameters 
    
    Returns:
    predictions -- vector of predictions of our model (0: blue, 1: red)
    """
    # Forward propagation - run the trained network
    A2, cache = forward_propagation(X, parameters)
    
    # Convert probabilities to binary predictions
    # If the probability is greater than 0.5, predict class 1, otherwise class 0
    # This is like setting a decision threshold - above 50% confidence, say "yes"
    predictions = (A2 > 0.5).astype(int)
    
    return predictions
```

## Training and Visualizing the Model

Let's train our neural network and visualize what boundary it creates:

```python
# Train the model with 2 neurons in the hidden layer
# This is like a 2-week intensive training program for our team
parameters = nn_model(X, Y, n_h=2, num_iterations=3000, learning_rate=1.2, print_cost=True)

# Define a function to plot the decision boundary
def plot_decision_boundary(predict, parameters, X, Y):
    # Create a grid of points covering our data space
    # This is like testing our team's decisions across all possible scenarios
    min1, max1 = X[0, :].min()-1, X[0, :].max()+1
    min2, max2 = X[1, :].min()-1, X[1, :].max()+1
    
    # Define the x and y scale
    x1grid = np.arange(min1, max1, 0.1)
    x2grid = np.arange(min2, max2, 0.1)
    
    # Create grid of points
    xx, yy = np.meshgrid(x1grid, x2grid)
    
    # Flatten grid points
    r1, r2 = xx.flatten(), yy.flatten()
    r1, r2 = r1.reshape((1, len(r1))), r2.reshape((1, len(r2)))
    
    # Stack to create input array for model
    grid = np.vstack((r1, r2))
    
    # Make predictions for every point on the grid
    predictions = predict(grid, parameters)
    
    # Reshape predictions back to grid
    zz = predictions.reshape(xx.shape)
    
    # Plot the grid of x, y and z values
    plt.contourf(xx, yy, zz, cmap=plt.cm.Spectral.reversed())
    plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']))
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')

# Plot the decision boundary - visualize what our network learned
plot_decision_boundary(predict, parameters, X, Y)
plt.title("Decision Boundary for hidden layer size " + str(n_h))
plt.show()
```

The resulting plot is like a map showing where our neural network would place the boundary between territories - all points on one side are classified as blue, all on the other side as red.

## Experimentation: Changing the Hidden Layer Size

Just as a company might experiment with different team structures, we can adjust our neural network by changing the number of "middle managers" (neurons in the hidden layer):

```python
# Try with different hidden layer sizes
hidden_layer_sizes = [1, 2, 5, 10, 20]
for n_h in hidden_layer_sizes:
    # Train the model with varying team sizes
    parameters = nn_model(X, Y, n_h=n_h, num_iterations=3000, learning_rate=1.2, print_cost=False)
    
    # Plot the decision boundary
    plt.figure(figsize=(5, 5))
    plot_decision_boundary(predict, parameters, X, Y)
    plt.title("Decision Boundary for hidden layer size " + str(n_h))
    plt.show()
```

With just 1 hidden neuron, our network can only create a simple boundary (like having just one middle manager with limited perspective). With more neurons, it can create more complex boundaries (like having a diverse team with different viewpoints).

## Conclusion

In this project, you've built a neural network from scratch, similar to constructing a small decision-making organization. You've seen how:

1. Adding a hidden layer allows the network to learn non-linear decision boundaries, like how a team of people can solve more complex problems than a single person
2. Forward propagation passes information through the network, like information flowing through an organization
3. Backward propagation calculates gradients for learning, like feedback helping employees improve
4. Different hidden layer sizes affect the complexity of the decision boundary, like how team size affects problem-solving capacity

The process of training a neural network is much like learning any skill - you start with limited ability, make mistakes, get feedback, adjust your approach, and gradually improve through practice and iteration.

## Further Challenges

1. Try implementing the network with different activation functions (like using different decision-making styles)
2. Experiment with different learning rates (like changing how quickly employees adapt to feedback)
3. Add regularization to prevent overfitting (like preventing employees from becoming too specialized in just the training examples)
4. Create your own custom dataset and test the model's performance
5. Extend the network to have multiple hidden layers (like adding more levels of management)