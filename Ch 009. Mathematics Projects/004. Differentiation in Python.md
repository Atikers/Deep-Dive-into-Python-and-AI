# Differentiation in Python

## Overview

This Practice Lab explores three different ways to compute derivatives in Python:

1. **Symbolic Differentiation** using **SymPy**  
2. **Numerical Differentiation** using **NumPy**  
3. **Automatic Differentiation** using **JAX** (powered by Autograd)

We’ll see how each approach works in code, learn their pros and cons, and compare their computational efficiencies.

---

## A Quick Refresher on Python Functions

A simple function like $f(x) = x^2$ is defined in Python as:

```python
def f(x):
    return x**2
```

And its analytical derivative $f'(x) = 2x$:

```python
def dfdx(x):
    return 2*x
```

If you use NumPy arrays, you can apply these functions element-wise:

```python
import numpy as np

x_array = np.array([1, 2, 3])
print(f(x_array))      # [1, 4, 9]
print(dfdx(x_array))   # [2, 4, 6]
```

This is straightforward, but usually in machine learning we have more complicated functions where you can’t easily write out the derivative by hand.

---

## Symbolic Differentiation (SymPy)

### Basic SymPy Usage
- **SymPy** does exact, algebraic manipulation.
- Define symbolic variables with `symbols`:
  ```python
  from sympy import symbols, diff, sqrt, exp, sin, lambdify
  x, y = symbols('x y')
  expr = 2*x**2 - x*y
  ```
- Evaluate expressions at particular values with `.evalf(subs={variable: value})`.
- Convert a symbolic expression to a NumPy-friendly function using `lambdify`.

### Differentiating Symbolically
- Use `diff(expression, variable)`:
  ```python
  f_symb = x**2
  dfdx_symb = diff(f_symb, x)         # 2*x
  dfdx_symb_func = lambdify(x, dfdx_symb, 'numpy')
  
  import numpy as np
  x_vals = np.array([1, 2, 3])
  print(dfdx_symb_func(x_vals))       # [2, 4, 6]
  ```
- SymPy is powerful for exact derivatives but can produce very complicated expressions (“expression swell”) and sometimes fails at points with discontinuities - e.g. $|x|$

---

## Numerical Differentiation (NumPy)

### Approximate the Derivative
The idea:  

$$
\frac{d}{dx} f(x) \approx \frac{f(x + \Delta x) - f(x)}{\Delta x}
$$

You can use `np.gradient`, which numerically estimates derivatives from sampled values:

```python
import numpy as np

def f_composed(x):
    return np.exp(-2*x) + 3*np.sin(3*x)

x_array_2 = np.linspace(-5, 5, 100)
dfdx_numerical = np.gradient(f_composed(x_array_2), x_array_2)
```

### Downsides of Numerical Differentiation
- It’s an **approximation**, so there’s round-off error.
- Handling discontinuities or sharp corners can be inaccurate.
- It’s **slower** in cases where you need a lot of derivative evaluations (e.g., in ML training with many parameters).

---

## Automatic Differentiation (JAX)

### Basics
JAX provides:
- `jax.numpy` (imported as `jnp`)—a drop-in for NumPy.
- `grad`—to compute derivatives automatically.
- `vmap`—to vectorize computation over arrays.

Example:
```python
from jax import grad, vmap
import jax.numpy as jnp

def f(x):
    return x**2

print(grad(f)(3.0))          # 6.0
```

For arrays, you can wrap `grad(f)` with `vmap`:

```python
x_array_jnp = jnp.array([1., 2., 3.])
dfdx_jax_vmap = vmap(grad(f))(x_array_jnp)   # [2., 4., 6.]
```

### Advantages
- JAX uses **computational graphs** + **chain rule** under the hood.
- **Very fast** for large-scale, repeated derivative computations (like training deep neural nets).

---

## Comparing Computational Efficiency

A time test shows:
- **Numerical** derivatives can be slow because each derivative requires a separate function evaluation.
- **Symbolic** can handle simple expressions quickly but can slow down due to “expression swell” if the function is complex.
- **Automatic Differentiation** (JAX) often scales better with complex functions and large inputs.

For a simple function $x^2$, symbolic vs. automatic might be similar. But for more complex polynomial or multi-operation functions, JAX’s automatic differentiation typically outperforms the rest.

---

## Key Takeaways for Beginners
1. **SymPy** is fantastic when you need exact, symbolic expressions (like doing algebra on a computer). However, watch for complicated expressions and “breakdowns” at discontinuities.
2. **NumPy**’s numerical differentiation is straightforward but can be noisy near sharp corners and quite slow if you have to do it at scale.
3. **JAX** (and Autograd-based approaches) are the go-to for most machine learning applications since they handle derivatives automatically and efficiently via computational graphs.

If you’re just starting out, I’d recommend:
- **Experiment** with SymPy to understand derivatives and algebraic manipulations.
- **Use** numerical differentiation for quick checks or simpler tasks.
- **Adopt** JAX (or another autodiff framework) for performance-critical ML tasks.

All three can deepen your intuition about how derivatives can be handled in Python, and each is helpful in different scenarios.