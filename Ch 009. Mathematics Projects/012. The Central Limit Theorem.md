# Mathematics Project: Visualizing the Central Limit Theorem

In this project, we'll bring to life the Central Limit Theorem using Python. Rather than just reading about this powerful concept, you'll actually see it in action through interactive visualizations. This hands-on approach will make the abstract mathematical ideas concrete and accessible.

## Project Overview

Through this project, you'll:
1. Generate data from different distribution objects
2. Sample from these distributions and calculate means
3. Visualize how these sample means behave
4. Test the limits of the Central Limit Theorem
5. Discover when and why the theorem breaks down

Let's approach this with our object-oriented mindset, thinking of distributions as objects with properties and behaviors!

## Setting Up Your Environment

First, we need to import the necessary libraries. Each of these libraries represents a collection of tools that we'll use throughout our project:

```python
import numpy as np                # For numerical operations and random sampling
import pandas as pd               # For data handling
import seaborn as sns             # For beautiful visualizations
import matplotlib.pyplot as plt   # For creating plots
from scipy import stats           # For statistical functions
from scipy.stats import norm      # For working with normal distributions
```

## Distribution Factory: Creating Our Test Data

Let's begin by creating our first distribution object - a Gaussian (normal) distribution:

```python
# Set the properties of our Gaussian object
mu = 10        # The mean property
sigma = 5      # The standard deviation property

# Create 100,000 instances from our Gaussian distribution
gaussian_population = np.random.normal(mu, sigma, 100_000)

# Visualize the distribution
sns.histplot(gaussian_population, stat="density")
plt.title("Gaussian Population Distribution")
plt.xlabel("Value")
plt.ylabel("Density")
plt.show()
```

This code creates a Gaussian distribution object with a mean of 10 and standard deviation of 5, then generates 100,000 values from this distribution. The resulting histogram should show the classic bell curve shape.

## The Transformation Machine: Sample Mean Creator

Now let's build our sample mean transformation machine. This function takes samples from our distribution and produces a new distribution of sample means:

```python
def sample_means(data, sample_size):
    """
    Transform a distribution into a distribution of sample means.
    
    Parameters:
    data (array): The original distribution values
    sample_size (int): How many values to include in each sample
    
    Returns:
    array: A collection of sample means
    """
    # Container for our sample means
    means = []

    # Generate 10,000 sample means for a smooth visualization
    for _ in range(10_000):
        # Take a random sample WITH replacement
        sample = np.random.choice(data, size=sample_size)
        
        # Calculate and store the mean of this sample
        means.append(np.mean(sample))

    # Return all sample means as a numpy array
    return np.array(means)
```

This function is the heart of our project. For each iteration, it:
1. Takes a random sample from our distribution
2. Calculates the mean of that sample
3. Stores the mean in our collection
4. Repeats this process 10,000 times to build a robust distribution of sample means

## Testing the Theorem with Different Distribution Objects

### 1. The Gaussian Case

Let's start by testing the theorem with our Gaussian distribution:

```python
# Transform our Gaussian distribution using a sample size of 5
gaussian_sample_means = sample_means(gaussian_population, sample_size=5)

# Calculate the theoretical properties of the sample means distribution
mu_sample_means = mu  # The mean remains the same
sigma_sample_means = sigma / np.sqrt(5)  # The standard deviation shrinks

# Create a range of x-values for our theoretical curve
x_range = np.linspace(min(gaussian_sample_means), max(gaussian_sample_means), 100)

# Plot both the empirical and theoretical distributions
plt.figure(figsize=(10, 6))

# Histogram of sample means
sns.histplot(gaussian_sample_means, stat="density", label="Sample Means")

# Estimated PDF through kernel density estimation
sns.kdeplot(
    data=gaussian_sample_means,
    color="crimson",
    label="KDE Estimate",
    linestyle="dashed",
    fill=True
)

# Theoretical normal distribution curve
plt.plot(
    x_range,
    norm.pdf(x_range, loc=mu_sample_means, scale=sigma_sample_means),
    color="black",
    label="Theoretical Normal"
)

plt.title("Sample Means from Gaussian Distribution (n=5)")
plt.xlabel("Sample Mean Value")
plt.ylabel("Density")
plt.legend()
plt.show()

# Create a QQ plot to check for normality
plt.figure(figsize=(6, 6))
stats.probplot(gaussian_sample_means, plot=plt)
plt.title("QQ Plot for Sample Means (Gaussian Population)")
plt.show()
```

This code produces two visualizations:
1. A histogram of sample means with both a KDE curve and the theoretical normal curve overlaid
2. A QQ plot, which should show points following a straight line if the distribution is normal

Even with a small sample size of 5, you should see the sample means follow a normal distribution almost perfectly. This happens because the original population is already Gaussian.

### 2. The Binomial Case: When Discrete Becomes Continuous

Next, let's try with a binomial distribution, which is discrete and potentially skewed:

```python
# Create a binomial distribution object
n = 5  # Number of trials
p = 0.8  # Probability of success

# Generate 100,000 values from this distribution
binomial_population = np.random.binomial(n, p, 100_000)

# Visualize the original distribution
plt.figure(figsize=(10, 6))
sns.histplot(binomial_population, stat="count")
plt.title("Binomial Population Distribution (n=5, p=0.8)")
plt.xlabel("Value")
plt.ylabel("Count")
plt.show()

# Calculate the theoretical properties of the binomial distribution
binomial_pop_mean = n * p
binomial_pop_std = np.sqrt(n * p * (1 - p))
print(f"Binomial population has mean: {binomial_pop_mean:.1f} and std: {binomial_pop_std:.1f}")

# Let's test with a small sample size first
sample_size = 3

# Calculate the condition value for binomial CLT rule of thumb
N = n * sample_size
condition_value = min(N * p, N * (1 - p))
clt_expected = condition_value >= 5
print(f"Condition value: {condition_value:.1f}. CLT should hold: {clt_expected}")

# Transform the binomial distribution using our sample means function
binomial_sample_means = sample_means(binomial_population, sample_size=sample_size)

# Calculate theoretical properties of the sample means distribution
mu_sample_means = binomial_pop_mean
sigma_sample_means = binomial_pop_std / np.sqrt(sample_size)

# Create a range of x-values for our theoretical curve
x_range = np.linspace(min(binomial_sample_means), max(binomial_sample_means), 100)

# Plot the results
plt.figure(figsize=(10, 6))

# Histogram of sample means
sns.histplot(binomial_sample_means, stat="density", label="Sample Means")

# Estimated PDF through kernel density estimation
sns.kdeplot(
    data=binomial_sample_means,
    color="crimson",
    label="KDE Estimate",
    linestyle="dashed",
    fill=True
)

# Theoretical normal distribution curve
plt.plot(
    x_range,
    norm.pdf(x_range, loc=mu_sample_means, scale=sigma_sample_means),
    color="black",
    label="Theoretical Normal"
)

plt.title(f"Sample Means from Binomial Distribution (sample_size={sample_size})")
plt.xlabel("Sample Mean Value")
plt.ylabel("Density")
plt.legend()
plt.show()

# Create a QQ plot to check for normality
plt.figure(figsize=(6, 6))
stats.probplot(binomial_sample_means, plot=plt)
plt.title(f"QQ Plot for Sample Means (Binomial, sample_size={sample_size})")
plt.show()

# Now let's try with a larger sample size
sample_size = 30

# Calculate the new condition value
N = n * sample_size
condition_value = min(N * p, N * (1 - p))
clt_expected = condition_value >= 5
print(f"Condition value: {condition_value:.1f}. CLT should hold: {clt_expected}")

# Transform the binomial distribution with the larger sample size
binomial_sample_means = sample_means(binomial_population, sample_size=sample_size)

# Calculate new theoretical properties
mu_sample_means = binomial_pop_mean
sigma_sample_means = binomial_pop_std / np.sqrt(sample_size)

# Plot the results
plt.figure(figsize=(10, 6))

# Histogram of sample means
sns.histplot(binomial_sample_means, stat="density", label="Sample Means")

# Estimated PDF through kernel density estimation
sns.kdeplot(
    data=binomial_sample_means,
    color="crimson",
    label="KDE Estimate",
    linestyle="dashed",
    fill=True
)

# Theoretical normal distribution curve
plt.plot(
    x_range,
    norm.pdf(x_range, loc=mu_sample_means, scale=sigma_sample_means),
    color="black",
    label="Theoretical Normal"
)

plt.title(f"Sample Means from Binomial Distribution (sample_size={sample_size})")
plt.xlabel("Sample Mean Value")
plt.ylabel("Density")
plt.legend()
plt.show()

# Create a QQ plot to check for normality
plt.figure(figsize=(6, 6))
stats.probplot(binomial_sample_means, plot=plt)
plt.title(f"QQ Plot for Sample Means (Binomial, sample_size={sample_size})")
plt.show()
```

This code shows the transformation of a binomial distribution (discrete and potentially skewed) into a normal distribution of sample means. Notice how the approximation improves dramatically as the sample size increases from 3 to 30.

### 3. When the Theorem Fails: The Cauchy Distribution

Finally, let's explore a case where the Central Limit Theorem doesn't hold - the Cauchy distribution:

```python
# Create a Cauchy distribution
cauchy_population = np.random.standard_cauchy(1000)

# Let's constrain the values for visualization purposes
cauchy_population = cauchy_population[(cauchy_population > -10) & (cauchy_population < 10)]

# Visualize the original distribution
plt.figure(figsize=(10, 6))
sns.histplot(cauchy_population, stat="density", bins=50)
plt.title("Cauchy Population Distribution")
plt.xlabel("Value")
plt.ylabel("Density")
plt.show()

# Transform with sample size 30
cauchy_sample_means = sample_means(cauchy_population, sample_size=30)

# Create a QQ plot to check for normality
plt.figure(figsize=(6, 6))
stats.probplot(cauchy_sample_means, plot=plt)
plt.title("QQ Plot for Sample Means (Cauchy, sample_size=30)")
plt.show()

# Try with an even larger sample size
cauchy_sample_means = sample_means(cauchy_population, sample_size=100)

# Create a QQ plot to check for normality
plt.figure(figsize=(6, 6))
stats.probplot(cauchy_sample_means, plot=plt)
plt.title("QQ Plot for Sample Means (Cauchy, sample_size=100)")
plt.show()
```

This section demonstrates a fascinating exception to the Central Limit Theorem. The Cauchy distribution has such heavy tails that even with large sample sizes, the sample means don't converge to a normal distribution.

## What We've Learned: Object-Oriented Perspective

From an object-oriented viewpoint, we've observed:

1. **Distribution objects** have properties (mean, variance) and behaviors (sampling)

2. **Transformation objects** (like our sample_means function) can convert one distribution object into another

3. **The Central Limit Theorem** acts as a universal transformation that converts almost any distribution object into a normal distribution object when we use sample means as the transformation method

4. **Distribution inheritance** - The sample means distribution inherits certain properties from the parent distribution (the mean), while modifying others (the variance)

5. **Polymorphic behavior** - Different distribution objects respond uniquely to the sample means transformation, yet most converge to the same type of object (normal distribution)

6. **Exceptions to inheritance** - Some distributions, like Cauchy, resist this transformation regardless of sample size, showing that not all objects follow the same inheritance patterns

## Extending the Project

Here are some ways you could extend this project:

1. Try different distributions: exponential, uniform, or create your own custom distribution

2. Explore the effect of different sample sizes on the transformation

3. Implement a class-based approach where each distribution is a proper Python class with methods for sampling, transformation, and visualization

4. Create an interactive visualization tool that lets you adjust parameters in real-time

The Central Limit Theorem reveals the hidden connections between seemingly different distribution objects. By understanding this fundamental principle, you gain insight into why the normal distribution appears so frequently in nature and human systems - it's the natural result of averaging many independent factors, regardless of their individual distributions.