# **Minimizing a One-Variable Cost Function**

This lab demonstrates how to minimize a cost function $\mathcal{L}(\omega)$ with respect to a single parameter $\omega$. 

The scenario: you have two suppliers, A and B, offering product $P$ at varying monthly prices. You need to decide **what fraction** - $\omega \in [0,1]$ - of your total purchase each month should come from supplier A (with the remainder from B) to minimize a certain risk-related metric—here represented as a **variance** of monthly costs.

## Data Loading and Overview

Monthly prices (in USD) are stored in a CSV file, containing columns for supplier A and supplier B. We load them as NumPy arrays (via `pandas` and `jax.numpy`):

```python
import pandas as pd
import jax.numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('data/prices.csv')  # "prices.csv" has monthly price data for both suppliers

prices_A = np.array(df['price_supplier_a_dollars_per_item']).astype('float32')
prices_B = np.array(df['price_supplier_b_dollars_per_item']).astype('float32')

plt.plot(prices_A, label='Supplier A')
plt.plot(prices_B, label='Supplier B')
plt.legend()
plt.show()
```
**Observation**: Sometimes supplier A is cheaper, sometimes B is cheaper—so a single fraction $\omega$ might balance costs over time.

## Formulating the Cost and Loss Functions

For month $i$,  

$$
f^i(\omega) = p_A^i \cdot \omega + p_B^i \cdot (1 - \omega)
$$

Here $p_A^i, p_B^i$ are historical prices of suppliers A and B in month $i$, and $\omega$ is the fraction of product you buy from A.

We then define a **loss function** $\mathcal{L}(\omega)$ as the **variance** of $\{f^i(\omega)\}$:  

$$
\mathcal{L}(\omega) = \frac{1}{k}\sum_{i=1}^{k} (f^i(\omega) - \overline{f(\omega)})^2,\quad \overline{f(\omega)} = \frac{1}{k}\sum_{i=1}^{k} f^i(\omega)
$$

Minimizing this variance can be seen as minimizing the fluctuation risk or volatility in monthly costs.

### Implementing the Functions in Python

```python
def f_of_omega(omega, pA, pB):
    return pA * omega + pB * (1 - omega)

def L_of_omega(omega, pA, pB):
    f_vals = f_of_omega(omega, pA, pB)
    return (1 / len(f_vals)) * np.sum((f_vals - np.mean(f_vals))**2)
```

## Finding the Minimum Numerically

### Line Search

We discretize $\omega \in [0,1]$ into many points, compute $\mathcal{L}(\omega)$ at each point, and pick the minimum:

```python
N = 1001
omega_array = np.linspace(0, 1, N)
L_array = np.zeros(N)

for i in range(N):
    L_val = L_of_omega(omega_array[i], prices_A, prices_B)
    L_array = L_array.at[i].set(L_val)

# Find the minimum index
i_opt = L_array.argmin()
omega_opt = omega_array[i_opt]
L_opt = L_array[i_opt]

print("Optimal omega =", float(omega_opt))
print("Minimum L =", float(L_opt))
```

**Result**: For the sample dataset, $\omega \approx 0.70$. In other words, buying about 70% from A and 30% from B minimized the variance of monthly costs.

### Checking the Gradient

To verify that this $\omega$ is indeed the minimizer, we can check whether its derivative $\frac{d\mathcal{L}}{d\omega}$ is close to 0. Using **JAX** for automatic differentiation:

```python
from jax import grad

dLdOmega_array = np.zeros(N)
for i in range(N):
    dLdOmega = grad(L_of_omega)(omega_array[i], prices_A, prices_B)
    dLdOmega_array = dLdOmega_array.at[i].set(dLdOmega)

# Verify derivative near the minimum is ~0
i_opt_grad = np.abs(dLdOmega_array).argmin()
omega_opt_grad = omega_array[i_opt_grad]
dLdOmega_opt_grad = dLdOmega_array[i_opt_grad]
print("Omega minimizing derivative:", float(omega_opt_grad))
print("Gradient at that point:", float(dLdOmega_opt_grad))
```

## Visualization

Plot both the loss $\mathcal{L}(\omega)$ and its derivative $\frac{d\mathcal{L}}{d\omega}$, and mark the minimum:

```python
plt.plot(omega_array, L_array, label="L(omega)")
plt.plot(omega_array, dLdOmega_array, label="dL/domega", color='orange')
plt.plot(omega_opt, L_opt, 'ro')  # Mark the minimum
plt.legend()
plt.show()
```

Where the derivative crosses zero aligns with the minimum of $\mathcal{L}(\omega)$.

## Key Takeaways

1. **Set Up**: We model the cost from each supplier, create a function of $\omega$.  
2. **Loss**: We define a variance-based objective $\mathcal{L}(\omega)$, aiming to stabilize costs over time.  
3. **Optimization**: We discretely sample $\omega \in [0,1]$ and find the point that minimizes the variance.  
4. **Verification**: Using JAX’s `grad`, we confirm that $\frac{d\mathcal{L}}{d\omega} \approx 0$ near the optimal $\omega$.  
5. **Result**: A ratio $\omega \approx 0.70$ is best for the historical dataset used, meaning around 70% should be purchased from supplier A and 30% from supplier B.  

For high-dimensional or more complex optimization, one typically employs **gradient-based** or **stochastic** methods. However, this one-dimensional case illustrates the fundamental process of:
- defining a cost (loss) function,  
- searching for its minimum,  
- and verifying via gradient.