# **Linear Regression with Gradient Descent**

Linear regression aims to find a **straight line** that best fits a set of points. Specifically, we have:  

$$
\hat{y}^{(i)} = m x^{(i)} + b
$$

where $m$ (slope) and $b$ (intercept) define the line. We want to adjust $m$ and $b$ so that $\hat{y}^{(i)}$ is as close as possible to $y^{(i)}$, the actual value.

### **Setup**

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression
```

### **Load the Dataset**
The file `"data/tvmarketing.csv"` has two columns: `TV` (marketing expenses) and `Sales` (actual sales).

```python
path = "data/tvmarketing.csv"
adv = pd.read_csv(path)
X = adv['TV']
Y = adv['Sales']

# Quick check
adv.head()
adv.plot(x='TV', y='Sales', kind='scatter', c='black')
plt.show()
```
**Goal**: Given a TV budget (`X`), predict sales (`Y`).

## **Method 1: Linear Regression with NumPy**

### **Using `np.polyfit`**
We can let `NumPy` do the work by fitting a polynomial of degree 1 (a straight line):
```python
m_numpy, b_numpy = np.polyfit(X, Y, 1)
print(f"NumPy fit -> slope: {m_numpy:.3f}, intercept: {b_numpy:.3f}")

def pred_numpy(m, b, X_vals):
    return m * X_vals + b

X_pred = np.array([50, 120, 280])  # example new data
Y_pred_numpy = pred_numpy(m_numpy, b_numpy, X_pred)
```
Plot:
```python
plt.scatter(X, Y, color='black')
plt.plot(X, m_numpy * X + b_numpy, color='red')
plt.scatter(X_pred, Y_pred_numpy, color='blue', s=60)
plt.xlabel('TV')
plt.ylabel('Sales')
plt.show()

print("Predictions (NumPy):", Y_pred_numpy)
```
**Interpretation**: The slope `m_numpy` shows how much `Sales` changes for each unit of `TV` budget, while `b_numpy` is the baseline when `TV = 0`.

## **Method 2: Linear Regression with Scikit-Learn**

`Scikit-Learn` is a robust machine learning library. Here’s how to do the same fit:

### **Model Creation & Fitting**
```python
lr_sklearn = LinearRegression()
# Reshape X and Y to 2D (shape (n_samples, 1))
X_sklearn = X[:, np.newaxis]
Y_sklearn = Y[:, np.newaxis]
lr_sklearn.fit(X_sklearn, Y_sklearn)

m_sklearn = lr_sklearn.coef_[0,0]
b_sklearn = lr_sklearn.intercept_[0]
print(f"Scikit-Learn fit -> slope: {m_sklearn:.3f}, intercept: {b_sklearn:.3f}")
```

### **Predict with the Model**
```python
def pred_sklearn(X_vals, lr_model):
    X_2D = X_vals[:, np.newaxis]
    return lr_model.predict(X_2D)

Y_pred_sklearn = pred_sklearn(X_pred, lr_sklearn)
print("Predictions (Scikit-Learn):", Y_pred_sklearn.flatten())
```
**Observation**: The slope/intercept should match the NumPy results.

## **Method 3: Linear Regression via Gradient Descent**

### **Why Gradient Descent?**
- **Idea**: Instead of solving equations directly, we start with random guesses for `m` and `b`.
- We measure “how bad” those guesses are using a **cost function**—often the **sum of squared errors**.
- Then, we iteratively **adjust** `m` and `b` in the direction that **reduces** the cost, until it (hopefully) can’t get any smaller.

### **Normalizing Data (Optional but Helpful)**
Large or different scales for `X` and `Y` can make gradient descent **slower** to converge, so we often **normalize** them:
```python
X_norm = (X - np.mean(X)) / np.std(X)
Y_norm = (Y - np.mean(Y)) / np.std(Y)
```

### **Defining the Cost Function**  

$$
E(m,b) = \frac{1}{2n} \sum_{i=1}^n (m X_i + b - Y_i)^2
$$

```python
def E(m, b, X_data, Y_data):
    return (1/(2*len(Y_data))) * np.sum((m * X_data + b - Y_data)**2)
```

### **Partial Derivatives**
We compute how the cost changes with respect to `m` and `b`:
```python
def dEdm(m, b, X_data, Y_data):
    return (1/len(Y_data)) * np.dot((m*X_data + b - Y_data), X_data)

def dEdb(m, b, X_data, Y_data):
    return (1/len(Y_data)) * np.sum(m*X_data + b - Y_data)
```

### **Gradient Descent Algorithm**
1. Pick initial guesses: `(m, b) = (m_init, b_init)`.
2. Repeat many times - $\alpha$ is the learning rate:  

$$
m = m - \alpha \frac{\partial E}{\partial m},\quad 
b = b - \alpha \frac{\partial E}{\partial b}
$$

3. Stop when changes are small or after a set number of iterations.

```python
def gradient_descent(dEdm, dEdb, m, b, X_data, Y_data, learning_rate=0.001, num_iterations=1000, print_cost=False):
    for i in range(num_iterations):
        m = m - learning_rate * dEdm(m, b, X_data, Y_data)
        b = b - learning_rate * dEdb(m, b, X_data, Y_data)
        if print_cost and i % 10 == 0:
            print(f"Iteration {i}, Cost: {E(m, b, X_data, Y_data):.4f}")
    return m, b

# Run Gradient Descent
m_init, b_init = 0, 0
m_gd, b_gd = gradient_descent(dEdm, dEdb, m_init, b_init, X_norm, Y_norm, learning_rate=1.2, num_iterations=30, print_cost=True)
print(f"Gradient Descent -> slope: {m_gd:.3f}, intercept: {b_gd:.3f}")
```

### **Denormalizing Predictions**
We fitted on normalized data, so we must **reverse** the normalization for predictions:
```python
X_pred_norm = (X_pred - np.mean(X)) / np.std(X)
Y_pred_gd_norm = m_gd * X_pred_norm + b_gd
Y_pred_gd = Y_pred_gd_norm * np.std(Y) + np.mean(Y)

print("Predictions (Gradient Descent):", Y_pred_gd)
```
**Result**: Similar to NumPy/Scikit-Learn solutions, confirming that gradient descent converged to the same best-fit line.

### **Key Takeaways**
1. **Linear Regression**: Find slope `m` and intercept `b` to minimize errors.
2. **Multiple Approaches**:
   - **Analytic** (like `np.polyfit`).
   - **Library** (like `Scikit-Learn`).
   - **From Scratch** with gradient descent (scalable to more complex scenarios).
3. **Gradient Descent**:
   - Iterative: start with guesses, repeatedly refine based on cost.
   - Learning rate $\alpha$ controls **step size**—too big overshoots, too small takes forever.
   - Normalization often **speeds up** convergence.
4. **Same End Result**: All methods produce comparable lines, showing how gradient descent underpins many ML solutions.