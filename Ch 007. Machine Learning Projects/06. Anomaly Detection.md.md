# **1. Scenario**

Imagine you’ve just been hired as a **data scientist** at *ServerSecure Inc.*, a company dedicated to selling servers to clients worldwide. These servers handle everything from social media platforms to secure bank transactions. You’ve received a large dataset showing how these servers behave in the field—measuring things like CPU usage, memory capacity, network traffic, and more.

Your job? **Figure out which servers might be malfunctioning** so that on-site technicians can visit those server rooms and fix them before clients experience downtime. Since you only have data about how the servers typically perform (with no explicit “this server failed” labels), you decide to use an **Anomaly Detection** approach. By modeling the *normal* behavior of servers, you can spot the “odd ones out” that need further inspection.

You set up a **Gaussian model** to learn the baseline performance for each of the server metrics. Then, you implement a thresholding mechanism to decide when a probability is “too low” and the server is likely anomalous. It’s your first step toward a proactive maintenance strategy at *ServerSecure Inc.*—reducing costly failures and keeping your customers happy.

# ***2. Steps***

## ***Step 1. Library Imports***  

```python
import numpy as np
import matplotlib.pyplot as plt
from utils import *

%matplotlib inline
```

## ***Step 2. Data Loading and Shapes***  

```python
X_train, X_val, y_val = load_data()
print ('The shape of X_train is:', X_train.shape)
print ('The shape of X_val is:', X_val.shape)
print ('The shape of y_val is: ', y_val.shape)
```

output:
```
The shape of X_train is: (1000, 11)
The shape of X_val is: (100, 11)
The shape of y_val is: (100,)
```

## ***Step 3. Estimating Gaussian Parameters***  

```python
def estimate_gaussian(X): 
       m, n = X.shape
       mu = 1/m * np.sum(X, axis=0)
       var = 1/m * np.sum((X - mu)**2, axis=0)
       return mu, var
```
- Using `1/m` is a common approach (though some statisticians prefer `1/(m-1)` for an unbiased estimate). In practice, either works for anomaly detection.

## ***Step 4. Multivariate Gaussian Probability***  

```python
# computes $p(x)$ for each example in `X_train` using the estimated `mu` and `var`.  
p = multivariate_gaussian(X_train, mu, var)
```

## Step 5. Selecting Threshold $\varepsilon$

```python
def select_threshold(y_val, p_val): 
       best_epsilon = 0
       best_F1 = 0
       F1 = 0
    
       step_size = (max(p_val) - min(p_val)) / 1000
    
       for epsilon in np.arange(min(p_val), max(p_val), step_size):
        
           predictions = (p_val < epsilon)
        
           tp = np.sum((predictions == 1) & (y_val == 1))
           fp = np.sum((predictions == 1) & (y_val == 0))
           fn = np.sum((predictions == 0) & (y_val == 1))
           tn = np.sum((predictions == 0) & (y_val == 0))
        
           prec = tp / (tp + fp)
           rec = tp / (tp + fn)
        
           F1 = 2 * prec * rec / (prec + rec)
        
           if F1 > best_F1:
               best_F1 = F1
               best_epsilon = epsilon
    
       return best_epsilon, best_F1
```

## Step 6. Final Output

```python
p_val = multivariate_gaussian(X_val, mu, var)
epsilon, F1 = select_threshold(y_val, p_val)

print('Best epsilon found using cross-validation: %e' % epsilon)
print('Best F1 on Cross Validation Set: %f' % F1)
print('# anomalies found: %d' % sum(p < epsilon))
```

output:
```
Best epsilon found using cross-validation: 1.377229e-18
Best F1 on Cross Validation Set: 0.615385
# anomalies found: 117
```

Output suggests the threshold is around `1.377229e-18` with an $F_1$ of ~0.615. The model flagged 117 out of 1000 examples as anomalies, which might or might not be reasonable, depending on the real-world context.

# ***3. Conclusion***

In this project, you tackled the problem of finding faulty servers within a large dataset of normal operations. By:

- **Fitting a Gaussian model** to learn what “normal” looks like,
- **Estimating** the mean and variance for each server feature,
- **Selecting** a threshold that balances catching anomalies vs. not over-flagging normal servers,
- **Using** cross-validation and $F_1$ scores to guide that threshold,

you successfully identified servers that **fall far outside** the normal operating range.

**Where to Go from Here?**  
- **Follow-Up Testing**: Using your best threshold, you can schedule on-site inspections for the servers flagged as anomalies. If the technicians confirm real defects, your approach is validated; if they find false alarms, you might refine your model or features further.  
- **Feature Engineering**: If certain anomalies were missed, consider adding or transforming features (like CPU-to-network ratio). This can help your model better distinguish normal from abnormal server states.  
- **Automation**: Over time, you can automate this anomaly detection pipeline to run daily or weekly, ensuring that newly installed servers stay healthy and that odd behavior is spotted before clients complain.

**Key Takeaways**:
1. **Power of Unsupervised Methods**: You needed few (if any) direct labels; by assuming most servers were normal, you modeled their behavior and spotted outliers.  
2. **Threshold Tuning is Crucial**: Getting an $F_1$ of 0.615 is a solid start, but you can often improve by customizing $\varepsilon$ or adding more robust features.  
3. **Practical Validation**: Real-world anomaly detection depends on validating results in the field. The perfect threshold in code means little if it doesn’t align with actual system health checks.  
4. **Iterative Process**: If you see too many false positives, adjust features or threshold. If you miss genuine anomalies, incorporate domain knowledge (like new metrics) to catch them.

Thus, you’ve not only built a baseline anomaly detection model but also laid the foundation for a proactive **server health monitoring** system. By continuously refining your approach, you’ll keep *ServerSecure Inc.* servers stable—and your customers satisfied!