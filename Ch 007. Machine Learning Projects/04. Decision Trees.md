# ***1. Scenario***

I’m starting a **small orchard** that grows a variety of apples. Some of them are ready to eat—**ripe**—while others need more time or are unsuitable for picking. I want a **quick** way to decide whether an apple is ripe based on its **color, shape, and where it grows** on the tree.

To do this, I’ll collect data about each apple and note:
- **Skin Color**: Red or Green  
- **Shape**: Conical or Round  
- **Position on the Tree**: Top Branch or Lower Branch  

Finally, I’ll label each apple as **ripe** (`1`) or **unripe** (`0`). I’ll then build a **decision tree** from scratch to predict if a freshly picked apple is ripe or not, using only its physical features.

---

# ***2. Steps***

## ***Step 1 : Importing Required Libraries***

First, I’ll import all the packages needed for this project.

```python
import numpy as np
import matplotlib.pyplot as plt
from public_tests import *
from utils import *

%matplotlib inline
```

## ***Step 2 : Dataset***

I’ve gathered a **small dataset** of 10 apples. For each one:

1. **Skin Color**: `Red` or `Green`
2. **Shape**: `Conical` or `Round`
3. **Position**: `Top` (if the apple was found on the top branch) or `Lower` (if on a lower branch)
4. **Label**: `Ripe` (`1`) or `Unripe` (`0`)

Below is a summary table with these features:

|            | Skin Color | Shape     | Position | Ripe |
|:----------:|:----------:|:---------:|:--------:|:----:|
| Apple 1    |  Red       | Conical   |  Top     |  1   |
| Apple 2    |  Red       | Round     |  Top     |  1   |
| Apple 3    |  Red       | Round     |  Lower   |  0   |
| Apple 4    |  Red       | Round     |  Lower   |  0   |
| Apple 5    |  Red       | Conical   |  Top     |  1   |
| Apple 6    |  Green     | Conical   |  Top     |  0   |
| Apple 7    |  Green     | Round     |  Lower   |  0   |
| Apple 8    |  Red       | Round     |  Top     |  1   |
| Apple 9    |  Green     | Conical   |  Lower   |  1   |
| Apple 10   |  Red       | Round     |  Lower   |  0   |

### ***Step 2.1 : One hot encoded dataset***

To make the decision tree simpler to implement, I converted each feature into **0/1** form (one-hot encoding). In the table below:

- `Red Color` = 1 means Red; 0 means Green  
- `Conical Shape` = 1 means Conical; 0 means Round  
- `Top Branch` = 1 means Top; 0 means Lower  

The label remains `1` for Ripe, `0` for Unripe.

|            | Red Color | Conical Shape | Top Branch | Ripe |
|:----------:|:---------:|:-------------:|:----------:|:----:|
| Apple 1    |    1      |      1        |     1      |  1   |
| Apple 2    |    1      |      0        |     1      |  1   |
| Apple 3    |    1      |      0        |     0      |  0   |
| Apple 4    |    1      |      0        |     0      |  0   |
| Apple 5    |    1      |      1        |     1      |  1   |
| Apple 6    |    0      |      1        |     1      |  0   |
| Apple 7    |    0      |      0        |     0      |  0   |
| Apple 8    |    1      |      0        |     1      |  1   |
| Apple 9    |    0      |      1        |     0      |  1   |
| Apple 10   |    1      |      0        |     0      |  0   |

Thus, our **feature matrix** (`X_train`) and **label array** (`y_train`) are:

```python
X_train = np.array([
    [1,1,1],
    [1,0,1],
    [1,0,0],
    [1,0,0],
    [1,1,1],
    [0,1,1],
    [0,0,0],
    [1,0,1],
    [0,1,0],
    [1,0,0]
])

y_train = np.array([1,1,0,0,1,0,0,1,1,0])
```

### ***Step 2.2 : View the variables***

Let’s look at the first few elements of `X_train` and `y_train`:

```python
print("First few elements of X_train:\n", X_train[:5])
print("Type of X_train:",type(X_train))

print("First few elements of y_train:", y_train[:5])
print("Type of y_train:",type(y_train))
```

The output should confirm that:
- `X_train` is a NumPy array with shape `(10, 3)`
- `y_train` is a NumPy array with shape `(10,)`

### ***Step 2.3 : Check the dimensions of variables***

We can check how many examples we have:

```python
print('The shape of X_train is:', X_train.shape)
print('The shape of y_train is:', y_train.shape)
print('Number of training examples (m):', len(X_train))
```

It should show `10` examples with `3` features each.

---

## ***Step 3 : Decision Tree Refresher***

We want to build a decision tree to classify each apple as **ripe** (`1`) or **unripe** (`0`). The algorithmic steps are:

1. **Start at the root node** with all samples.
2. **Compute Information Gain** for splitting on each available feature.
3. **Split** on the feature that yields the highest information gain.
4. **Repeat** splitting the data at each branch until we hit a stopping criterion (e.g., a max depth).

We’ll implement these core functions:
- `compute_entropy()`
- `split_dataset()`
- `compute_information_gain()`
- `get_best_split()`

Then we’ll **recursively** build the tree. For this demonstration, we set a **max depth of 2** to keep the example short.

### ***Step 3.1 : Calculate entropy***

```python
def compute_entropy(y):
    """
    Computes the entropy for the array y,
    where y[i] is 1 (ripe) or 0 (unripe).
    """
    entropy = 0.
    if len(y) != 0:
        p1 = len(y[y == 1]) / len(y)
        if p1 != 0 and p1 != 1:
            entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)
    return entropy
```

### ***Step 3.2 : Split dataset***

```python
def split_dataset(X, node_indices, feature):
    """
    Splits the data at the given node on a selected feature.
    Left branch = feature value == 1
    Right branch = feature value == 0
    """
    left_indices = []
    right_indices = []
    
    for i in node_indices:   
        if X[i][feature] == 1:
            left_indices.append(i)
        else:
            right_indices.append(i)
            
    return left_indices, right_indices
```

### ***Step 3.3 : Calculate information gain***

```python
def compute_information_gain(X, y, node_indices, feature):
    # Split on the feature
    left_indices, right_indices = split_dataset(X, node_indices, feature)
    
    # Calculate entropies
    y_node = y[node_indices]
    y_left = y[left_indices]
    y_right = y[right_indices]
    
    node_entropy = compute_entropy(y_node)
    left_entropy = compute_entropy(y_left)
    right_entropy = compute_entropy(y_right)
    
    w_left = len(y_left) / len(y_node)
    w_right = len(y_right) / len(y_node)
    
    weighted_entropy = w_left * left_entropy + w_right * right_entropy
    information_gain = node_entropy - weighted_entropy
    
    return information_gain
```

### ***Step 3.4 : Get best split***

```python
def get_best_split(X, y, node_indices):
    """
    Iterates over all features,
    returns the one with the maximum information gain.
    """
    num_features = X.shape[1]
    best_feature = -1
    max_info_gain = 0
    
    for feature in range(num_features):
        info_gain = compute_information_gain(X, y, node_indices, feature)
        
        if info_gain > max_info_gain:
            max_info_gain = info_gain
            best_feature = feature
            
    return best_feature
```

We can then verify which feature provides the highest information gain at the **root**.

---

## ***Step 4 : Building the Tree***

We’ll define a simple recursive function to:
1. **Stop** if the current depth equals the max depth.
2. **Split** on the best feature, print that info, and recursively build the left and right subtrees.

```python
tree = []

def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):
    if current_depth == max_depth:
        formatting = " "*current_depth + "-"*current_depth
        print(formatting, "%s leaf node with indices" % branch_name, node_indices)
        return
    
    best_feature = get_best_split(X, y, node_indices)
    
    formatting = "-"*current_depth
    print("%s Depth %d, %s: Split on feature: %d" % (formatting, current_depth, branch_name, best_feature))
    
    left_indices, right_indices = split_dataset(X, node_indices, best_feature)
    tree.append((left_indices, right_indices, best_feature))
    
    build_tree_recursive(X, y, left_indices, "Left", max_depth, current_depth+1)
    build_tree_recursive(X, y, right_indices, "Right", max_depth, current_depth+1)
```

Finally, we call:

```python
root_indices = list(range(len(X_train)))
build_tree_recursive(X_train, y_train, root_indices, "Root", max_depth=2, current_depth=0)
```

We’ll see the **splits** at each depth and which samples (indices) go to each leaf.

---

# ***3. Conclusion***

In this demonstration, I built a **short decision tree** to classify apples as **ripe** or **unripe**, based on a few binary features:
- **Red Color**  
- **Conical Shape**  
- **Top Branch**  

Though we only used 10 examples and stopped at a **depth of 2**, this process illustrates:
1. **One-hot encoding** for easy splitting on 0/1 features.  
2. **Entropy & Information Gain** to find the best split.  
3. **Recursive** tree-building logic.

In a **real orchard**, you’d likely:
- Collect many more samples, possibly from multiple picking seasons.  
- Add more features (e.g., size, texture, sugar content).  
- Allow a **deeper tree** or use an **ensemble** method (like a Random Forest or XGBoost) for higher accuracy and robustness.

Still, this **initial tree** can be a valuable tool for a small farm just starting out. By examining each apple’s color, shape, and position on the tree, you get a **quick, automated** guess about whether it’s ripe—helping ensure you sell only the **best** apples and keep your customers happy!