# Analyzing Stock Performance and Building a Dashboard, Visualizing using a Web Scraping
## 1) Scenario
This time, I will use web scraping to gather financial data, analyze stock performance, and build a dashboard.    
The focus will be on collecting historical share prices and revenue data for ***Microsoft***.    
By leveraging Python's web scraping libraries, I aim to visualize this data interactively to identify trends and provide insights for investment strategies.

## 2) Setup
```python
!pip install yfinance
!pip install pandas            # For data manipulation and analysis
!pip install requests          # For making HTTP requests to fetch web pages
!pip install nbformat          # For Jupyter Notebook file handling (if needed)
!pip install bs4               # For parsing HTML and XML documents
!pip install lxml              # For parsing XML and HTML (used by BeautifulSoup)
!pip install plotly            # For interactive data visualization
!pip install selenium          # For web scraping, especially for dynamically loaded content
!pip install webdriver-manager # For automatically downloading and managing WebDriver binaries for Selenium
```
```python
import os                                                    # For operating system interactions, such as file and directory operations
import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import plotly.graph_objects as go                            # For creating interactive plots and visualizations
from plotly.subplots import make_subplots                    # For creating subplots in Plotly
from selenium import webdriver                               # For automating web browser interactions
from selenium.webdriver.chrome.service import Service        # For managing the ChromeDriver service
from webdriver_manager.chrome import ChromeDriverManager     # For automatically downloading and managing WebDriver binaries for Selenium
import time                                                  # For adding delays in the script, useful for waiting for page loads
```

I will Set up **ChromeDriver** service using webdriver-manager
```python
service = Service(ChromeDriverManager().install())                # For Automatically downloads and sets up the ChromeDriver binary
```
```python
# Configuring Chrome options for the WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(service=service, options=options)        # For Initializing the Chrome WebDriver with the specified service and options
```
In this project, I define the function `make_graph`. It takes a dataframe with **stock data** (**Date** and **Close** columns), a dataframe with **revenue data** (**Date** and **Revenue** columns), and the name of the stock.
```python
def make_graph(stock_data, revenue_data, stock):
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=("Historical Share Price", "Historical Revenue"), vertical_spacing = .3)

    # Converting the 'Date' columns to datetime
    stock_data['Date'] = pd.to_datetime(stock_data['Date'])
    revenue_data['Date'] = pd.to_datetime(revenue_data['Date'])
    
    # Filtering data up to the specified dates
    stock_data_specific = stock_data[stock_data['Date'] <= '2024-03-31']
    revenue_data_specific = revenue_data[revenue_data['Date'] <= '2024-03-31']
    
    # Adding traces for stock prices and revenue
    fig.add_trace(go.Scatter(x=stock_data_specific['Date'], y=stock_data_specific['Close'].astype("float"), name="Share Price"), row=1, col=1)
    fig.add_trace(go.Scatter(x=revenue_data_specific['Date'], y=revenue_data_specific['Revenue'].astype("float"), name="Revenue"), row=2, col=1)
    
    # Updating x-axes and y-axes titles
    fig.update_xaxes(title_text="Date", row=1, col=1)
    fig.update_xaxes(title_text="Date", row=2, col=1)
    fig.update_yaxes(title_text="Price ($US)", row=1, col=1)
    fig.update_yaxes(title_text="Revenue ($US Millions)", row=2, col=1)
    
    # Updating layout
    fig.update_layout(showlegend=True,
                      height=900,
                      title=stock,
                      xaxis_rangeslider_visible=True)
    
    # Showing the figure
    fig.show()
```

## 3) Steps
### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) Step 1 : Using yfinance to Extract Stock Data    
```python
# Using yfinance to Extract Stock Data
Microsoft = yf.Ticker("MSFT")

# Extracting stock information and save it in a dataframe named Microsoft_data   
Microsoft_data = Microsoft.history(period="max")

# Resetting the index
Microsoft_data.reset_index(inplace=True)
print(Microsoft_data.head())
```
Then, I get the following :    
![image1](https://github.com/Atikers/Images/blob/main/Project%20%234%20-%20image(1).jpg)


### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) Step 2 : Using Webscraping to Extract Microsoft Revenue Data
```python
# Fetching the web page
url = "https://www.macrotrends.net/stocks/charts/MSFT/microsoft/revenue"
driver.get(url)
```
```python
# Waiting for the page to fully load
time.sleep(5)
```
```python
# Extracting page source and parse with BeautifulSoup
html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')
```
```python
# Closing the WebDriver
driver.quit()
```
```python
# Saving the HTML content to a file
current_directory = os.getcwd()
file_path = os.path.join(current_directory, "saved_page.html")
with open(file_path, 'w', encoding='utf-8') as file:
    file.write(str(soup))            # For Writing the string representation of the BeautifulSoup object to the file
```
```python
# Reading the saved HTML file
with open(file_path, 'r', encoding='utf-8') as file:
    html_content = file.read()
```
```python
# Parsing the HTML using BeautifulSoup
soup = BeautifulSoup(html_content, 'html.parser')
```
```python
# Finding the table containing the financial data
table = soup.find('table', {'class': 'historical_data_table'})
```
```python
# Initializing a list to store revenue data
data = []        # For Creating an empty list to store the extracted revenue data
```
```python
# Looping through each row in the table
for row in table.find_all('tr')[1:]:  # For Skipping the header row
    cols = row.find_all('td')
    if len(cols) > 1:
        date = cols[0].text.strip()
        revenue = cols[1].text.strip().replace('$', '').replace(',', '')
        data.append([date, revenue])
```
```python
# Converting the data into a pandas DataFrame
columns = ['Date', 'Revenue']
Microsoft_revenue = pd.DataFrame(data, columns=columns)
```
```python
# Converting 'Date' column to datetime format
Microsoft_revenue['Date'] = pd.to_datetime(Microsoft_revenue['Date'])
```
```python
# Converting 'Revenue' column to float
Microsoft_revenue['Revenue'] = Microsoft_revenue['Revenue'].astype(float)
```
```python
# Printing the DataFrame to verify the data
print(Microsoft_revenue.head())
```


### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) Step 3 : Plotting Microsoft Stock Graph
To graph the Microsoft Stock Data, Using the `make_graph` function(Graph will only show data upto March 2024).

```python
make_graph(Microsoft_data, Microsoft_revenue, 'Microsoft')
```
Then, I get the following :
![image2](https://github.com/Atikers/Images/blob/main/Project%20%234%20-%20image(2).jpg)


## 4) Conclusion
In this project, I successfully utilized Python's web scraping libraries to extract financial data for ***Microsoft***.    

By combining `yfinance` for historical stock prices and `BeautifulSoup` with `selenium` for revenue data, I was able to gather and visualize key financial metrics.    

The `make_graph` function provided a clear, interactive dashboard to identify trends and make informed investment decisions.    
This approach can be extended to other stocks and financial metrics, offering a versatile tool for financial analysis.    

By leveraging these techniques, I can automate the process of data collection and visualization, enabling more efficient and effective financial analysis.  

This project demonstrates the power of combining various Python libraries to achieve comprehensive data analysis and visualization.
