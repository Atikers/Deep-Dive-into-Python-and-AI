# Model Development

## 1. Introduction

A **model** can be thought of as a mathematical equation used to predict a value given one or more other values, relating one or more independent variables or features to a dependent variable. A model is similar to a black box: we input data and get an output.

### Simple Linear Regression

The simplest form of a model is the **simple linear regression model**, which uses a single independent variable to predict a dependent variable. The model is represented by the equation:

$$
Y = a + bX
$$

where:
- $Y$ is the dependent variable (e.g., car price).
- $X$ is the independent variable (e.g., engine size).
- $a$ is the intercept (e.g., the predicted price of a car with no engine).
- $b$ is the slope (e.g., the price increase for every unit increase in engine size).

### Importance of Data

Usually, the more relevant data you have, the more accurate your model is. This means three things:

1. **Same Variables but More Data**: Increasing the amount of data for the same set of variables generally improves the model's ability to learn patterns and relationships, leading to more accurate predictions. With more data points, the model can better generalize from the training data to unseen data.
  
2. **More Variables**: Adding more relevant variables (features) can provide the model with additional information that might be critical in making accurate predictions. For instance, including features that were previously excluded (like the color of a car) can significantly improve the model's performance if those features are influential in determining the target variable.

3. **Relevant Data**: It's essential that the data used for modeling is relevant to the problem at hand. Irrelevant data, even in large quantities, can lead to misleading results.

### Moving Beyond Simple Models

In addition to gathering more data or incorporating more variables, you can also experiment with different types of models. For example, you might use a **polynomial model**, which is a generalization of the simple linear regression model. The equation is given by:

$$
Y = a + b_1X + b_2X^2 + b_3X^3 + \cdots
$$

However, be careful! While a polynomial model can be a good approximation of the data, it can also lead to **overfitting**. Overfitting occurs when a model is too complex and starts to capture the noise in the data rather than the underlying pattern, leading to poor performance on new, unseen data.

Now, let's dive into how to develop a model using Python!

## 2. Simple Linear Regression and Multiple Linear Regression

Linear regression can refer to one independent variable to make a prediction(simple linear regression) or to multiple independent variables(multiple linear regression) to make a prediction.

### 1) Simple Linear Regression(SLR)

It is a method to help us understand the relationship between two variables:

- The predictor/independent variable ($X$)
- The target/dependent variable ($Y$)

The result of linear regression is a **linear function** that predicts the target variable as a function of the predictor variable.

For example, let's say we want to predict the car price using the engine size. The equation of the simple linear regression model is:

$$
Y = 10,000 + 320X
$$

If we assume there is a linear relationship between these variables, we can use this equation to predict the car price given the engine size. The slope of the line is 320, meaning that for every unit increase in engine size, the price of the car increases by $320. The intercept of the line is 10,000, which is the **predicted** price of a car with no engine.

How do we find the best-fitting line? for example, we have data points for engine size and car price. We can use the data to estimate the coefficients $a$ and $b$ in the equation. The coefficients are chosen in such a way that they minimize the difference between the predicted value and the actual value.

we store the dependent variable in the data frame or array `x` and the independent variable in the data frame or array `y`. Each sample corresponds to a different row in each data frame or array.

In many cases, many factors influence how much people pay for a car, for example, the make or how old the car is. This uncertainty is taken into account by assuming a small random value is added to the point on the line. This is called **noise**.

I mentioned that "the difference between the predicted value and the actual value is minimized". This "difference" is actually the residual, noise is one of the components contributing to it. The best-fitting line is the one that has the smallest residuals. At specific points, usually small values are added or subracted from the predicted value to get the actual value. 

So, to briefly summarize the simple linear regression model : If we have a set of data points, we can use a simple linear regression model to predict the target variable(car price) as a function of the predictor variable(engine size). Then, we can use this model to predict values that we haven't seen before.

However, our model is not always correct. The model is an approximation of the real relationship between the variables.

We can create a simple linear regression model using the `scikit-learn` library in Python.

#### *1. Importing Libraries*

```python
from sklearn.linear_model import LinearRegression   # Import linear_model from scikit-learn
lm = LinearRegression()  # Create a linear regression object using the constructor
lm
```

#### *2. Training the model*

```python
X = df[['engine-size']]  # Independent variable
Y = df['price']  # Dependent variable
lm.fit(X, Y)  # Fit the linear model using the data
```

#### *3. Obtaining a prediction*

```python
Yhat = lm.predict(X)  # Predict the value of Y using the model
Yhat[0:5]  # Display the first 5 predicted values
```

#### *4. Obtaining the value of the intercept and slope*

- intercept($a$) : `lm.intercept_`
- the slope($b$) : `lm.coef_`

#### *5. Obtaining the final model*

- The final model is: $Price = a + b\cdot EngineSize$
- we can define $\hat{Y} = a + bX$, becuase we obtain the estimated value of $Y$ using the model, not the actual value of $Y$.
- However, $a$ and $b$ are the estimated parameters, not the actual ones. Therefore, strictly speaking, the model is $\hat{Y} = \hat{a} + \hat{b}X$.

### 2) Multiple Linear Regression(MLR)

This method is used to explain the relationship between one continuous target(dependent) variable and two or more predictor(independent) variables.

For example, let's say we want to predict the car price using *horsepower*, *curb-weight*, *engine-size* of the car. The equation of the multiple linear regression model is:

$$
\hat{Y} = a + b_1X_1 + b_2X_2 + b_3X_3
$$

where:
- $\hat{Y}$ is the predicted value of the target variable.
- $a$ is the intercept.
- $b_1, b_2, b_3$ are the coefficients of the predictors $X_1, X_2, X_3$.
- Strictly speaking, the model is $\hat{Y} = \hat{a} + \hat{b_1}X_1 + \hat{b_2}X_2 + \hat{b_3}X_3$.

we can visualze the values. Let's consider the following example:

| $X_1$ | $X_2$ | $Y$  |
|------|-----|--|
| 2.4  | 100 | 10,000|
| 3.5  | 200 | 20,000|
| 4.2  | 300 | 30,000|
| 5.0  | 400 | 40,000|

The table above can be visualized on a 3D plot. The x-axis represents $X_1$, the y-axis represents $X_2$, and the z-axis represents $Y$. The points on the plot represent the actual values of $X_1$, $X_2$, and $Y$. The plane represents the predicted values of $Y$ based on the values of $X_1$ and $X_2$. If the z-axis represents $\hat{Y}$, the plane represents the predicted values of $\hat{Y}$ based on the values of $X_1$ and $X_2$.

Now, we can create a multiple linear regression model using the `scikit-learn` library in Python.

#### *1. Importing Libraries*

```python
from sklearn.linear_model import LinearRegression   
lm = LinearRegression()  
lm
```

#### *2. Training the model*

```python
X = df[['horsepower', 'curb-weight', 'engine-size']]  # Independent variables
Y = df['price']  # Dependent variable
lm.fit(X, Y)  
```

#### *3. Obtaining a prediction*

```python
Yhat = lm.predict(X)
Yhat[0:5]  
```

#### *4. Obtaining the value of the intercept and slope*

- intercept($a$) : lm.intercept_
- the slope($b_1, b_2, b_3$) : lm.coef_

#### *5. Obtaining the final model*

- The final model is: $Price = a + b_1*Horsepower + b_2*CurbWeight + b_3*EngineSize$
- we can define $\hat{Y} = a + b_1X_1 + b_2X_2 + b_3X_3$

### 3) Summary

On this page, we explored the fundamentals of regression models, particularly focusing on simple and multiple linear regression. Here are the key takeaways:

#### *1. Model Definition*

- A model is a mathematical equation that predicts a dependent variable (such as car price) based on one or more independent variables (such as engine size). Linear regression is one of the simplest and most widely used types of models.

#### *2. Simple Linear Regression (SLR):*

- Equation: The simple linear regression model is represented as $Y=a+bX$, where $Y$ is the dependent variable, $X$ is the independent variable, $a$ is the intercept, and $b$ is the slope.

- Application: SLR is used to understand the relationship between two variables. For instance, predicting car price based on engine size. The model estimates the slope and intercept to fit the best possible line through the data, minimizing the difference (residual) between the actual and predicted values.

#### *3. Noise:* 

- In real-world data, predictions are often influenced by random variability or noise. Noise represents the small, unpredictable factors that cause deviations from the predicted values. The model aims to minimize this noise, but some level of noise is inherent and expected.

#### *4. Multiple Linear Regression (MLR):*

- Equation: MLR extends SLR by incorporating multiple independent variables, represented as $\hat{Y}=a+b_1X_1+b_2X_2+b_3X_3$. Here, $\hat{Y}$ is the predicted value, $X_1, X_2, X_3$ are the independent variables, and $b_1, b_2, b_3$ are their respective coefficients.

- Application: MLR is used when predicting an outcome based on several factors. For example, predicting car price using horsepower, curb weight, and engine size. The model fits a plane (in higher dimensions) to the data, optimizing the coefficients to minimize residuals across all data points.

#### *5. Model Implementation in Python:*

- Both SLR and MLR models can be implemented using the scikit-learn library in Python. The process involves importing necessary libraries, training the model with data, and then using the model to make predictions.
Key outputs include the intercept and coefficients, which define the final regression equation used for prediction.

#### *6. Key Considerations:*

- While more data and more variables can improve model accuracy, careful attention must be paid to avoid overfitting, particularly with more complex models like polynomial regression.
- A model is always an approximation of the true relationship between variables, and its predictions should be interpreted with this in mind.

## 3. Model Evaluation using Visualization

## 4. Polynomial Regression and Pipelines

## 5. R-squared and MSE for In-Sample Evaluation

## 6. Prediction and Decision Making