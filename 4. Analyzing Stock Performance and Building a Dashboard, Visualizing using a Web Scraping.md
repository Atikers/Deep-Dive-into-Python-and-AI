# Analyzing Stock Performance and Building a Dashboard, Visualizing using a Web Scraping
## 1) Scenario
This time, I will use web scraping to gather financial data, analyze stock performance, and build a dashboard.    
The focus will be on collecting historical share prices and revenue data for ***Microsoft***.    
By leveraging Python's web scraping libraries, I aim to visualize this data interactively to identify trends and provide insights for investment strategies.

## 2) Setup
```python
!pip install yfinance
!pip install pandas            # For data manipulation and analysis
!pip install requests          # For making HTTP requests to fetch web pages
!pip install nbformat          # For Jupyter Notebook file handling (if needed)
!pip install bs4               # For parsing HTML and XML documents
!pip install lxml              # For parsing XML and HTML (used by BeautifulSoup)
!pip install plotly            # For interactive data visualization
!pip install selenium          # For web scraping, especially for dynamically loaded content
```
```python
import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import lxml
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time
import os        # Importing the os library for operating system interactions
import json      # Importing the json library for JSON file handling
```
```python
# Setting up Selenium WebDriver
service = Service(ChromeDriverManager().install())
options = webdriver.ChromeOptions()
options.add_argument("--headless")          # Headless mode for running without a GUI
driver = webdriver.Chrome(service=service, options=options)
```
In this project, I define the function `make_graph`. It takes a dataframe with **stock data** (**Date** and **Close** columns), a dataframe with **revenue data** (**Date** and **Revenue** columns), and the name of the stock.
```python
def make_graph(stock_data, revenue_data, stock):
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=("Historical Share Price", "Historical Revenue"), vertical_spacing = .3)

    # Converting the 'Date' columns to datetime
    stock_data['Date'] = pd.to_datetime(stock_data['Date'])
    revenue_data['Date'] = pd.to_datetime(revenue_data['Date'])
    
    # Filtering data up to the specified dates
    stock_data_specific = stock_data[stock_data['Date'] <= '2024-03-31']
    revenue_data_specific = revenue_data[revenue_data['Date'] <= '2024-03-31']
    
    # Adding traces for stock prices and revenue
    fig.add_trace(go.Scatter(x=stock_data_specific['Date'], y=stock_data_specific['Close'].astype("float"), name="Share Price"), row=1, col=1)
    fig.add_trace(go.Scatter(x=revenue_data_specific['Date'], y=revenue_data_specific['Revenue'].astype("float"), name="Revenue"), row=2, col=1)
    
    # Updating x-axes and y-axes titles
    fig.update_xaxes(title_text="Date", row=1, col=1)
    fig.update_xaxes(title_text="Date", row=2, col=1)
    fig.update_yaxes(title_text="Price ($US)", row=1, col=1)
    fig.update_yaxes(title_text="Revenue ($US Millions)", row=2, col=1)
    
    # Updating layout
    fig.update_layout(showlegend=True,
                      height=900,
                      title=stock,
                      xaxis_rangeslider_visible=True)
    
    # Showing the figure
    fig.show()
```

## 3) Steps
### (1) Step 1 : Using yfinance to Extract Stock Data    
```python
# Using yfinance to Extract Stock Data
Microsoft = yf.Ticker("MSFT")

# Extracting stock information and save it in a dataframe named Microsoft_data   
Microsoft_data = Microsoft.history(period="max")

# Resetting the index
Microsoft_data.reset_index(inplace=True)
print(Microsoft_data.head())
```
Then, I get the following :    
![image1](


### (2) Step 2 : Using Webscraping to Extract Microsoft Revenue Data
```python
# Fetching the web page
url = "https://finance.yahoo.com/quote/MSFT/financials/"
driver.get(url)

# Wait for the page to fully load
time.sleep(10)

# Extracting page source and parse with BeautifulSoup
html = driver.page_source
soup = BeautifulSoup(html, 'lxml')

# Finding the table containing financial data
tables = soup.find_all('div', {'data-test': 'fin-row'})
data = []

# Looping through each table row
for table in tables:
    # Get the text content of each cell
    row_data = [cell.text for cell in table.find_all('div', {'data-test': 'fin-col'})]
    # Check if the first cell contains 'Total Revenue'
    if 'Total Revenue' in row_data[0]:
        data.append(row_data)

# Closing the WebDriver
driver.quit()

# Convert the data into a pandas DataFrame
columns = ["Metric", "TTM", "6/30/2023", "6/30/2022", "6/30/2021", "6/30/2020"]
financials = pd.DataFrame(data, columns=columns)

# Print the DataFrame to verify the data
print(financials.head())
```


### (3) Step 3 : Identifying the HTML tags

```python
nvidia_data=pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])
```
>**Working on HTML table**    
>**&lt;table&gt;** : This tag is a root tag used to define the start and end of the table. All the content of the table is enclosed within these tags.    
>**&lt;tr&gt;** : This tag is used to define a table **row**. Each row of the table is defined within this tag.    
>**&lt;td&gt;** : This tag is used to define a table **cell**. Each cell of the table is defined within this tag. You can specify the content of the cell between the opening and closing tags.    
>**&lt;th&gt;** : This tag is used to define a header cell in the table. The header cell is used to describe the contents of a column or row. By default, the text inside a tag is bold and centered.    
>**&lt;tbody&gt;** : This is the main content of the table, which is defined using the tag. It contains one or more rows of elements.    

### (4) Step 4 : Using a BeautifulSoup method for extracting data
I will use `find()` and `find_all()` methods of the BeautifulSoup object to locate the table body and table row respectively in the HTML.
* The `find()` method will return particular tag content.
* The `find_all()` method returns a list of all matching tags in the HTML.
```python
for row in soup.find("tbody").find_all('tr'):
    col = row.find_all("td")
if len(col) == 7 and "Dividend" not in col[4].text:
    date = col[0].text.strip()
    open_price = col[1].text.strip()
    high = col[2].text.strip()
    low = col[3].text.strip()
    close = col[4].text.strip()
    adj_close = col[5].text.strip()
    volume = col[6].text.strip()

    new_row=pd.DataFrame({
        "Date":[date],
        "Open":[open_price],
        "High":[high],
        "Low":[low],
        "Close":[close],
        "Adj Close":[adj_close],
        "Volume":[volume]
    })
    nvidia_data = pd.concat([nvidia_data, new_row], ignore_index=True)
```
### (5) Step 5 : Printing the extracted data
```python
nvidia_data.head()
```
