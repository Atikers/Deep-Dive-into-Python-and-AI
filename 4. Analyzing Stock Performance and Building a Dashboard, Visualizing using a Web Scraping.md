# Analyzing Stock Performance and Building a Dashboard, Visualizing using a Web Scraping
## 1) Scenario
This time, I will use web scraping to gather financial data, analyze stock performance, and build a dashboard.    
The focus will be on collecting historical share prices and revenue data for ***Microsoft***.    
By leveraging Python's web scraping libraries, I aim to visualize this data interactively to identify trends and provide insights for investment strategies.

## 2) Setup
```python
!pip install yfinance
!pip install pandas            # For data manipulation and analysis
!pip install requests          # For making HTTP requests to fetch web pages
!pip install nbformat          # For Jupyter Notebook file handling (if needed)
!pip install bs4               # For parsing HTML and XML documents
!pip install lxml              # For parsing XML and HTML (used by BeautifulSoup)
!pip install plotly            # For interactive data visualization
!pip install selenium          # For web scraping, especially for dynamically loaded content
```
```python
import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import lxml
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import os                                                            # Importing the os library for operating system interactions
import json                                                          # Importing the json library for JSON file handling
```
In this project, I define the function `make_graph`. It takes a dataframe with **stock data** (**Date** and **Close** columns), a dataframe with **revenue data** (**Date** and **Revenue** columns), and the name of the stock.
```python
def make_graph(stock_data, revenue_data, stock):
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=("Historical Share Price", "Historical Revenue"), vertical_spacing = .3)

    # Converting the 'Date' columns to datetime
    stock_data['Date'] = pd.to_datetime(stock_data['Date'])
    revenue_data['Date'] = pd.to_datetime(revenue_data['Date'])
    
    # Filtering data up to the specified dates
    stock_data_specific = stock_data[stock_data['Date'] <= '2024-04-30']
    revenue_data_specific = revenue_data[revenue_data['Date'] <= '2024-03-31']
    
    # Adding traces for stock prices and revenue
    fig.add_trace(go.Scatter(x=stock_data_specific['Date'], y=stock_data_specific['Close'].astype("float"), name="Share Price"), row=1, col=1)
    fig.add_trace(go.Scatter(x=revenue_data_specific['Date'], y=revenue_data_specific['Revenue'].astype("float"), name="Revenue"), row=2, col=1)
    
    # Updating x-axes and y-axes titles
    fig.update_xaxes(title_text="Date", row=1, col=1)
    fig.update_xaxes(title_text="Date", row=2, col=1)
    fig.update_yaxes(title_text="Price ($US)", row=1, col=1)
    fig.update_yaxes(title_text="Revenue ($US Millions)", row=2, col=1)
    
    # Updating layout
    fig.update_layout(showlegend=True,
                      height=900,
                      title=stock,
                      xaxis_rangeslider_visible=True)
    
    # Showing the figure
    fig.show()
```

## 3) Steps
### (1) Step 1 : Using yfinance to Extract Stock Data    
Using the `Ticker` function enter the ticker symbol of the stock we want to extract data on to create a ticker object. The stock is Microsoft and its ticker symbol is `MSFT`.    
```python
Microsoft = yf.Ticker("MSFT")
```
Using the ticker object and the function `history` extract stock information and save it in a dataframe named `tesla_data`. Set the `period` parameter to ` "max" ` so we get information for the maximum amount of time.    
```python
Microsoft_data = Microsoft.history(period="max")
```
**Resetting the index** using the `reset_index(inplace=True)` function on the Microsoft_data DataFrame and display the first five rows of the `Microsoft_data` dataframe using the `head` function.
```python
Microsoft_data.reset_index(inplace=True)
print(Microsoft_data.head())
```

I will use the request library for sending an HTTP request to the web page.  
```python
url="https://finance.yahoo.com/quote/NVDA/history/?frequency=1mo&period1=1533772800&period2=1717891200"
```
```python
driver=webdriver.Chrome()
driver.get(url)
```
```python
time.sleep(5)
```

### (2) Step 2 : Parsing the HTML content

Parsing the data using the BeautifulSoup library
```python
soup=BeautifulSoup(data, 'html.parser')
```
```python
driver.quit()
```

### (3) Step 3 : Identifying the HTML tags

```python
nvidia_data=pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])
```
>**Working on HTML table**    
>**&lt;table&gt;** : This tag is a root tag used to define the start and end of the table. All the content of the table is enclosed within these tags.    
>**&lt;tr&gt;** : This tag is used to define a table **row**. Each row of the table is defined within this tag.    
>**&lt;td&gt;** : This tag is used to define a table **cell**. Each cell of the table is defined within this tag. You can specify the content of the cell between the opening and closing tags.    
>**&lt;th&gt;** : This tag is used to define a header cell in the table. The header cell is used to describe the contents of a column or row. By default, the text inside a tag is bold and centered.    
>**&lt;tbody&gt;** : This is the main content of the table, which is defined using the tag. It contains one or more rows of elements.    

### (4) Step 4 : Using a BeautifulSoup method for extracting data
I will use `find()` and `find_all()` methods of the BeautifulSoup object to locate the table body and table row respectively in the HTML.
* The `find()` method will return particular tag content.
* The `find_all()` method returns a list of all matching tags in the HTML.
```python
for row in soup.find("tbody").find_all('tr'):
    col = row.find_all("td")
if len(col) == 7 and "Dividend" not in col[4].text:
    date = col[0].text.strip()
    open_price = col[1].text.strip()
    high = col[2].text.strip()
    low = col[3].text.strip()
    close = col[4].text.strip()
    adj_close = col[5].text.strip()
    volume = col[6].text.strip()

    new_row=pd.DataFrame({
        "Date":[date],
        "Open":[open_price],
        "High":[high],
        "Low":[low],
        "Close":[close],
        "Adj Close":[adj_close],
        "Volume":[volume]
    })
    nvidia_data = pd.concat([nvidia_data, new_row], ignore_index=True)
```
### (5) Step 5 : Printing the extracted data
```python
nvidia_data.head()
```
