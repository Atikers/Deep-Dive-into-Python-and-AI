# Python Basics with Numpy

> ***"Any fool can write code that a computer can understand. Good programmers write code that humans can understand."***  
> — Martin Fowler

## Numbers as Objects: Understanding the Fundamental Tools of Deep Learning

> ***Have you ever wondered how the human brain processes billions of signals simultaneously? And have you considered how computers might mimic this parallel processing? What if I told you that the key to both lies in treating numbers not as mere values, but as sophisticated objects with behaviors and relationships?***

Welcome to our first journey into deep learning projects! Before building complex AI systems, we must first understand our fundamental tools: Python and NumPy, from an object-oriented perspective. This is like a chef sharpening their knives before cooking, or an artist preparing their brushes before creating a masterpiece.

## The Object-Oriented View of Numerical Computation

In object-oriented thinking, we view everything in our computational universe as objects with properties and behaviors. This perspective revolutionizes how we understand the building blocks of deep learning:

### 1. Arrays: Data Objects with Identities and Behaviors

In NumPy, arrays aren't just collections of numbers—they're sophisticated objects with:
- **Properties**: dimensions, shape, data type
- **Behaviors**: they can transform (reshape), measure themselves (norm), and interact with other arrays

When we create an array in NumPy, we're instantiating a powerful object that knows how to manage its data efficiently.

### 2. Functions as Transformation Methods

Each function we define acts as a specialized transformation method that converts input objects into output objects:

```python
def sigmoid(x):
    """
    Compute the sigmoid of x
    
    Arguments:
    x -- A scalar or numpy array of any size
    
    Return:
    s -- sigmoid(x)
    """
    s = 1 / (1 + np.exp(-x))
    return s
```

The sigmoid function isn't just a mathematical formula—it's a universal transformation object that appears across many domains:
- In neuroscience, it models neuron activation
- In economics, it models adoption of new technologies
- In biology, it models population growth under constraints

This same object structure repeats across domains—evidence of the universal patterns that object-oriented thinking reveals.

### 3. From Images to Vectors: Interface Transformation

When we reshape an image from a 3D array to a 1D vector, we're changing the interface through which we interact with the object, while preserving all of its information:

```python
def image2vector(image):
    """
    Argument:
    image -- a numpy array of shape (length, height, depth)
    
    Returns:
    v -- a vector of shape (length*height*depth, 1)
    """
    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)
    return v
```

This transformation is like changing the format of a document from a structured book to a continuous scroll—the content remains the same, but the interface changes to suit different needs.

### 4. Normalization: Creating Comparable Objects

Normalization functions transform diverse inputs into standardized forms that can be meaningfully compared:

```python
def normalize_rows(x):
    """
    Normalizes each row to have unit length
    
    Argument:
    x -- A numpy matrix of shape (n, m)
    
    Returns:
    x -- The normalized matrix
    """
    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)
    x = x / x_norm
    return x
```

This is analogous to how we might normalize metrics in different fields:
- Economists normalize GDP by population to compare countries
- Doctors normalize dosages by patient weight
- Scientists normalize measurements to standard conditions

### 5. Softmax: The Decision Distribution Object

The softmax function transforms raw values into probability distributions—it's a sophisticated decision-making object:

```python
def softmax(x):
    """
    Calculates softmax for each row
    
    Argument:
    x -- A numpy matrix of shape (m,n)
    
    Returns:
    s -- Softmax output
    """
    x_exp = np.exp(x)
    x_sum = np.sum(x_exp, axis=1, keepdims=True)
    s = x_exp / x_sum
    return s
```

This pattern appears in:
- Consumer choice theory (distributing preferences among products)
- Ecological competition (species competing for resources)
- Political systems (voters distributing support among candidates)

## Broadcasting: Objects That Adapt Their Interfaces

One of NumPy's most powerful features is broadcasting—the ability for arrays of different shapes to intelligently adapt their interfaces during operations. When we divide a matrix by a vector in `normalize_rows`, NumPy automatically "broadcasts" the vector across each row:

```python
# x has shape (n,m), x_norm has shape (n,1)
x = x / x_norm  # Broadcasting automatically applies the right division to each row
```

This is like having smart objects that automatically adjust how they communicate based on context—similar to how humans adapt their explanations depending on the audience.

## Vectorization: Parallel Processing Objects

Vectorization transforms sequential operations into parallel ones:

```python
# Sequential processing (slow)
dot = 0
for i in range(len(x1)):
    dot += x1[i] * x2[i]

# Parallel processing with vectorization (fast)
dot = np.dot(x1, x2)
```

The speed difference is dramatic:

```
Non-vectorized dot = 278  Computation time = 0.234ms
Vectorized dot = 278  Computation time = 0.078ms
```

This mirrors the difference between:
- A sequential assembly line where each worker completes one product at a time
- A parallel production system where all components are processed simultaneously

## Loss Functions: Quality Measurement Objects

Loss functions act as specialized quality measurement objects, each with unique behaviors:

```python
def L1(yhat, y):
    """
    Arguments:
    yhat -- predicted values
    y -- true values
    
    Returns:
    loss -- L1 loss value
    """
    loss = np.sum(abs(y - yhat))
    return loss

def L2(yhat, y):
    """
    Arguments:
    yhat -- predicted values
    y -- true values
    
    Returns:
    loss -- L2 loss value
    """
    loss = np.sum(np.dot(yhat-y, yhat-y))
    return loss
```

These functions embody different error measurement philosophies:
- L1 (Manhattan distance): More robust to outliers, like navigation through city blocks
- L2 (Euclidean distance): More sensitive to large errors, like direct "as the crow flies" distance

## The Complete Toolkit

Here's the comprehensive code toolkit, now viewed through an object-oriented lens:

```python
import math
import time
import numpy as np

# Sigmoid transformation object - converts any value to range (0,1)
def sigmoid(x):
    """Compute the sigmoid of x (scalar or numpy array)."""
    s = 1/(1 + np.exp(-x))
    return s

# Sigmoid gradient - measures sensitivity to input changes
def sigmoid_derivative(x):
    """Compute the gradient of the sigmoid function w.r.t. its input x."""
    s = sigmoid(x)
    ds = s*(1-s)
    return ds

# Interface transformer - changes how we interact with image data
def image2vector(image):
    """Reshapes a 3D matrix into a 2D vector."""
    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)
    return v

# Normalization object - creates universal comparison framework
def normalize_rows(x):
    """Normalizes each row of matrix x to have unit length."""
    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)
    x = x / x_norm
    return x

# Decision distribution object - creates probability distributions
def softmax(x):
    """Compute softmax for each row of matrix x."""
    x_exp = np.exp(x)
    x_sum = np.sum(x_exp, axis=1, keepdims=True)
    s = x_exp / x_sum
    return s

# Loss measurement objects - different ways to quantify errors
def L1(yhat, y):
    """L1 loss function. Sum of absolute differences."""
    loss = np.sum(abs(y - yhat))
    return loss

def L2(yhat, y):
    """L2 loss function. Sum of squared differences."""
    loss = np.sum(np.dot(yhat-y, yhat-y))
    return loss
```

## Why This Matters for Deep Learning

Understanding NumPy from an object-oriented perspective prepares you to think about deep learning models in a much more powerful way:

1. **Neural Networks as Object Hierarchies**: Neural networks are composed of layers (objects) which contain neurons (objects) that perform transformations on inputs. The entire structure is an object hierarchy.

2. **Gradient Descent as Object Communication**: When training networks, gradients flow backward through the network—this is objects communicating with each other to optimize their behaviors.

3. **Reusable Components**: Just as we've created reusable function objects here, deep learning frameworks like PyTorch and TensorFlow let you create reusable network components.

4. **Interface Standardization**: All our functions accept arrays as inputs and produce arrays as outputs—this standardized interface is essential for building complex systems from simple components.

## Summary

In this project, we've seen how NumPy transforms raw numbers into powerful computational objects:
- Arrays as data objects with behaviors
- Functions as transformation methods
- Broadcasting as adaptive communication
- Vectorization as parallel processing

By viewing numerical computation through this object-oriented lens, we've laid the foundation for understanding more complex deep learning architectures. In these systems, everything from individual neurons to entire networks can be understood as objects that interact to create intelligent behavior.

> ***Remember: In deep learning, we're not just manipulating numbers—we're creating a universe of interacting objects that process information in increasingly sophisticated ways. And when working with NumPy, think whole objects, not individual elements. Arrays are objects with behaviors, not just collections of numbers.***